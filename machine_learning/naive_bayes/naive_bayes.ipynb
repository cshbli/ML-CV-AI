{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "Naïve Bayes algorithm is a supervised learning algorithm, which is based on `Bayes theorem` and used for solving classification problems.\n",
    "\n",
    "The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "- <b>Naïve</b>: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. \n",
    "\n",
    "- <b>Bayes</b>: It is called Bayes because it depends on the principle of Bayes' Theorem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem:\n",
    "\n",
    "Bayes' theorem is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.\n",
    "\n",
    "The formula for Bayes' theorem is given as:\n",
    "\n",
    "$$P(A|B)=\\dfrac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "- <b>P(A|B) is Posterior probability</b>: Probability of hypothesis A on the observed event B.\n",
    "\n",
    "- <b>P(B|A) is Likelihood probability</b>: Probability of the evidence given that the probability of a hypothesis is true.\n",
    "\n",
    "- <b>P(A) is Prior Probability</b>: Probability of hypothesis before observing the evidence.\n",
    "\n",
    "- <b>P(B) is Marginal Probability</b>: Probability of Evidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Assumption\n",
    "\n",
    "Now, its time to put a naive assumption to the Bayes’ theorem, which is, <b>independence</b> among the features. So now, we split evidence into the independent parts.\n",
    "\n",
    "Now, if any two events A and B are independent, then,\n",
    "\n",
    "$$P(A,B) = P(A)P(B)$$\n",
    "\n",
    "Hence, we reach to the result:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = \\dfrac{ P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)}$$ \n",
    "\n",
    "which can be expressed as:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = \\dfrac{P(y)\\prod_{i=1}^{n}P(x_i|y)}{P(x_1)P(x_2)...P(x_n)}$$\n",
    "\n",
    "Now, as the denominator remains constant for a given input, we can remove that term:\n",
    "\n",
    "$$P(y|x_1,...,x_n)\\propto P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "Now, we need to create a classifier model. For this, we find the probability of given set of inputs for all possible values of the class variable y and pick up the output with maximum probability. This can be expressed mathematically as:\n",
    "\n",
    "$$y = argmax_{y} P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "So, finally, we are left with the task of calculating P(y) and $P(x_i|y)$.\n",
    "\n",
    "Please note that P(y) is also called <b>class probability</b> and $P(x_i|y)$ is called <b>conditional probability</b>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Here is a tabular representation of our dataset.\n",
    "\n",
    "||Outlook|\tTemperature|\tHumidity|\tWindy|\tPlay Golf|\n",
    "|---|---|---|---|---|---|\n",
    "|0|\tRainy|\tHot|\tHigh|\tFalse|\tNo|\n",
    "|1|\tRainy|\tHot\tHigh|\tTrue|\tNo|\n",
    "|2|\tOvercast|\tHot\tHigh|\tFalse|\tYes|\n",
    "|3|\tSunny|\tMild|\tHigh|\tFalse|\tYes|\n",
    "|4|\tSunny|\tCool|\tNormal|\tFalse|\tYes|\n",
    "|5|\tSunny|\tCool|\tNormal|\tTrue|\tNo|\n",
    "|6|\tOvercast|\tCool|\tNormal|\tTrue|\tYes|\n",
    "|7|\tRainy|\tMild|\tHigh|\tFalse|\tNo|\n",
    "|8|\tRainy|\tCool|\tNormal|\tFalse|\tYes|\n",
    "|9|\tSunny|\tMild|\tNormal|\tFalse|\tYes|\n",
    "|10|\tRainy|\tMild|\tNormal|\tTrue|\tYes|\n",
    "|11|\tOvercast|\tMild|\tHigh|\tTrue|\tYes|\n",
    "|12|\tOvercast|\tHot\tNormal|\tFalse|\tYes|\n",
    "|13|\tSunny|\tMild|\tHigh|\tTrue|\tNo|\n",
    "\n",
    "We need to find P(xi | yj) for each xi in X and yj in y. All these calculations have been demonstrated in the tables below:\n",
    "\n",
    "<img src=\"naive-bayes-classification.png\">\n",
    "\n",
    "So, in the figure above, we have calculated $P(x_i|y_j)$ for each $x_i$ in X and $y_j$ in y manually in the tables 1-4. For example, probability of playing golf given that the temperature is cool, i.e $P(temp.=cool|play\\;golf = Yes) = 3/9$.\n",
    "\n",
    "Also, we need to find class probabilities P(y) which has been calculated in the table 5. For example, $P(play\\;golf = Yes) = 9/14$.\n",
    "\n",
    "So now, we are done with our pre-computations and the classifier is ready!\n",
    "\n",
    "Let us test it on a new set of features (let us call it today):\n",
    "\n",
    "today = (Sunny, Hot, Normal, False)\n",
    "\n",
    "So, probability of playing golf is given by:\n",
    "\n",
    "$$P(Yes|today) = \\dfrac{P(Sunny\\;Outlook|Yes)P(Hot\\;Temperature|Yes)P(Normal\\;Humidity|Yes)P(No\\;Wind|Yes)P(Yes)}{P(today)}$$ \n",
    "\n",
    "and probability to not play golf is given by:\n",
    "\n",
    "$$P(No|today) = \\dfrac{P(Sunny\\;Outlook|No)P(Hot\\;Temperature|No)P(Normal\\;Humidity|No)P(No\\;Wind|No)P(No)}{P(today)}$$\n",
    "\n",
    "Since, P(today) is common in both probabilities, we can ignore P(today) and find proportional probabilities as:\n",
    "\n",
    "$$P(Yes|today) \\propto \\dfrac{2}{9}.\\dfrac{2}{9}.\\dfrac{6}{9}.\\dfrac{6}{9}.\\dfrac{9}{14} \\approx 0.0141$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(No|today) \\propto \\dfrac{3}{5}.\\dfrac{2}{5}.\\dfrac{1}{5}.\\dfrac{2}{5}.\\dfrac{5}{14} \\approx 0.0068$$\n",
    "\n",
    "Now, since\n",
    "\n",
    "$$P(Yes|today) + P(No|today) = 1$$ \n",
    "\n",
    "These numbers can be converted into a probability by making the sum equal to 1 (normalization):\n",
    "\n",
    "$$P(Yes|today) = \\dfrac{0.0141}{0.0141 + 0.0068} = 0.67$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P(No|today) = \\dfrac{0.0068}{0.0141 + 0.0068} = 0.33$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$P(Yes|today) > P(No|today)$$\n",
    "\n",
    "So, prediction that golf would be played is ‘Yes’.\n",
    "\n",
    "The method that we discussed above is applicable for discrete data. In case of continuous data, we need to make some assumptions regarding the distribution of values of each feature. The different Naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i|y)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Naïve Bayes Model for continuous data:\n",
    "\n",
    "There are three types of Naive Bayes Model, which are given below:\n",
    "\n",
    "- <b>Gaussian</b>: In Gaussian Naive Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution. A Gaussian distribution is also called Normal distribution. When plotted, it gives a bell shaped curve which is symmetric about the mean of the feature values as shown below:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"naive-bayes-classification-1.png\">\n",
    "</p>\n",
    "\n",
    "The likelihood of the features is assumed to be Gaussian, hence, conditional probability is given by:\n",
    "\n",
    "$$P(x_i|y) = \\dfrac{1}{\\sqrt{2\\pi\\sigma _{y}^{2} }} exp \\left (-\\frac{(x_i-\\mu _{y})^2}{2\\sigma _{y}^{2}}  \\right )$$ \n",
    "\n",
    "- <b>Multinomial</b>: Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.\n",
    "\n",
    "- <b>Bernoulli</b>: In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence(i.e. a word occurs in a document or not) features are used rather than term frequencies(i.e. frequency of a word in the document).\n",
    "\n",
    "- <b>Complement Naive Bayes</b>: It is an adaptation of Multinomial NB where the complement of each class is used to calculate the model weights. So, this is suitable for imbalanced data sets and often outperforms the MNB on text classification tasks.\n",
    "\n",
    "- <b>Categorical Naive Bayes</b>: Categorical Naive Bayes is useful if the features are categorically distributed. We have to encode the categorical variable in the numeric format using the ordinal encoder for using this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "  \n",
    "# store the feature matrix (X) and response vector (y)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "  \n",
    "# splitting X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)\n",
    "  \n",
    "# training the model on training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "  \n",
    "# making predictions on the testing set\n",
    "y_pred = gnb.predict(X_test)\n",
    "  \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred)\n",
    "from sklearn import metrics\n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Naive Bayes Classifiers](https://www.geeksforgeeks.org/naive-bayes-classifiers/)\n",
    "\n",
    "- [Naïve Bayes Classifier Algorithm](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
