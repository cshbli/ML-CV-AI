{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Data Encoding\n",
    "\n",
    "There are different types of data in datasets:\n",
    "\n",
    "- Numerical data:Such as house price, temperature, etc.\n",
    "- Categorical data:Such as Yes/No, Red/Blue/green, etc.\n",
    "    - Ordinal features: such as t-shirt size small, medium and large. \n",
    "    - Nominal features: such as t-shirt color red, green, and blue.     \n",
    "\n",
    "Machine learning model completely works on mathematics and numbers. If our dataset would have a categorical variable, it is necessary to encode these categorical variables into numbers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding\n",
    "\n",
    "This technique is used to encode categorical variables which have a natural rank ordering. Ex. good, very good, excellent could be encoded as 1,2,3.\n",
    "\n",
    "| | Rating|Encoded Rating|\n",
    "|---|---|---|\n",
    "|Rater1|good|1|\n",
    "|Rater2|very good|2|\n",
    "|Rater3|excellent|3|\n",
    "\n",
    "In this technique, each category is assigned an integer value. Ex. Miami is 1, Sydney is 2 and New York is 3. However, it is important to realise that this introduced an ordinality to the data which the ML models will try to use to look for relationships in the data. Therefore, using this data where no ordinal relatinship exists (ranking between the categorical variables) is not a good practice. Maybe as you may have realised already, the example we just used for the cities is actually not a good idea. Because Miami, Sydney and New York do not have any ranking relationship between them. In this case, One-Hot encoder would be a better option which we will see in the next section. Let’s create a better example for ordinal encoding.\n",
    "\n",
    "Ordinal encoding tranformation is available in the scikit-learn library. So let’s use the OrdinalEncoder class to build a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "            Rating\n",
      "Rater 1       good\n",
      "Rater 2  very good\n",
      "Rater 3  excellent\n",
      "\n",
      "Data after encoding:\n",
      "            Rating  Encoded Rating\n",
      "Rater 1       good             1.0\n",
      "Rater 2  very good             2.0\n",
      "Rater 3  excellent             0.0\n"
     ]
    }
   ],
   "source": [
    "# example of a ordinal encoding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# define data\n",
    "data = np.asarray([['good'], ['very good'], ['excellent']])\n",
    "df = pd.DataFrame(data, columns=[\"Rating\"],  index=[\"Rater 1\", \"Rater 2\", \"Rater 3\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# transform data\n",
    "df[\"Encoded Rating\"] = encoder.fit_transform(df)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the encoder assigned the integer values according to the alphabetical order which is the case for text variables. Although we usually do not need to explicitly define the order of the categories, as ML algorihms will be able to extract the relationship anyway, for the sake of this example we can define an explicit order of the `categories` using the `categories` variable of the OrdinalEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "            Rating\n",
      "Rater 1       good\n",
      "Rater 2  very good\n",
      "Rater 3  excellent\n",
      "\n",
      "Data after encoding:\n",
      "            Rating  Encoded Rating\n",
      "Rater 1       good             0.0\n",
      "Rater 2  very good             1.0\n",
      "Rater 3  excellent             2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# define data\n",
    "data = np.asarray([['good'], ['very good'], ['excellent']])\n",
    "df = pd.DataFrame(data, columns=[\"Rating\"], index=[\"Rater 1\", \"Rater 2\", \"Rater 3\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define ordinal encoding\n",
    "categories = [['good', 'very good', 'excellent']]\n",
    "encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "# transform data\n",
    "df[\"Encoded Rating\"] = encoder.fit_transform(df)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "LabelEncoder class from scikit-learn is used to encode the Target labels in the dataset. It actually does exactly the same thing as OrdinalEncoder however expects only a `one-dimensional input` which comes in very handy when encoding the target labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "input_labels = ['x-small', 'small', 'medium', 'large', 'x-large']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Labels = ['small', 'medium', 'large']\n",
      "\n",
      "Encoded values = [2, 1, 0]\n",
      "\n",
      "Encoded values = [3, 0, 4, 1]\n",
      "\n",
      "Decoded labels = ['x-large', 'large', 'x-small', 'medium']\n"
     ]
    }
   ],
   "source": [
    "test_labels = ['small', 'medium', 'large']\n",
    "encoded_values = encoder.transform(test_labels)\n",
    "print(\"\\nLabels =\", test_labels)\n",
    "print(\"\\nEncoded values =\", list(encoded_values))\n",
    "encoded_values = [3,0,4,1]\n",
    "decoded_list = encoder.inverse_transform(encoded_values)\n",
    "print(\"\\nEncoded values =\", encoded_values)\n",
    "print(\"\\nDecoded labels =\", list(decoded_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "This technique is used to encode categorical variables which do not have a natural rank ordering. Ex. Male or female do not have any ordering between them.\n",
    "\n",
    "| | Gender|Male|Female|\n",
    "|---|---|---|---|\n",
    "|Alex|male|1|0|\n",
    "|Joe|male|1|0|\n",
    "|Alice|female|0|1|\n",
    "\n",
    "Like we mentioned previously, for categorical data where there is no ordinal relationship, ordinal encoding is not the suitable technique because it results in making the model look for natural order relationships within the categorical data which does not actually exist which could worsen the model performance.\n",
    "\n",
    "This is where the One-Hot encoding comes into play. This technique works by creating a new column for each unique categorical variable in the data and representing the presence of this category using a binary representation (0 or 1). Looking at the previous example:\n",
    "\n",
    "| |Gender|\n",
    "|---|---|\n",
    "|Alex|male|\n",
    "|Joe|male|\n",
    "|Alice|female|\n",
    "\n",
    "The simple table transforms to the following table where we have a new column repesenting each unique categorical variable (male and female) and a binary value to mark if it exists for that.\n",
    "\n",
    "| |Male|Female|\n",
    "|---|---|---|\n",
    "|Alex|1|0|\n",
    "|Joe|1|0|\n",
    "|Alice|0|1|\n",
    "\n",
    "Just like OrdinalEncoder class, scikit-learn library also provides us with the OneHotEncoder class which we can use to encode categorical data. Let’s use it to encode a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "           City\n",
      "Alex      Miami\n",
      "Joe      Sydney\n",
      "Alice  New York\n",
      "\n",
      "Data after encoding:\n",
      "      Miami New York Sydney\n",
      "Alex    1.0      0.0    0.0\n",
      "Joe     0.0      0.0    1.0\n",
      "Alice   0.0      1.0    0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# define data\n",
    "data = np.asarray([['Miami'], ['Sydney'], ['New York']])\n",
    "df = pd.DataFrame(data, columns=[\"City\"], index=[\"Alex\", \"Joe\", \"Alice\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define one hot encoding\n",
    "categories = [['Miami', 'Sydney', 'New York']]\n",
    "encoder = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "\n",
    "# transform data\n",
    "encoded_data = encoder.fit_transform(df)\n",
    "\n",
    "#fit_transform method return an array, we should convert it to dataframe\n",
    "df_encoded = pd.DataFrame(encoded_data, columns=encoder.categories_, index= df.index)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "           Size\n",
      "shirt0  x-small\n",
      "shirt1    small\n",
      "shirt2   medium\n",
      "shirt3    large\n",
      "shirt4  x-large\n",
      "shirt5  x-small\n",
      "shirt6   medium\n",
      "\n",
      "Data after encoding:\n",
      "       x-small small medium large x-large\n",
      "shirt0     1.0   0.0    0.0   0.0     0.0\n",
      "shirt1     0.0   1.0    0.0   0.0     0.0\n",
      "shirt2     0.0   0.0    1.0   0.0     0.0\n",
      "shirt3     0.0   0.0    0.0   1.0     0.0\n",
      "shirt4     0.0   0.0    0.0   0.0     1.0\n",
      "shirt5     1.0   0.0    0.0   0.0     0.0\n",
      "shirt6     0.0   0.0    1.0   0.0     0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = [['x-small'], ['small'], ['medium'], ['large'], ['x-large'], ['x-small'], ['medium']]\n",
    "df = pd.DataFrame(data, columns=[\"Size\"], index=[\"shirt0\", \"shirt1\", \"shirt2\", \"shirt3\", \"shirt4\", \"shirt5\", \"shirt6\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define one hot encoding\n",
    "categories = [['x-small', 'small', 'medium', 'large', 'x-large']]\n",
    "onehot_encoder = OneHotEncoder(categories=categories, sparse_output=False)\n",
    "\n",
    "#transform data\n",
    "encoded_data = onehot_encoder.fit_transform(df)\n",
    "df_encoded = pd.DataFrame(encoded_data, columns=onehot_encoder.categories_, index= df.index)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the encoder generated a new column for each unique categorical variable and assigned 1 if it exists for that specific sample and 0 if it does not. This is a powerful method to encode non-ordinal categorical data. However, it also has its drawbacks… As you can imagine for dataset with many unique categorical variables, one-hot encoding would result in a huge dataset because each variable has to be represented by a new column. For example, if we had a column/feature with 10.000 unique categorical variables (high cardinality), one-hot encoding would result in 10.000 additional columns resulting in a very sparse matrix and huge increase in memory consumption and computational cost (which is also called the curse of dimensionality). For dealing with categorical features with high cardinality, we can use target encoding…"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "\n",
    "Target encoding or also called mean encoding is a technique where number of occurence of a categorical variable is taken into account along with the target variable to encode the categorical variables into numerical values. Basically, it is a process where we replace the categorical variable with the mean of the target variable. We can explain it better using a simple example dataset…\n",
    "\n",
    "| |Fruit|Target|\n",
    "|---|---|---|\n",
    "|0|Apple|1|\n",
    "|1|Banana|0|\n",
    "|2|Banana|0|\n",
    "|3|Tomato|0|\n",
    "|4|Apple|1|\n",
    "|5|Tomato|1|\n",
    "|6|Apple|0|\n",
    "|7|Banana|1|\n",
    "|8|Tomato|0|\n",
    "|9|Tomato|0|\n",
    "\n",
    "Group the table for each categorical variable to calculated its probability for target = 1:\n",
    "| |Category|Target=0|Target=1|Probability Target=1|\n",
    "|---|---|---|---|---|\n",
    "|0|Apple|1|2|0.66|\n",
    "|1|Banana|2|1|0.33|\n",
    "|2|Tomato|3|1|0.25|\n",
    "\n",
    "Then we take these probabilites that we calculated for target=1, and use it to encode the given categorical variable in the dataset:\n",
    "\n",
    "| |Fruit|Target|Encoded Fruit|\n",
    "|---|---|---|---|\n",
    "|0|Apple|1|0.66|\n",
    "|1|Banana|0|0.33|\n",
    "|2|Banana|0|0.33|\n",
    "|3|Tomato|0|0.25|\n",
    "|4|Apple|1|0.66|\n",
    "|5|Tomato|1|0.25|\n",
    "|6|Apple|0|0.66|\n",
    "|7|Banana|1|0.33|\n",
    "|8|Tomato|0|0.25|\n",
    "|9|Tomato|0|0.25|\n",
    "\n",
    "Similar to ordinal encoding and one-hot encoding, we can use the TargetEncoder class but this time we import it from category_encoders library:\n",
    "\n",
    "category_encoders can be installed with \n",
    "\n",
    "```pip install category_encoders```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "    Fruit  Target\n",
      "0   Apple       1\n",
      "1  Banana       0\n",
      "2  Banana       0\n",
      "3  Tomato       0\n",
      "4   Apple       1\n",
      "5  Tomato       1\n",
      "6   Apple       0\n",
      "7  Banana       1\n",
      "8  Tomato       0\n",
      "9  Tomato       0\n",
      "\n",
      "Data after encoding:\n",
      "    Fruit  Target  Fruit Encoded\n",
      "0   Apple       1       0.666667\n",
      "1  Banana       0       0.333333\n",
      "2  Banana       0       0.333333\n",
      "3  Tomato       0       0.250000\n",
      "4   Apple       1       0.666667\n",
      "5  Tomato       1       0.250000\n",
      "6   Apple       0       0.666667\n",
      "7  Banana       1       0.333333\n",
      "8  Tomato       0       0.250000\n",
      "9  Tomato       0       0.250000\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# define data\n",
    "fruit = [\"Apple\", \"Banana\", \"Banana\", \"Tomato\", \"Apple\", \"Tomato\", \"Apple\", \"Banana\", \"Tomato\", \"Tomato\"]\n",
    "target = [1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
    "df = pd.DataFrame(list(zip(fruit, target)), columns=[\"Fruit\", \"Target\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define target encoding\n",
    "encoder = TargetEncoder(smoothing=-1) #smoothing effect to balance categorical average vs prior.Higher value means stronger regularization.\n",
    "\n",
    "# transform data\n",
    "df[\"Fruit Encoded\"] = encoder.fit_transform(df[\"Fruit\"], df[\"Target\"])\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with `smoothing` parameter to have different encoding results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of target encoding\n",
    "\n",
    "Target encoding is a simple and fast technique and it does not add additional dimensionality to the dataset. Therefore, it is a good encoding method for dataset involving feature with high cardinality (unique categorical variables pof more than 10.000).\n",
    "\n",
    "### Disadvantages of target encoding\n",
    "\n",
    "Target encoding makes use of the distribution of the target variable which can result in overfitting and data leakage. Data leakage in the sense that we are using the target classes to encode the feature may result in rendering the feature in a biased way. This is why there is the smoothing parameter while initializing the class. This parameter helps us reduce this problem (in our example above, we deliberately set it to a very small value to achieve the same results as our hand calculation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Dealing with Categorical Data: Encoding Features for ML Algorithms](https://medium.com/@berk-hakbilen/dealing-with-categorical-data-encoding-categorical-features-for-ml-agorithms-e6ef881e4670)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244b52c780d94b616429aadbde29e2a1db21faa7d2cf5e7aa305dc90879c3934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
