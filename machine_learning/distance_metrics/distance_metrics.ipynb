{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Distance Metrics\n",
    "\n",
    "Distance metrics play an important role in machine learning. They provide a strong foundation for several machine learning algorithms like `k-nearest neighbors` for supervised learning and `k-means clustering` for unsupervised learning. \n",
    "\n",
    "A short list of some of the more popular machine learning algorithms that use distance measures at their core is as follows:\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "- K-Means Clustering\n",
    "- Self-Organizing Map (SOM)\n",
    "- Learning Vector Quantization (LVQ)\n",
    "\n",
    "There are many kernel-based methods may also be considered distance-based algorithms. Perhaps the most widely known kernel method is the support vector machine algorithm, or SVM for short.\n",
    "\n",
    "Different distance metrics are chosen depending upon the type of the data. So, it is important to know the various distance metrics and the intuitions behind it.\n",
    "\n",
    "<img src=\"figs/1_FTVRr_Wqz-3_k6Mk6G4kew.webp\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance - L2 distance\n",
    "\n",
    "Euclidean distance is the straight line distance between two data points in Euclidean space. It is also called as L2 norm or L2 distance.\n",
    "\n",
    "<img src=\"figs/0_mTKOOEj3EOzkr7aA.png\">\n",
    "\n",
    "### Two dimensions\n",
    "\n",
    "If p=(p1, p2) and q=(q1, q2) are two points in the Euclidean space, the Euclidean distance is given by -\n",
    "\n",
    "$$d(p,q)=\\sqrt{(q_1-p_1)^2+(q_2-p_2)^2}$$\n",
    "\n",
    "### Three dimensions\n",
    "\n",
    "If p=(p1, p2, p3), q=(q1, q2, q3) are two points in Euclidean space, the Euclidean distance is given by -\n",
    "\n",
    "$$d(p,q)=\\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+(p_3-q_3)^2}$$\n",
    "\n",
    "### n dimensions\n",
    "\n",
    "If p=(p1, p2…pn) and q=(q1, q2…qn) are two points in Euclidean space, the Euclidean distance is given by -\n",
    "\n",
    "$$d(p,q)=\\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+...+(p_n-q_n)^2} = \\sqrt{\\sum_{i=1}^{n}(p_i-q_i)^2}$$\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. Most commonly used to find the shortest distance between two points in a Euclidean space and also the length of a straight line between two points. Widely used in Machine Learning algorithms.\n",
    "2. Used in clustering algorithms such as K-means, fuzzy c-means clustering, etc.\n",
    "3. Also used as a simple metric to measure the similarity between two data points.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. Although it is a common distance measure, Euclidean distance is not scale in-variant which means that distances computed might be skewed depending on the units of the features. Typically, one needs to normalize the data before using this distance measure.\n",
    "\n",
    "2. Moreover, as the dimensionality increases of your data, the less useful Euclidean distance becomes. This has to do with the curse of dimensionality which relates to the notion that higher-dimensional space does not act as we would, intuitively, expect from 2- or 3-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8284271247461903\n",
      "1.7320508075688772\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "print(distance.euclidean([1, 2, 3], [3, 2, 1]))\n",
    "print(distance.euclidean([1, 0, 0], [0, 1, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan Distance - L1 Distance\n",
    "\n",
    "Manhattan distance between two points in two dimensions is the sum of absolute differences of their cartesian coordinates. Manhattan distance is also called with different names such as rectilinear distance, L1 distance, L1 norm, snake distance, city block distance, etc.\n",
    "\n",
    "<img src=\"figs/1_oByZreebXMIHHST2YytAKg.webp\">\n",
    "\n",
    "In the above figure, Manhattan distances (red, yellow, and blue paths) have the same shortest path length of 12. And Euclidean distance, the green line has length 8.49.\n",
    "\n",
    "### two dimensions\n",
    "\n",
    "If p=(p1, p2) and q=(p1,p2) are two vectors in the plane, the Manhattan distance in 2-D is given by -\n",
    "\n",
    "$$d_1(p,q)=||p_1-q_1||+||p_2-q_2||$$\n",
    "\n",
    "### n dimensions\n",
    "\n",
    "If p=(p1, p2 …. pn) and q=(q1, q2 …. qn) are two vectors in the plane, the Manhattan distance n-D is given by -\n",
    "\n",
    "$$d_1(p,q)=||p-q||=\\sum_{i=1}^{n}||p_i-q_i||$$\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Manhattan distance is preferred over Euclidean distance when dealing with high-dimensional data.\n",
    "\n",
    "- When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "print(distance.cityblock([1, 2, 3], [3, 2, 1]))\n",
    "print(distance.cityblock([1, 0, 0], [0, 1, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minkowski Distance\n",
    "\n",
    "Minkowski distance can be considered as a generalized form of both the Euclidean distance and the Manhattan distance.\n",
    "\n",
    "The Minkowski distance of order p (where p is an integer) between two points X = (x1, x2 … xn) and Y = (y1, y2….yn) is given by:\n",
    "\n",
    "$$d(X,Y)=(\\sum_{i=1}^{n}||x_i-y_i||^p)^{1/p}$$\n",
    "\n",
    "Minkowski distance is typically used with p being 1 or 2, which corresponds to the Manhattan distance and the Euclidean distance, respectively.\n",
    "\n",
    "### Applications\n",
    "\n",
    "As Minkowski distance is a generalized form of Euclidean and Manhattan distance, the uses we just went through applies to Minkowski distance as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.4142135623730951\n",
      "1.2599210498948732\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "print(distance.minkowski([1, 1, 1], [1, 0, 1], 1))\n",
    "print(distance.minkowski([1, 1, 0], [0, 1, 1], 2))\n",
    "print(distance.minkowski([1, 0, 0], [0, 1, 0], 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming Distance\n",
    "\n",
    "It is named after Richard Hamming. The hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. The strings can be letters, bits, or decimal digits, etc.\n",
    "\n",
    "### Applications\n",
    "1. Used in Machine Learning to calculate the similarity between two strings.\n",
    "2. It is used in telecommunications for error detection and correction.\n",
    "3. It is used genetics to calculate the genetic distance.\n",
    "4. Moreover, you can also use Hamming distance to measure the distance between categorical variables.\n",
    "\n",
    "### Disadvantages\n",
    "- As you might expect, hamming distance is difficult to use when two vectors are not of equal length. You would want to compare same-length vectors with each other in order to understand which positions do not match.\n",
    "\n",
    "- Moreover, it does not take the actual value into account as long as they are different or equal. Therefore, it is not advised to use this distance measure when the magnitude is an important measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "print(distance.hamming([1, 0, 0], [1, 1, 0]))\n",
    "print(distance.hamming([1, 0, 0], [1, 1, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance and Cosine Similarity\n",
    "\n",
    "The cosine of two non-zero vectors is given by using the Euclidean dot product formula as below:\n",
    "\n",
    "$$A.B=||A||\\,||B||cos\\theta$$\n",
    "\n",
    "Given two vectors A and B, the cosine similarity, cos(θ), is represented using a dot product and magnitude as below:\n",
    "\n",
    "$$Similarity=cos\\theta=\\dfrac{A.B}{||A||\\,||B||}=\\dfrac{\\sum_{i=1}^{n}A_iB_i}{\\sqrt{\\sum_{i=1}^{n}A_i^2}\\sqrt{\\sum_{i=1}^{n}B_i^2}}$$\n",
    "\n",
    "Cosine similarity is denoted by Cos θ, and cosine distance is given by 1- Cos θ.\n",
    "\n",
    "The cosine similarity value ranges from −1 to 1 (inclusive). \n",
    "- The value of -1 indicates exactly the opposite, \n",
    "- 1 indicates the same, \n",
    "- 0 indicates orthogonality or decorrelation, \n",
    "- and all other values indicate intermediate similarity or dissimilarity.\n",
    "\n",
    "### Application\n",
    "\n",
    "- We use cosine similarity often when we have high-dimensional data and when the magnitude of the vectors is not of importance.\n",
    "- Cosine similarity is used in many places in Machine Learning such as information retrieval, data mining, etc. \n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "One main disadvantage of cosine similarity is that the magnitude of vectors is not taken into account, merely their direction. In practice, this means that the differences in values are not fully taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.29289321881345254\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "print(distance.cosine([100, 0, 0], [0, 1, 0]))\n",
    "print(distance.cosine([1, 1, 0], [0, 1, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev Distance\n",
    "\n",
    "Chebyshev distance is defined as the greatest of difference between two vectors along any coordinate dimension. In other words, it is simply the maximum distance along one axis. Due to its nature, it is often referred to as Chessboard distance since the minimum number of moves needed by a king to go from one square to another is equal to Chebyshev distance.\n",
    "\n",
    "$$D(x,y)=max_i(|x_i-y_i|)$$\n",
    "\n",
    "### Disadvantages\n",
    "Chebyshev is typically used in very specific use-cases, which makes it difficult to use as an all-purpose distance metric, like Euclidean distance or Cosine similarity. For that reason, it is suggested to only use it when you are absolutely sure it suits your use-case.\n",
    "\n",
    "### Use Cases\n",
    "As mentioned before, Chebyshev distance can be used to extract the minimum number of moves needed to go from one square to another. Moreover, it can be a useful measure in games that allow unrestricted 8-way movement.\n",
    "\n",
    "In practice, Chebyshev distance is often used in warehouse logistics as it closely resembles the time an overhead crane takes to move an object."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Index\n",
    "\n",
    "The Jaccard index (or Intersection over Union) is a metric used to calculate the similarity and diversity of sample sets. It is the size of the intersection divided by the size of the union of the sample sets.\n",
    "\n",
    "In practice, it is the total number of similar entities between sets divided by the total number of entities. For example, if two sets have 1 entity in common and there are 5 different entities in total, then the Jaccard index would be 1/5 = 0.2.\n",
    "\n",
    "To calculate the Jaccard distance we simply subtract the Jaccard index from 1:\n",
    "\n",
    "$$D(x,y)=1-\\dfrac{|x \\cap y|}{|x \\cup y|}$$\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "A major disadvantage of the Jaccard index is that it is highly influenced by the size of the data. Large datasets can have a big impact on the index as it could significantly increase the union whilst keeping the intersection similar.\n",
    "\n",
    "### Use-Cases\n",
    "\n",
    "- The Jaccard index is often used in applications where binary or binarized data are used. When you have a deep learning model predicting segments of an image, for instance, a car, the Jaccard index can then be used to calculate how accurate that predicted segment given true labels.\n",
    "\n",
    "- Similarly, it can be used in text similarity analysis to measure how much word choice overlap there is between documents. Thus, it can be used to compare sets of patterns."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sørensen-Dice Index\n",
    "\n",
    "The Sørensen-Dice index is very similar to Jaccard index in that it measures the similarity and diversity of sample sets. Although they are calculated similarly the Sørensen-Dice index is a bit more intuitive because it can be seen as the percentage of overlap between two sets, which is a value between 0 and 1:\n",
    "\n",
    "$$D(x,y)=\\dfrac{2|x \\cap y|}{|x|+|y|}$$\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "The use cases are similar, if not the same, as Jaccard index. You will find it typically used in either image segmentation tasks or text similarity analyses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance\n",
    "\n",
    "Mahalanobis Distance is an effective distance metric that finds the distance between the point and distribution. It works quite effectively on multivariate data because it uses a covariance matrix of variables to find the distance between data points and the center, this means that MD detects outliers based on the distribution pattern of data points, unlike the Euclidean distance that assumes the sample points are distributed about the center of mass in a spherical manner.\n",
    "\n",
    "<img src=\"figs/inbox_6892640_db00cb320c6416adf477510795b322a0_Malahanobis_euclidean.jpeg\">\n",
    "\n",
    "The Mahalanobis distance provides a way to measure how far away an observation is from the center of a sample while accounting for correlations in the data. The Mahalanobis distance is a good way to detect outliers in multivariate normal data. It is better than looking at the univariate z-scores of each coordinate because a multivariate outlier does not necessarily have extreme coordinate values.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figs/MVOutliers1.png\">\n",
    "</p>\n",
    "\n",
    "### Mahalanobis Distance General Equation\n",
    "\n",
    "$$d_M(x,y)=\\sqrt{(x-y)^TS^-1(x-y)}$$\n",
    "\n",
    "The probability density function for a multivariate Gaussian distribution uses the Mahalanobis distance instead of the Euclidean distance.\n",
    "\n",
    "### Mahalanobis Distance with Zero Covariance\n",
    "\n",
    "Assuming no correlation, our covariance matrix is \n",
    "\n",
    "$$\n",
    "S=\n",
    "\\begin{bmatrix}\n",
    "\\sigma _1^2 & 0 \\\\\n",
    "0 & \\sigma _2 ^2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The inverse of a 2×2 matrix can be found using the following:\n",
    "<p align=\"center\">\n",
    "<img src=\"figs/2x2matrixinverse.png\">\n",
    "</p>\n",
    "\n",
    "Applying this to get the inverse of the covariance matrix:\n",
    "<p align=\"center\">\n",
    "<img src=\"figs/covarianceinverse.png\">\n",
    "</p>\n",
    "\n",
    "Mahalanobis equation for variance-normalized distance equation.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"figs/mdist_wo_correlation.png\">\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haversine\n",
    "\n",
    "Haversine distance is the distance between two points on a sphere given their longitudes and latitudes. It is very similar to Euclidean distance in that it calculates the shortest line between two points. The main difference is that no straight line is possible since the assumption here is that the two points are on a sphere.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "One disadvantage of this distance measure is that it is assumed the points lie on a sphere. In practice, this is seldom the case as, for example, the earth is not perfectly round which could make calculation in certain cases difficult. Instead, it would be interesting to look towards <b>Vincenty distance</b> which assumes an ellipsoid instead.\n",
    "\n",
    "### Use Cases\n",
    "As you might have expected, Haversine distance is often used in navigation. For example, you can use it to calculate the distance between two countries when flying between them. Note that it is much less suited if the distances by themselves are already not that large. The curvature will not have that large of an impact."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [5 Most Commonly Used Distance Metrics in Machine Learning](https://pub.towardsai.net/5-most-commonly-used-distance-metrics-in-machine-learning-97c27527b011)\n",
    "\n",
    "- [9 Distance Measures in Data Science](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
