{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Categorical Data: Encoding Features for ML Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "\n",
    "Numerical data, as the name suggests, has features with only numbers (integers or floating-point). On the other hand, categorical data has variables that contain label values (text) and not numerical values. Machine learning models can only accept numerical input variables. What happens if we have a dataset with categorical data instead of numerical data?\n",
    "\n",
    "| | Gender|Height|City|\n",
    "|---|---|---|---|\n",
    "|Alex|male|180|Miami|\n",
    "|Joe|male|172|Sydney|\n",
    "|Alice|female|170|New York|\n",
    "\n",
    "Then we have to convert the data which contains categorical variables to numbers before we can train a ML model. This is called `encoding`. Two most popular encoding techniques are `Ordinal Encoding` and `One-Hot Encoding`.\n",
    "\n",
    "- Ordinal Ecoding: This technique is used to encode categorical variables which habe a natural rank ordering. Ex. good, very good, excellent could be encoded as 1,2,3.\n",
    "\n",
    "| | Rating|Encoded Rating|\n",
    "|---|---|---|\n",
    "|Rater1|good|1|\n",
    "|Rater2|very good|2|\n",
    "|Rater3|excellent|3|\n",
    "\n",
    "- One-Hot Encoding: This technique is used to encode categorical variables which do not have a natural rank ordering. Ex. Male or female do not have any ordering between them.\n",
    "\n",
    "| | Gender|Male|Female|\n",
    "|---|---|---|---|\n",
    "|Alex|male|1|0|\n",
    "|Joe|male|1|0|\n",
    "|Alice|female|0|1|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding\n",
    "\n",
    "In this technique, each category is assigned an integer value. Ex. Miami is 1, Sydney is 2 and New York is 3. However, it is important to realise that this introduced an ordinality to the data which the ML models will try to use to look for relationships in the data. Therefore, using this data where no ordinal relatinship exists (ranking between the categorical variables) is not a good practice. Maybe as you may have realised already, the example we just used for the cities is actually not a good idea. Because Miami, Sydney and New York do not have any ranking relationship between them. In this case, One-Hot encoder would be a better option which we will see in the next section. Let’s create a better example for ordinal encoding.\n",
    "\n",
    "Ordinal encoding tranformation is available in the scikit-learn library. So let’s use the OrdinalEncoder class to build a small example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "            Rating\n",
      "Rater 1       good\n",
      "Rater 2  very good\n",
      "Rater 3  excellent\n",
      "\n",
      "Data after encoding:\n",
      "            Rating  Encoded Rating\n",
      "Rater 1       good             1.0\n",
      "Rater 2  very good             2.0\n",
      "Rater 3  excellent             0.0\n"
     ]
    }
   ],
   "source": [
    "# example of a ordinal encoding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# define data\n",
    "data = np.asarray([['good'], ['very good'], ['excellent']])\n",
    "df = pd.DataFrame(data, columns=[\"Rating\"],  index=[\"Rater 1\", \"Rater 2\", \"Rater 3\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define ordinal encoding\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "# transform data\n",
    "df[\"Encoded Rating\"] = encoder.fit_transform(df)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the encoder assigned the integer values according to the alphabetical order which is the case for text variables. Although we usually do not need to explicitly define the order of the categories, as ML algorihms will be able to extract the relationship anyway, for the sake of this example we can define an explicit order of the `categories` using the `categories` variable of the OrdinalEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "            Rating\n",
      "Rater 1       good\n",
      "Rater 2  very good\n",
      "Rater 3  excellent\n",
      "\n",
      "Data after encoding:\n",
      "            Rating  Encoded Rating\n",
      "Rater 1       good             0.0\n",
      "Rater 2  very good             1.0\n",
      "Rater 3  excellent             2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# define data\n",
    "data = np.asarray([['good'], ['very good'], ['excellent']])\n",
    "df = pd.DataFrame(data, columns=[\"Rating\"], index=[\"Rater 1\", \"Rater 2\", \"Rater 3\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define ordinal encoding\n",
    "categories = [['good', 'very good', 'excellent']]\n",
    "encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "# transform data\n",
    "df[\"Encoded Rating\"] = encoder.fit_transform(df)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "LabelEncoder class from scikit-learn is used to encode the Target labels in the dataset. It actually does exactly the same thing as OrdinalEncoder however expects only a one-dimensional input which comes in very handy when encoding the target labels in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "Like we mentioned previously, for categorical data where there is no ordinal relationship, ordinal encoding is not the suitable technique because it results in making the model look for natural order relationships within the categorical data which does not actually exist which could worsen the model performance.\n",
    "\n",
    "This is where the One-Hot encoding comes into play. This technique works by creating a new column for each unique categorical variable in the data and representing the presence of this category using a binary representation (0 or 1). Looking at the previous example:\n",
    "\n",
    "| |Gender|\n",
    "|---|---|\n",
    "|Alex|male|\n",
    "|Joe|male|\n",
    "|Alice|female|\n",
    "\n",
    "The simple table transforms to the following table where we have a new column repesenting each unique categorical variable (male and female) and a binary value to mark if it exists for that.\n",
    "\n",
    "| |Male|Female|\n",
    "|---|---|---|\n",
    "|Alex|1|0|\n",
    "|Joe|1|0|\n",
    "|Alice|0|1|\n",
    "\n",
    "Just like OrdinalEncoder class, scikit-learn library also provides us with the OneHotEncoder class which we can use to encode categorical data. Let’s use it to encode a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "           City\n",
      "Alex      Miami\n",
      "Joe      Sydney\n",
      "Alice  New York\n",
      "\n",
      "Data after encoding:\n",
      "      Miami New York Sydney\n",
      "Alex    1.0      0.0    0.0\n",
      "Joe     0.0      0.0    1.0\n",
      "Alice   0.0      1.0    0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# define data\n",
    "data = np.asarray([['Miami'], ['Sydney'], ['New York']])\n",
    "df = pd.DataFrame(data, columns=[\"City\"], index=[\"Alex\", \"Joe\", \"Alice\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define one hot encoding\n",
    "categories = [['Miami', 'Sydney', 'New York']]\n",
    "encoder = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "\n",
    "# transform data\n",
    "encoded_data = encoder.fit_transform(df)\n",
    "\n",
    "#fit_transform method return an array, we should convert it to dataframe\n",
    "df_encoded = pd.DataFrame(encoded_data, columns=encoder.categories_, index= df.index)\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the encoder generated a new column for each unique categorical variable and assigned 1 if it exists for that specific sample and 0 if it does not. This is a powerful method to encode non-ordinal categorical data. However, it also has its drawbacks… As you can imagine for dataset with many unique categorical variables, one-hot encoding would result in a huge dataset because each variable has to be represented by a new column. For example, if we had a column/feature with 10.000 unique categorical variables (high cardinality), one-hot encoding would result in 10.000 additional columns resulting in a very sparse matrix and huge increase in memory consumption and computational cost (which is also called the curse of dimensionality). For dealing with categorical features with high cardinality, we can use target encoding…"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "\n",
    "Target encoding or also called mean encoding is a technique where number of occurence of a categorical variable is taken into account along with the target variable to encode the categorical variables into numerical values. Basically, it is a process where we replace the categorical variable with the mean of the target variable. We can explain it better using a simple example dataset…\n",
    "\n",
    "| |Fruit|Target|\n",
    "|---|---|---|\n",
    "|0|Apple|1|\n",
    "|1|Banana|0|\n",
    "|2|Banana|0|\n",
    "|3|Tomato|0|\n",
    "|4|Apple|1|\n",
    "|5|Tomato|1|\n",
    "|6|Apple|0|\n",
    "|7|Banana|1|\n",
    "|8|Tomato|0|\n",
    "|9|Tomato|0|\n",
    "\n",
    "Group the table for each categorical variable to calculated its probability for target = 1:\n",
    "| |Category|Target=0|Target=1|Probability Target=1|\n",
    "|---|---|---|---|---|\n",
    "|0|Apple|1|2|0.66|\n",
    "|1|Banana|2|1|0.33|\n",
    "|2|Tomato|3|1|0.25|\n",
    "\n",
    "Then we take these probabilites that we calculated for target=1, and use it to encode the given categorical variable in the dataset:\n",
    "\n",
    "| |Fruit|Target|Encoded Fruit|\n",
    "|---|---|---|---|\n",
    "|0|Apple|1|0.66|\n",
    "|1|Banana|0|0.33|\n",
    "|2|Banana|0|0.33|\n",
    "|3|Tomato|0|0.25|\n",
    "|4|Apple|1|0.66|\n",
    "|5|Tomato|1|0.25|\n",
    "|6|Apple|0|0.66|\n",
    "|7|Banana|1|0.33|\n",
    "|8|Tomato|0|0.25|\n",
    "|9|Tomato|0|0.25|\n",
    "\n",
    "Similar to ordinal encoding and one-hot encoding, we can use the TargetEncoder class but this time we import it from category_encoders library:\n",
    "\n",
    "category_encoders can be installed with \n",
    "\n",
    "```pip install category_encoders```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before encoding:\n",
      "    Fruit  Target\n",
      "0   Apple       1\n",
      "1  Banana       0\n",
      "2  Banana       0\n",
      "3  Tomato       0\n",
      "4   Apple       1\n",
      "5  Tomato       1\n",
      "6   Apple       0\n",
      "7  Banana       1\n",
      "8  Tomato       0\n",
      "9  Tomato       0\n",
      "\n",
      "Data after encoding:\n",
      "    Fruit  Target  Fruit Encoded\n",
      "0   Apple       1       0.666667\n",
      "1  Banana       0       0.333333\n",
      "2  Banana       0       0.333333\n",
      "3  Tomato       0       0.250000\n",
      "4   Apple       1       0.666667\n",
      "5  Tomato       1       0.250000\n",
      "6   Apple       0       0.666667\n",
      "7  Banana       1       0.333333\n",
      "8  Tomato       0       0.250000\n",
      "9  Tomato       0       0.250000\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# define data\n",
    "fruit = [\"Apple\", \"Banana\", \"Banana\", \"Tomato\", \"Apple\", \"Tomato\", \"Apple\", \"Banana\", \"Tomato\", \"Tomato\"]\n",
    "target = [1, 0, 0, 0, 1, 1, 0, 1, 0, 0]\n",
    "df = pd.DataFrame(list(zip(fruit, target)), columns=[\"Fruit\", \"Target\"])\n",
    "print(\"Data before encoding:\")\n",
    "print(df)\n",
    "\n",
    "# define target encoding\n",
    "encoder = TargetEncoder(smoothing=-1) #smoothing effect to balance categorical average vs prior.Higher value means stronger regularization.\n",
    "\n",
    "# transform data\n",
    "df[\"Fruit Encoded\"] = encoder.fit_transform(df[\"Fruit\"], df[\"Target\"])\n",
    "print(\"\\nData after encoding:\")\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with `smoothing` parameter to have different encoding results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of target encoding\n",
    "\n",
    "Target encoding is a simple and fast technique and it does not add additional dimensionality to the dataset. Therefore, it is a good encoding method for dataset involving feature with high cardinality (unique categorical variables pof more than 10.000).\n",
    "\n",
    "### Disadvantages of target encoding\n",
    "\n",
    "Target encoding makes use of the distribution of the target variable which can result in overfitting and data leakage. Data leakage in the sense that we are using the target classes to encode the feature may result in rendering the feature in a biased way. This is why there is the smoothing parameter while initializing the class. This parameter helps us reduce this problem (in our example above, we deliberately set it to a very small value to achieve the same results as our hand calculation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Dealing with Categorical Data: Encoding Features for ML Algorithms](https://medium.com/@berk-hakbilen/dealing-with-categorical-data-encoding-categorical-features-for-ml-agorithms-e6ef881e4670)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, May 17 2022, 12:55:41) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "244b52c780d94b616429aadbde29e2a1db21faa7d2cf5e7aa305dc90879c3934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
