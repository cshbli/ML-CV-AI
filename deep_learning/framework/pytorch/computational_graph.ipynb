{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Computational Graphs\n",
    "\n",
    "Modern neural network architectures can have millions of learnable parameters. From a computational point of view, training a neural network consists of two phases:\n",
    "\n",
    "* A forward pass to compute the value of the loss function.\n",
    "\n",
    "* A backward pass to compute the gradients of the learnable parameters.\n",
    "\n",
    "The forward pass is pretty straight forward. The output of one layer is the input to the next and so forth.\n",
    "\n",
    "Backward pass is a bit more complicated since it requires us to use the chain rule to compute the gradients of weights w.r.t to the loss function.\n",
    "\n",
    "In PyTorch: \n",
    "\n",
    "* The autograd package provides automatic differentiation to automate the computation of the backward passes in neural networks. \n",
    "\n",
    "* The forward pass of your network defines the computational graph; \n",
    "\n",
    "    * nodes in the graph are Tensors\n",
    "\n",
    "    * edges are functions that produced the output Tensors from input Tensors. \n",
    "    \n",
    "    * Back-propagation through this graph then gives the gradients.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors: Basic Building Blocks of PyTorch\n",
    "\n",
    "Tensor is a data structure which is a fundamental building block of PyTorch. Tensors are pretty much like numpy arrays, except that unlike numpy, tensors are designed to take advantage of parallel computation capabilities of a GPU. A lot of Tensor syntax is similar to that of numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.6977e+02, 4.5836e-41, 4.6155e+02, 4.5836e-41, 4.7412e+02],\n",
      "        [4.5836e-41, 4.6894e+02, 4.5836e-41, 4.6893e+02, 4.5836e-41],\n",
      "        [4.6156e+02, 4.5836e-41, 4.7194e+02, 4.5836e-41, 4.6157e+02]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor(3, 5)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaf Tensor\n",
    "\n",
    "All Tensors that have `requires_grad` set to `False` will be leaf Tensors by convention. For Tensors that have `requires_grad` which is `True`, they will be leaf Tensors if they were created by the user(Eg. weights of your neural network). This means that they are not the result of an operation and so grad_fn is None.\n",
    "\n",
    "Basically, if require_grad is False then it will be a leaf tensor. Moreover, if requires_grad is True and it is created by user, it is also a leaf tensor. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requires_grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One it's own, Tensor is just like a numpy ndarray. A data structure that can let you do fast linear algebra options. \n",
    "\n",
    "Every Tensor in PyTorch has a flag: `required_grad` that allows for fine-grained exclusion of subgraphs from gradient computation and can increase efficiency. If x is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of x with respect to some scalar value.    \n",
    "\n",
    "The API can be a bit confusing here. There are multiple ways to initialise tensors in PyTorch. While some ways can let you explicitly define that the `requires_grad` in the constructor itself, others require you to set it manually after creation of the Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn((3,3), requires_grad = True) \n",
    "\n",
    "t2 = torch.FloatTensor(3,3) # No way to specify requires_grad while initiating \n",
    "t2.requires_grad = True\n",
    "\n",
    "print(t2.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requires_grad` is contagious. It means that when a Tensor is created by operating on other Tensors, the requires_grad of the resultant Tensor would be set True given at least one of the tensors used for creation has it's `requires_grad` set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3,3) # requires_grad=False by default\n",
    "y = torch.randn(3,3) #requires_grad=False by default\n",
    "z = torch.randn((3,3),requires_grad=True)\n",
    "a = x+y # since both x and y don't require gradients, a also doesn't require gradients\n",
    "print(a.requires_grad) #output: False\n",
    "b = a+z #since z requires gradient, b also requires gradient\n",
    "print(b.requires_grad) #output: True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the above example, if there is a single input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grad_fn\n",
    "\n",
    "Each Tensor has an attribute called `grad_fn`, which refers to the mathematical operator that create the variable. If `requires_grad` is set to False, `grad_fn` would be None. \n",
    "\n",
    "If a Tensor is a leaf node (initialised by the user), then the `grad_fn` is also None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "<AddBackward0 object at 0x7fc581b8c100>\n"
     ]
    }
   ],
   "source": [
    "print(x.grad_fn)    # x is a leaf node, no grad_fn\n",
    "print(y.grad_fn)    # y is a leaf node, no grad_fn\n",
    "print(z.grad_fn)    # z is a leaf node, no grad_fn\n",
    "print(a.grad_fn)    # a's requires_grad is False, no grad_fn\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "All mathematical operations in PyTorch are implemented by the `torch.autograd.Function` class. This class has two important member functions we need to look at. \n",
    "\n",
    "1. The first is it's `forward`  function, which simply computes the output using it's inputs. \n",
    "\n",
    "2. The `backward` function takes the incoming gradient coming from the the part of the network in front of it. \n",
    "\n",
    "These concepts can be represented as following diagram.\n",
    "\n",
    "<img src=\"figs/0_p9_fUhKXCf0LWAxh.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that PyTorch gives an error if you call `backward` on vector-valued Tensor. This means you can only call `backward` on a scalar valued Tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m d \u001b[39m=\u001b[39m w3\u001b[39m*\u001b[39mb \u001b[39m+\u001b[39m w4\u001b[39m*\u001b[39mc \n\u001b[1;32m     15\u001b[0m L \u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m \u001b[39m-\u001b[39m d)\n\u001b[0;32m---> 17\u001b[0m L\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[0;32m~/venv/torch1.13.0/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/venv/torch1.13.0/lib/python3.8/site-packages/torch/autograd/__init__.py:190\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    186\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    187\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    189\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 190\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/venv/torch1.13.0/lib/python3.8/site-packages/torch/autograd/__init__.py:85\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     84\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 85\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     87\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "L = (10 - d)\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to overcome this.\n",
    "\n",
    "1. If you just make a small change in the above code setting L to be the sum of all the errors, our problem will be solved\n",
    "\n",
    "2. Second way is, for some reason have to absolutely call backward on a vector function, you can pass a torch.ones of size of shape of the tensor you are trying to call backward with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "# Replace L = (10 - d) by \n",
    "L = (10 -d).sum()\n",
    "\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "a = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)\n",
    "w2 = torch.randn((3,3), requires_grad = True)\n",
    "w3 = torch.randn((3,3), requires_grad = True)\n",
    "w4 = torch.randn((3,3), requires_grad = True)\n",
    "\n",
    "b = w1*a \n",
    "c = w2*a\n",
    "\n",
    "d = w3*b + w4*c \n",
    "\n",
    "# Replace L = (10 - d) by \n",
    "L = (10 -d)\n",
    "\n",
    "L.backward(torch.ones(L.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, we can have gradients for every Tensor , and we can update them using Optimisation algorithm of our choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "w1 = w1 - learning_rate * w1.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Computation Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch creates something called a <b>Dynamic Computation Graph</b>, which means that the graph is generated on the fly.\n",
    "\n",
    "Until the forward function of a Variable is called, there exists no node for the Tensor (it’s grad_fn) in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((3,3), requires_grad = True)   #No graph yet, as a is a leaf\n",
    "\n",
    "w1 = torch.randn((3,3), requires_grad = True)  #Same logic as above\n",
    "\n",
    "b = w1*a   #Graph with node `mulBackward` is created."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is created as a result of `forward` function of many Tensors being invoked. Only then, the buffers for the non-leaf nodes allocated for the graph and intermediate values (used for computing gradients later).  When you call `backward`, as the gradients are computed, these buffers (for non-leaf variables) are essentially freed, and the graph is destroyed ( In a sense, you can't backpropagate through it since the buffers holding values to compute the gradients are gone).\n",
    "\n",
    "Next time, you will call `forward` on the same set of tensors, the leaf node buffers from the previous run will be shared, while the non-leaf nodes buffers will be created again.\n",
    "\n",
    "If you call backward more than once on a graph with non-leaf nodes, you'll be met with the following error.\n",
    "\n",
    "```text\n",
    "RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the non-leaf buffers gets destroyed the first time `backward()` is called and hence, there’s no path to navigate to the leaves when backward is invoked the second time. You can undo this non-leaf buffer destroying behaviour by adding `retain_graph = True` argument to the backward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward(retain_graph = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.no_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are computing gradients, we need to cache input values, and intermediate features as they maybe required to compute the gradient later. This affects the memory footprint of the network.\n",
    "\n",
    "While, we are performing inference, we don't compute gradients, and thus, don't need to store these values. Infact, no graph needs to be create during inference as it will lead to useless consumption of memory.\n",
    "\n",
    "PyTorch offers a context manager, called `torch.no_grad()` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inference code goes here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No graph is defined for operations executed under this context manager."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd \n",
    "\n",
    "Conceptually, autograd keeps a graph recording of all of the operations that created the data as you execute operations, giving you a `directed acyclic graph` whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the `chain rule (back-propagation)`.\n",
    "\n",
    "Every primitive autograd operator is two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar and computes the gradient of the input Tensors with respect to that same scalar.\n",
    "\n",
    "To summarize, Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of the computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user since their grad_fn is None). If you want to compute the derivatives, you can call `.backward()` on a Tensor. After the call to the backwards function the gradient values are stored as tensors in `grad` attribute.\n",
    "\n",
    "So for example if you create two Tensors a and b. Followed by c = a/b. The `grad_fn` of c would be `DivBackward` which is the backward function for the / operator. And as discussed earlier a collection of these `grad_fn` makes the backward graph. The forward and backward function are a member of `torch.autograd.Function`. You can define your own autograd operator by defining a subclass of torch.autograd.Function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_leaf() and retain_grad()\n",
    "\n",
    "is_leaf: All Tensors that have `requires_grad` which is False are leaf Tensors by convention. For Tensors that have `requires_grad` with is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so `grad_fn` is None. Only leaf Tensors have their grad populated during a call to `backward()`. To get grad populated for non-leaf Tensors, you can use `retain_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the graph a,b,c,d are leaf nodes and e is the root node\n",
    "# The graph is constructed with every line since the \n",
    "# computational graphs are dynamic in PyTorch\n",
    "a = torch.tensor([2.0],requires_grad=True)\n",
    "b = torch.tensor([3.0],requires_grad=True)\n",
    "c = torch.tensor([5.0],requires_grad=True)\n",
    "d = torch.tensor([10.0],requires_grad=True)\n",
    "u = a*b\n",
    "t = torch.log(d)\n",
    "v = t*c\n",
    "t.retain_grad()\n",
    "e = u+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is leaf: True\n",
      "a grad_fn: None\n",
      "a grad: None\n",
      "\n",
      "e is leaf: False\n",
      "e grad_fn: <AddBackward0 object at 0x7fc581aef2e0>\n",
      "e grad: None\n",
      "\n",
      "t is leaf: False\n",
      "t grad_fn: <LogBackward0 object at 0x7fc581aef2e0>\n",
      "t grad: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_463788/3089450936.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(f\"e grad: {e.grad}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"a is leaf: {a.is_leaf}\")\n",
    "print(f\"a grad_fn: {a.grad_fn}\")\n",
    "print(f\"a grad: {a.grad}\")\n",
    "print()\n",
    "\n",
    "print(f\"e is leaf: {e.is_leaf}\")\n",
    "print(f\"e grad_fn: {e.grad_fn}\")\n",
    "print(f\"e grad: {e.grad}\")\n",
    "print()\n",
    "\n",
    "print(f\"t is leaf: {t.is_leaf}\")\n",
    "print(f\"t grad_fn: {t.grad_fn}\")\n",
    "print(f\"t grad: {t.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaves don’t have grad_fn but will have gradients. Non leaf nodes have grad_fn but don’t have gradients. Before the backward() is called there are no grad values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial a} = 3.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial b} = 2.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial c} = 2.3025851249694824$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial d} = 0.5$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "e.backward()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial a}} = {a.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial b}} = {b.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial c}} = {c.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial d}} = {d.grad.item()}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is leaf: True\n",
      "a grad_fn: None\n",
      "a grad: tensor([3.])\n",
      "\n",
      "e is leaf: False\n",
      "e grad_fn: <AddBackward0 object at 0x7fc5819ad5b0>\n",
      "e grad: None\n",
      "\n",
      "t is leaf: False\n",
      "t grad_fn: <LogBackward0 object at 0x7fc5819ad5b0>\n",
      "t grad: tensor([5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_463788/3089450936.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(f\"e grad: {e.grad}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"a is leaf: {a.is_leaf}\")\n",
    "print(f\"a grad_fn: {a.grad_fn}\")\n",
    "print(f\"a grad: {a.grad}\")\n",
    "print()\n",
    "\n",
    "print(f\"e is leaf: {e.is_leaf}\")\n",
    "print(f\"e grad_fn: {e.grad_fn}\")\n",
    "print(f\"e grad: {e.grad}\")\n",
    "print()\n",
    "\n",
    "print(f\"t is leaf: {t.is_leaf}\")\n",
    "print(f\"t grad_fn: {t.grad_fn}\")\n",
    "print(f\"t grad: {t.grad}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [PyTorch 101, Part 1: Understanding Graphs, Automatic Differentiation and Autograd](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
