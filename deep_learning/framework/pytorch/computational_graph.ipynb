{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Computational Graphs\n",
    "\n",
    "PyTorch computational graphs are dynamic graphs.\n",
    "\n",
    "A PyTorch Tensor it nothing but an n-dimensional array. The framework provides a lot of functions for operating on these Tensors.\n",
    "\n",
    "In PyTorch: \n",
    "* The autograd package provides automatic differentiation to automate the computation of the backward passes in neural networks. \n",
    "\n",
    "* The forward pass of your network defines the computational graph; \n",
    "\n",
    "    * nodes in the graph are Tensors\n",
    "\n",
    "    * edges are functions that produced the output Tensors from input Tensors. \n",
    "    \n",
    "    * Back-propagation through this graph then gives the gradients.\n",
    "\n",
    "Every Tensor in PyTorch has a flag: `required_grad` that allows for fine-grained exclusion of subgraphs from gradient computation and can increase efficiency. If x is a Tensor that has `x.requires_grad=True` then x.grad is another Tensor holding the gradient of x with respect to some scalar value.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3,3) # requires_grad=False by default\n",
    "y = torch.randn(3,3) #requires_grad=False by default\n",
    "z = torch.randn((3,3),requires_grad=True)\n",
    "a = x+y # since both x and y don't require gradients, a also doesn't require gradients\n",
    "print(a.requires_grad) #output: False\n",
    "b = a+z #since z requires gradient, b also requires gradient\n",
    "print(b.requires_grad) #output: True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the above example, if there is a single input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd \n",
    "\n",
    "Conceptually, autograd keeps a graph recording of all of the operations that created the data as you execute operations, giving you a `directed acyclic graph` whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the `chain rule (back-propagation)`.\n",
    "\n",
    "Internally, autograd represents this graph as a graph of Function objects, which can be apply()-ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph). When the forward pass completed, the graph is evaluated in the backwards pass to compute the gradients.\n",
    "\n",
    "The computational graphs in PyTorch are dynamic and thus are recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training — what you run is what you differentiate.\n",
    "\n",
    "Every primitive autograd operator is two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar and computes the gradient of the input Tensors with respect to that same scalar.\n",
    "\n",
    "To summarize, Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of the computation. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor (except for Tensors created by the user since their grad_fn is None). If you want to compute the derivatives, you can call .backward() on a Tensor. After the call to the backwards function the gradient values are stored as tensors in grad attribute.\n",
    "\n",
    "These concepts can be represented as following diagram.\n",
    "\n",
    "<img src=\"figs/0_p9_fUhKXCf0LWAxh.png\">\n",
    "\n",
    "So for example if you create two Tensors a and b. Followed by c = a/b. The grad_fn of c would be DivBackward which is the backward function for the / operator. And as discussed earlier a collection of these grad_fn makes the backward graph. The forward and backward function are a member of torch.autograd.Function. You can define your own autograd operator by defining a subclass of torch.autograd.Function.\n",
    "\n",
    "is_leaf: All Tensors that have requires_grad which is False are leaf Tensors by convention. For Tensors that have requires_grad with is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the graph a,b,c,d are leaf nodes and e is the root node\n",
    "# The graph is constructed with every line since the \n",
    "# computational graphs are dynamic in PyTorch\n",
    "a = torch.tensor([2.0],requires_grad=True)\n",
    "b = torch.tensor([3.0],requires_grad=True)\n",
    "c = torch.tensor([5.0],requires_grad=True)\n",
    "d = torch.tensor([10.0],requires_grad=True)\n",
    "u = a*b\n",
    "t = torch.log(d)\n",
    "v = t*c\n",
    "t.retain_grad()\n",
    "e = u+v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "None\n",
      "\n",
      "False\n",
      "<AddBackward0 object at 0x7f86305835e0>\n",
      "None\n",
      "\n",
      "False\n",
      "<LogBackward0 object at 0x7f86305835e0>\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14768/580164801.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(e.grad)\n"
     ]
    }
   ],
   "source": [
    "print(a.is_leaf)\n",
    "print(a.grad_fn)\n",
    "print(a.grad)\n",
    "print()\n",
    "\n",
    "print(e.is_leaf)\n",
    "print(e.grad_fn)\n",
    "print(e.grad)\n",
    "print()\n",
    "\n",
    "print(t.is_leaf)\n",
    "print(t.grad_fn)\n",
    "print(t.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaves don’t have grad_fn but will have gradients. Non leaf nodes have grad_fn but don’t have gradients. Before the backward() is called there are no grad values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial a} = 3.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial b} = 2.0$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial c} = 2.3025851249694824$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\partial e}{\\partial d} = 0.5$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Math\n",
    "\n",
    "e.backward()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial a}} = {a.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial b}} = {b.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial c}} = {c.grad.item()}'))\n",
    "print()\n",
    "display(Math(fr'\\frac{{\\partial e}}{{\\partial d}} = {d.grad.item()}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n",
      "tensor([3.])\n",
      "\n",
      "False\n",
      "<AddBackward0 object at 0x7f86305a3e20>\n",
      "None\n",
      "\n",
      "False\n",
      "<LogBackward0 object at 0x7f86305a3e20>\n",
      "tensor([5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14768/580164801.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:480.)\n",
      "  print(e.grad)\n"
     ]
    }
   ],
   "source": [
    "print(a.is_leaf)\n",
    "print(a.grad_fn)\n",
    "print(a.grad)\n",
    "print()\n",
    "\n",
    "print(e.is_leaf)\n",
    "print(e.grad_fn)\n",
    "print(e.grad)\n",
    "print()\n",
    "\n",
    "print(t.is_leaf)\n",
    "print(t.grad_fn)\n",
    "print(t.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
