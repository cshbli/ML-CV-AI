{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nn.Module vs nn.Functional\n",
    "\n",
    "In PyTorch, layers are often implemented as either one of `torch.nn.Module` objects or `torch.nn.Functional` functions. Which one to use? Which is better?\n",
    "\n",
    "`torch.nn.Module` is basically the cornernstone of PyTorch. The way it works is you first define an `nn.Module` object, and then invoke it's forward method to run it. This is a Object Oriented way of doing things.\n",
    "\n",
    "On the other hand, `nn.functional` provides some layers / activations in form of functions that can be directly called on the input rather than defining the an object. For example, in order to rescale an image tensor, you call `torch.nn.functional.interpolate` on an image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "inp = torch.randn(1,3,64,64)     # random input image\n",
    "\n",
    "# Same thing using two approaches\n",
    "# ---------------------------------------\n",
    "\n",
    "# torch.nn\n",
    "avg_pool = nn.AvgPool2d(4)     # create an object\n",
    "nn_out = avg_pool(inp)         # invoke the forward method\n",
    "\n",
    "# torch.nn.Functional\n",
    "f_out = F.avg_pool2d(inp, 4)\n",
    "\n",
    "\n",
    "print (torch.equal(nn_out, f_out))        # check whether the same result is produced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful or Stateless"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, any layer can be seen as a function. For example, a convolutional operation is just a bunch of multiplication and addition operations. From a programmatical angle, a layer is more than function. It also needs to hold data, which changes as we train our network.\n",
    "\n",
    "For the layers which hold data, we would prefer to use the `nn.Module` objects where we have weights or other states which might define the behaviour of the layer. For example, a dropout / Batch Norm layer behaves differently during training and inference.\n",
    "\n",
    "On the other hand, where no state or weights are required, one could use the `nn.functional`. Examples being, resizing (nn.functional.interpolate), average pooling (nn.functional.AvgPool2d).\n",
    "\n",
    "Despite the above reasoning, most of the `nn.Module` classes have their `nn.functional` counterparts. However, the above line of reasoning is to be respected during practical work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
