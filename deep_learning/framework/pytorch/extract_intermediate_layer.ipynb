{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract an Intermediate Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Registering a forward hook on a certain layer of the network\n",
    "\n",
    "* define one `get_activation()` function\n",
    "\n",
    "    ``` python\n",
    "    activation = {}\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    ```\n",
    "\n",
    "* register_forward_hook on the layer you want to extract\n",
    "\n",
    "    ``` python\n",
    "    model.fc2.register_forward_hook(get_activation('fc2'))\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0114, -0.0545, -0.1958, -0.0217,  0.0052, -0.0270, -0.1623, -0.0467,\n",
      "          0.0420,  0.0792, -0.0398, -0.1767, -0.0108, -0.0066,  0.0003, -0.0294,\n",
      "          0.1365,  0.2850,  0.0139,  0.0335,  0.0147,  0.0164, -0.1176,  0.0268,\n",
      "          0.0120,  0.0711,  0.0201,  0.0755, -0.0263,  0.1325,  0.0510,  0.0175,\n",
      "         -0.0562, -0.0440, -0.0282,  0.2546,  0.0930,  0.2249,  0.0215, -0.0762,\n",
      "         -0.2013,  0.0587,  0.0408, -0.1045,  0.0395, -0.0668, -0.0763,  0.0857,\n",
      "          0.1199, -0.0237,  0.0630, -0.1114,  0.2050,  0.0433, -0.0524,  0.0951,\n",
      "          0.0485,  0.1931,  0.0054, -0.1557, -0.1906, -0.1204, -0.0229, -0.1178,\n",
      "         -0.0255, -0.1037,  0.0319, -0.0813,  0.0408, -0.0681, -0.0855, -0.1064,\n",
      "          0.1218,  0.0064, -0.0844, -0.0134, -0.0517,  0.1180, -0.0070, -0.0217,\n",
      "          0.0549, -0.0024,  0.0295, -0.0462]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.cl1 = nn.Linear(25, 60)\n",
    "        self.cl2 = nn.Linear(60, 16)\n",
    "        self.fc1 = nn.Linear(16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cl1(x))\n",
    "        x = F.relu(self.cl2(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "model.fc2.register_forward_hook(get_activation('fc2'))\n",
    "x = torch.randn(1, 25)\n",
    "output = model(x)\n",
    "print(activation['fc2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0593, -0.0217, -0.1351, -0.0384,  0.0221, -0.0265, -0.1854, -0.0423,\n",
      "         -0.0430,  0.0263, -0.0571, -0.1501,  0.0137,  0.0437,  0.0296, -0.0021,\n",
      "          0.1003,  0.2045,  0.0583,  0.0202,  0.0149,  0.0095, -0.1051, -0.0425,\n",
      "         -0.0082,  0.0107,  0.0240,  0.0963, -0.0122,  0.0945,  0.0904,  0.0995,\n",
      "         -0.0547, -0.0915, -0.0320,  0.2111,  0.1059,  0.2046,  0.0776, -0.0775,\n",
      "         -0.1315,  0.0776, -0.0063, -0.0944,  0.0301, -0.0324, -0.0726,  0.0483,\n",
      "          0.0622, -0.0694,  0.1037, -0.1082,  0.1846,  0.0397, -0.0064,  0.0464,\n",
      "          0.0380,  0.1812, -0.0057, -0.1008, -0.1975, -0.1179, -0.0587, -0.0972,\n",
      "         -0.0103, -0.0964,  0.0199, -0.1028, -0.0028,  0.0164, -0.0622, -0.0736,\n",
      "          0.0811, -0.0414, -0.0015, -0.1005, -0.0189,  0.0445, -0.0213,  0.0140,\n",
      "          0.0217,  0.0507,  0.0098, -0.0195]])\n",
      "tensor([[ 0.0728, -0.5055,  0.0482, -0.8321,  0.0047, -0.3677,  0.2545, -0.4243,\n",
      "          0.8094, -0.7969, -0.3151, -0.3689,  0.4332, -0.1288,  0.0800, -0.4333,\n",
      "         -0.5357,  0.0528,  0.1746,  0.6271,  0.2883, -0.4899,  0.5083, -0.4224,\n",
      "         -0.0236,  0.5089, -0.3609, -0.3143, -1.0274,  0.0354, -1.1223, -0.2669,\n",
      "         -0.5385,  0.6743, -0.0220,  0.1556,  0.3669,  0.2014, -0.2013, -0.8865,\n",
      "         -0.7651,  0.2093, -0.2195,  0.4664,  0.1175, -0.5554,  0.8362, -0.0082,\n",
      "         -0.0557,  0.0183,  0.0992, -0.6036,  0.1204,  0.1336,  0.7501,  0.4020,\n",
      "         -0.1304, -0.0889, -0.1712, -0.6307]])\n",
      "tensor([[-0.0546,  0.1724, -0.2104,  0.2149, -0.2812,  0.1451, -0.0742, -0.1476,\n",
      "          0.2153, -0.0999, -0.3650,  0.0256,  0.2763, -0.3599,  0.1054, -0.3485]])\n",
      "tensor([[-0.0147,  0.1662,  0.0718,  0.2022,  0.0485,  0.2435,  0.2139,  0.1467,\n",
      "          0.2400,  0.0831, -0.1181,  0.0026,  0.1765, -0.2432, -0.1232, -0.0849,\n",
      "         -0.0177, -0.1921,  0.1592, -0.1135,  0.0034,  0.2798,  0.0985, -0.0330,\n",
      "          0.1116,  0.0654, -0.0507, -0.3148,  0.2867,  0.0595,  0.2936,  0.2137,\n",
      "          0.2647, -0.0952,  0.0282, -0.2252,  0.0631, -0.0083, -0.1674, -0.0775,\n",
      "         -0.3250,  0.2383, -0.1878, -0.3764,  0.0114, -0.2811,  0.0088, -0.1145,\n",
      "         -0.1801, -0.2493,  0.0336,  0.0009,  0.1816,  0.0017,  0.0653, -0.1254,\n",
      "          0.1532, -0.0537, -0.1060,  0.1005, -0.1270,  0.0978, -0.0394, -0.1652,\n",
      "         -0.2801,  0.0552,  0.0602,  0.1805, -0.0359, -0.2206, -0.0088, -0.0139,\n",
      "         -0.0758, -0.0980,  0.1348, -0.0746,  0.3663, -0.0813,  0.0553, -0.0073,\n",
      "         -0.0316,  0.1293, -0.1342,  0.1575,  0.2683, -0.1968, -0.1041, -0.0052,\n",
      "         -0.1185, -0.3342,  0.0006,  0.1156,  0.2781,  0.0867,  0.0435,  0.1849,\n",
      "         -0.2720,  0.1647, -0.1633, -0.1152,  0.1977,  0.2506,  0.1962, -0.1441,\n",
      "          0.0963, -0.1147, -0.2380,  0.0933, -0.2519, -0.3448, -0.0922, -0.1176,\n",
      "         -0.1617, -0.0793, -0.1174,  0.3419, -0.0502,  0.2365,  0.0552,  0.3049]])\n",
      "tensor([[-0.0465,  0.0092, -0.1411, -0.0706, -0.0871, -0.1096, -0.1170, -0.0870,\n",
      "          0.0798,  0.0074]])\n",
      "tensor([[-2.2950, -2.2393, -2.3895, -2.3191, -2.3356, -2.3581, -2.3655, -2.3355,\n",
      "         -2.1687, -2.2411]])\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "x = torch.randn(1, 25)\n",
    "output = model(x)\n",
    "for key in activation:\n",
    "    print(activation[key])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Use torch_intermediate_layer_getter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install torch_intermediate_layer_getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/hongbing/.cache/pip/wheels/7d/f4/b2/0793f47b888179b3a89e3d6e0c53180903e5cd3a6ea1ec4b95/torch_intermediate_layer_getter-0.1.post1-py3-none-any.whl\n",
      "Installing collected packages: torch-intermediate-layer-getter\n",
      "Successfully installed torch-intermediate-layer-getter-0.1.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_intermediate_layer_getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc2', tensor([[ 4.5227e-02,  3.9497e-02, -5.3636e-02,  6.4118e-02,  8.0344e-02,\n",
      "          1.2241e-01, -6.7592e-02,  1.3136e-01, -1.6434e-01,  9.6387e-02,\n",
      "          1.1345e-03,  2.2225e-03,  1.2926e-01,  2.5593e-01,  9.0569e-02,\n",
      "         -2.7969e-02,  1.7415e-01, -1.4138e-01, -1.9583e-01,  9.9759e-02,\n",
      "          3.9906e-02, -1.4257e-02, -1.2426e-01, -1.4839e-01, -3.7735e-02,\n",
      "         -4.1160e-02, -5.3619e-02, -1.1741e-02,  1.3697e-03, -1.9188e-02,\n",
      "         -5.3952e-02, -2.6832e-02,  1.1674e-01,  4.0707e-02, -6.7977e-02,\n",
      "         -2.2417e-02,  1.0721e-01,  1.2081e-01,  5.5723e-03,  7.8668e-02,\n",
      "          1.0027e-02,  1.5414e-01,  5.1735e-02,  5.9609e-03, -6.2577e-02,\n",
      "         -1.5737e-01,  1.2430e-01,  5.0084e-02,  3.8888e-02,  1.6673e-02,\n",
      "          8.9586e-02, -4.3854e-02,  9.3739e-02, -6.8255e-02, -9.1607e-02,\n",
      "          2.9572e-02,  1.9342e-02,  1.9910e-01,  2.9072e-02,  4.0594e-02,\n",
      "         -3.3934e-02,  7.1060e-05, -2.2376e-02,  3.8505e-02,  1.0437e-02,\n",
      "          1.4113e-01,  2.3193e-02, -1.0644e-01,  2.3142e-01,  1.3021e-01,\n",
      "         -5.6924e-02, -1.4435e-01, -5.8456e-02,  2.8119e-01, -1.1460e-02,\n",
      "          6.8875e-03,  1.0033e-01, -3.6165e-03,  1.6068e-01, -8.9569e-03,\n",
      "          2.4606e-02, -7.2680e-02,  2.7500e-02, -8.9764e-02]],\n",
      "       grad_fn=<AddmmBackward0>))])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.cl1 = nn.Linear(25, 60)\n",
    "        self.cl2 = nn.Linear(60, 16)\n",
    "        self.fc1 = nn.Linear(16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.cl1(x))\n",
    "        x = F.relu(self.cl2(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyModel()\n",
    "return_layers = {\n",
    "    'fc2': 'fc2',\n",
    "}\n",
    "mid_getter = MidGetter(model, return_layers, keep_output=True)\n",
    "x = torch.randn(1, 25)\n",
    "mid_outputs, model_output = mid_getter(x)\n",
    "print(mid_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
