{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a leaf Variable that requires grad is being used in an in-place operation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaf Tensor\n",
    "\n",
    "All Tensors that have `requires_grad` set to False will be leaf Tensors by convention. For Tensors that have `requires_grad` which is True, they will be leaf Tensors if they were created by the user(Eg. weights of your neural network). This means that they are not the result of an operation and so `grad_fn` is None.\n",
    "\n",
    "Basically, if `require_grad` is False then it will be a leaf tensor. Moreover, if `requires_grad` is True and it is created by user, it is also a leaf tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-Place Operation\n",
    "\n",
    "It is an operation which updates the value of the same tensor object on which it is called upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "a = torch.randn( (), requires_grad=False)\n",
    "initial_address = a.data_ptr()\n",
    "a += 5  #in-place operation\n",
    "print(initial_address == a.data_ptr())\n",
    "a = a + 5 #out-of-place operation\n",
    "print(initial_address == a.data_ptr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, `variable += any_thing` is inplace but `variable = variable + any_thing` is NOT inplace\n",
    "\n",
    "Now, let’s see what happens when we change `requires_grad` to True when we initialize a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn( (), requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m initial_address \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mdata_ptr()\n\u001b[0;32m----> 7\u001b[0m a \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m#in-place operation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(initial_address \u001b[39m==\u001b[39m a\u001b[39m.\u001b[39mdata_ptr())\n\u001b[1;32m      9\u001b[0m a \u001b[39m=\u001b[39m a \u001b[39m+\u001b[39m \u001b[39m5\u001b[39m \u001b[39m#out-of-place operation\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "a = torch.randn( (), requires_grad=True)\n",
    "initial_address = a.data_ptr()\n",
    "a += 5  #in-place operation\n",
    "print(initial_address == a.data_ptr())\n",
    "a = a + 5 #out-of-place operation\n",
    "print(initial_address == a.data_ptr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve this issue?\n",
    "\n",
    "You can wrap the update operations under `torch.no_grad():` which will tell the PyTorch to not track and validate the operations being performed under it’s hood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_473267/344507128.py:45: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  a = a - learning_rate * a.grad\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     41\u001b[0m \u001b[39m# Manually update weights using gradient descent. Wrap in torch.no_grad()\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# because weights have requires_grad=True, but we don't need to track this\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# in autograd.\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m a \u001b[39m=\u001b[39m a \u001b[39m-\u001b[39m learning_rate \u001b[39m*\u001b[39;49m a\u001b[39m.\u001b[39;49mgrad\n\u001b[1;32m     46\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m learning_rate \u001b[39m*\u001b[39m b\u001b[39m.\u001b[39mgrad\n\u001b[1;32m     47\u001b[0m c \u001b[39m=\u001b[39m c \u001b[39m-\u001b[39m learning_rate \u001b[39m*\u001b[39m c\u001b[39m.\u001b[39mgrad\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(id(a))\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "\n",
    "    a = a - learning_rate * a.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    c = c - learning_rate * c.grad\n",
    "    d = d - learning_rate * d.grad\n",
    "    # Manually zero the gradients after updating weights\n",
    "    a.grad = None\n",
    "    b.grad = None\n",
    "    c.grad = None\n",
    "    d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are updating your weights at line 46, you are making this NEW tensor object a result of some mathematical operation on your original tensor which is making it(the new a) an intermediate tensor.\n",
    "\n",
    "What PyTorch does in case of `intermediate tensor` is, it doesn’t accumulate the gradient in the `.grad` attribute of the tensor which would have been the case if it was a `leaf` tensor.\n",
    "\n",
    "So, since the weight/parameter that you are updating is no longer a leaf tensor, it’s .grad will be None as the gradient is not being accumulated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retain_grad\n",
    "\n",
    "retain_grad() will let you save the gradients in the `.grad` attribute and won’t make it None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140347014812352\n",
      "99 3238.300537109375\n",
      "140347015472064\n",
      "199 2245.867431640625\n",
      "140347015930816\n",
      "299 1559.936279296875\n",
      "140347018197504\n",
      "399 1085.354248046875\n",
      "140347014811264\n",
      "499 756.6680908203125\n",
      "140347014145024\n",
      "599 528.8026733398438\n",
      "140346854566720\n",
      "699 370.6806335449219\n",
      "140346853855488\n",
      "799 260.8537292480469\n",
      "140346853619136\n",
      "899 184.50198364257812\n",
      "140346853387200\n",
      "999 131.3756866455078\n",
      "140347015638656\n",
      "1099 94.37876892089844\n",
      "140347015706752\n",
      "1199 68.59319305419922\n",
      "140347015075456\n",
      "1299 50.60741424560547\n",
      "140347015536320\n",
      "1399 38.052650451660156\n",
      "140347014838720\n",
      "1499 29.282575607299805\n",
      "140347015457344\n",
      "1599 23.151979446411133\n",
      "140346851508096\n",
      "1699 18.863677978515625\n",
      "140346851321152\n",
      "1799 15.862088203430176\n",
      "140346851097088\n",
      "1899 13.759873390197754\n",
      "140346850856896\n",
      "1999 12.286673545837402\n",
      "Result: y = 0.058623336255550385 + 0.8372474908828735 x + -0.01011350192129612 x^2 + -0.09055762737989426 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(id(a))\n",
    "        print(t, loss.item())\n",
    "\n",
    "    a.retain_grad()\n",
    "    b.retain_grad()\n",
    "    c.retain_grad()\n",
    "    d.retain_grad()\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    \n",
    "    a = a - learning_rate * a.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    c = c - learning_rate * c.grad\n",
    "    d = d - learning_rate * d.grad\n",
    "    # Manually zero the gradients after updating weights\n",
    "    a.grad = None\n",
    "    b.grad = None\n",
    "    c.grad = None\n",
    "    d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>That will be really slow compared to what you were achieving by using `in-place` operation under the hood of `torch.no_grad()`</b>.\n",
    "\n",
    "We are creating new objects with the updated value(out-of-place operation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.no_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3238.300537109375\n",
      "199 2245.867431640625\n",
      "299 1559.936279296875\n",
      "399 1085.354248046875\n",
      "499 756.6680908203125\n",
      "599 528.8026733398438\n",
      "699 370.6806335449219\n",
      "799 260.8537292480469\n",
      "899 184.50198364257812\n",
      "999 131.3756866455078\n",
      "1099 94.37876892089844\n",
      "1199 68.59319305419922\n",
      "1299 50.60741424560547\n",
      "1399 38.052650451660156\n",
      "1499 29.282575607299805\n",
      "1599 23.151979446411133\n",
      "1699 18.863677978515625\n",
      "1799 15.862088203430176\n",
      "1899 13.759873390197754\n",
      "1999 12.286673545837402\n",
      "Result: y = 0.058623336255550385 + 0.8372474908828735 x + -0.01011350192129612 x^2 + -0.09055762737989426 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    \n",
    "    # a -= learning_rate * a.grad\n",
    "    # b -= learning_rate * b.grad\n",
    "    # c -= learning_rate * c.grad\n",
    "    # d -= learning_rate * d.grad\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>This is much faster!</b>\n",
    "\n",
    "The resultant tensor weight objects that you were getting by using `torch.no_grad()` were the same as you initialized as you only did `in-place` operations on them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[Understanding the Error:- A leaf Variable that requires grad is being used in an in-place operation.](https://medium.com/@mrityu.jha/understanding-the-grad-of-autograd-fc8d266fd6cf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.9.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29481b19616862e67e4bae0fd078da766e713ba66d65398c80d88a2e079c89da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
