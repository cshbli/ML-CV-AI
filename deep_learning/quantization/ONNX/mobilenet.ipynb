{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mobilenet v2 Quantization with ONNX Runtime on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will load a mobilenet v2 model pretrained with [PyTorch](https://pytorch.org/), export the model to ONNX, quantize then run with ONNXRuntime, and convert the ONNX models to ORT format for ONNXRuntime Mobile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites ##\n",
    "\n",
    "If you have Jupyter Notebook, you can run this notebook directly with it. You may need to install or upgrade [PyTorch](https://pytorch.org/), [OnnxRuntime](https://microsoft.github.io/onnxruntime/), and other required packages.\n",
    "\n",
    "Otherwise, you can setup a new environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.13.0+cu117\n",
      "onnxruntime: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import onnxruntime\n",
    "print(\"torch: {}\".format(torch.__version__))\n",
    "print(\"onnxruntime: {}\".format(onnxruntime.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Download pretrained model and export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load a pretrained mobilenet v2 model, and export it to ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the pretrained model\n",
    "Use torchvision provides API to load mobilenet_v2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /home/hongbing/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8e7218654f4cf9975f9d384a5c335d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# pretrained=True is deprecated\n",
    "# mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
    "mobilenet_v2 = models.mobilenet_v2(weights=\"MobileNet_V2_Weights.DEFAULT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Export the model to ONNX\n",
    "Pytorch onnx export API to export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "image_height = 224\n",
    "image_width = 224\n",
    "x = torch.randn(1, 3, image_height, image_width, requires_grad=True)\n",
    "torch_out = mobilenet_v2(x)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(mobilenet_v2,              # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"data/mobilenet_v2_float.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=13,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output']) # the model's output names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Sample Execution with ONNXRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an sample with the full precision ONNX model. Firstly, implement the preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import torch\n",
    "\n",
    "def preprocess_image(image_path, height, width, channels=3):\n",
    "    image = Image.open(image_path)\n",
    "    #image = image.resize((width, height), Image.ANTIALIAS)\n",
    "    image = image.resize((width, height), Image.Resampling.LANCZOS)\n",
    "    image_data = np.asarray(image).astype(np.float32)\n",
    "    image_data = image_data.transpose([2, 0, 1]) # transpose to CHW\n",
    "    mean = np.array([0.079, 0.05, 0]) + 0.406\n",
    "    std = np.array([0.005, 0, 0.001]) + 0.224\n",
    "    for channel in range(image_data.shape[0]):\n",
    "        image_data[channel, :, :] = (image_data[channel, :, :] / 255 - mean[channel]) / std[channel]\n",
    "    image_data = np.expand_dims(image_data, 0)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the imagenet labels and load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10472  100 10472    0     0  54259      0 --:--:-- --:--:-- --:--:-- 54259\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "!curl -o imagenet_classes.txt https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the example with ONNXRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabby 0.18907423\n",
      "Egyptian cat 0.0869033\n",
      "tiger cat 0.08618623\n",
      "studio couch 0.007404346\n",
      "tiger 0.0064240964\n"
     ]
    }
   ],
   "source": [
    "session_fp32 = onnxruntime.InferenceSession(\"data/mobilenet_v2_float.onnx\")\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def run_sample(session, image_file, categories):\n",
    "    output = session.run([], {'input':preprocess_image(image_file, image_height, image_width)})[0]\n",
    "    output = output.flatten()\n",
    "    output = softmax(output) # this is optional\n",
    "    top5_catid = np.argsort(-output)[:5]\n",
    "    for catid in top5_catid:\n",
    "        print(categories[catid], output[catid])\n",
    "\n",
    "run_sample(session_fp32, 'calibration_imagenet/cat.jpg', categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Quantize the model with ONNXRuntime \n",
    "In this step, we load the full precison model, and quantize it with ONNXRuntime quantization tool. And show the model size comparison between full precision and quantized model. Finally, we run the same sample with the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement a CalibrationDataReader\n",
    "CalibrationDataReader takes in calibration data and generates input for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "import os\n",
    "\n",
    "def preprocess_func(images_folder, height, width, size_limit=0):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "    for image_name in batch_filenames:\n",
    "        image_filepath = images_folder + '/' + image_name\n",
    "        image_data = preprocess_image(image_filepath, height, width)\n",
    "        unconcatenated_batch_data.append(image_data)\n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class MobilenetDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder):\n",
    "        self.image_folder = calibration_image_folder\n",
    "        self.preprocess_flag = True\n",
    "        self.enum_data_dicts = []\n",
    "        self.datasize = 0\n",
    "\n",
    "    def get_next(self):\n",
    "        if self.preprocess_flag:\n",
    "            self.preprocess_flag = False\n",
    "            nhwc_data_list = preprocess_func(self.image_folder, image_height, image_width, size_limit=0)\n",
    "            self.datasize = len(nhwc_data_list)\n",
    "            self.enum_data_dicts = iter([{'input': nhwc_data} for nhwc_data in nhwc_data_list])\n",
    "        return next(self.enum_data_dicts, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Quantize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can not upload full calibration data set for copy right issue, we only demonstrate with some example images. You need to use your own calibration data set in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX full precision model size (MB): 13.344060897827148\n",
      "ONNX quantized model size (MB): 3.4843597412109375\n"
     ]
    }
   ],
   "source": [
    "# change it to your real calibration data set\n",
    "calibration_data_folder = \"calibration_imagenet\"\n",
    "dr = MobilenetDataReader(calibration_data_folder)\n",
    "\n",
    "quantize_static('data/mobilenet_v2_float.onnx',\n",
    "                'data/mobilenet_v2_uint8.onnx',\n",
    "                dr)\n",
    "\n",
    "print('ONNX full precision model size (MB):', os.path.getsize(\"data/mobilenet_v2_float.onnx\")/(1024*1024))\n",
    "print('ONNX quantized model size (MB):', os.path.getsize(\"data/mobilenet_v2_uint8.onnx\")/(1024*1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Run the model with OnnxRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabby 0.15160517\n",
      "tiger cat 0.06722946\n",
      "Egyptian cat 0.06498974\n",
      "lynx 0.0069449684\n",
      "tiger 0.0046248008\n"
     ]
    }
   ],
   "source": [
    "session_quant = onnxruntime.InferenceSession(\"data/mobilenet_v2_uint8.onnx\")\n",
    "run_sample(session_quant, 'calibration_imagenet/cat.jpg', categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Convert the models to ORT format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is optional, we will convert the `mobilenet_v2_float.onnx` and `mobilenet_v2_uint8.onnx` to ORT format, to be used in mobile applications.\n",
    "\n",
    "If you intend to run these models using ONNXRuntime Mobile Execution Providers such as [NNAPI Execution Provider](https://www.onnxruntime.ai/docs/reference/execution-providers/NNAPI-ExecutionProvider.html) or [CoreML Execution Provider](https://www.onnxruntime.ai/docs/reference/execution-providers/CoreML-ExecutionProvider.html), please set the `optimization_level` of the conversion to `basic`. If you intend to run these models using CPU only, please set the `optimization_level` of the conversion to `all`. \n",
    "\n",
    "For further details, please see [Converting ONNX models to ORT format](https://www.onnxruntime.ai/docs/how-to/mobile/model-conversion.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting models with optimization style 'Fixed' and level 'all'\n",
      "Converting optimized ONNX model /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.onnx to ORT format model /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.ort\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Fixed' and level 'all'\n",
      "2022-12-12 21:33:49,659 ort_format_model.utils [INFO] - Created config in /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.required_operators.config\n",
      "Converting models with optimization style 'Runtime' and level 'all'\n",
      "Converting optimized ONNX model /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.onnx to ORT format model /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.with_runtime_opt.ort\n",
      "Converted 1/1 models successfully.\n",
      "Converting models again without runtime optimizations to generate a complete config file. These converted models are temporary and will be deleted.\n",
      "Converting optimized ONNX model /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.onnx to ORT format model /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/tmplmhz5af3.without_runtime_opt/mobilenet_v2_uint8.ort\n",
      "Converted 1/1 models successfully.\n",
      "Generating config file from ORT format models with optimization style 'Runtime' and level 'all'\n",
      "2022-12-12 21:33:49,753 ort_format_model.utils [INFO] - Created config in /home/hongbing/Projects/MachineLearning-ComputerVision-DataScience/deep_learning/quantization/ONNX/data/mobilenet_v2_uint8.required_operators.with_runtime_opt.config\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m onnxruntime.tools.convert_onnx_models_to_ort ./data/mobilenet_v2_uint8.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find the following converted models in the same directory,\n",
    "* mobilenet_v2_uint8.ort\n",
    "\n",
    "The above models are used in [ONNX Runtime Mobile image classification Android sample application](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/mobile/examples/image_classification/android).\n",
    "\n",
    "Please note, there are temporary ONNX model files generated by the quantization process, which are converted to ORT format as well, please ignore these files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
