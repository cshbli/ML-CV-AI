{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8033b2eb",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/tensorrt_torchtrt_qat_vgg/nvidia_logo.png\" width=\"90px\">\n",
    "\n",
    "# Deploying Quantization Aware Trained models in INT8 using Torch-TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec3ca7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Quantization Aware training (QAT) simulates quantization during training by quantizing weights and activation layers. This will help to reduce the loss in accuracy when we convert the network trained in FP32 to INT8 for faster inference. QAT introduces additional nodes in the graph which will be used to learn the dynamic ranges of weights and activation layers. In this notebook, we illustrate the following steps from training to inference of a QAT model in Torch-TensorRT.\n",
    "\n",
    "1. [Requirements](#1)\n",
    "2. [VGG16 Overview](#2)\n",
    "3. [Training a baseline VGG16 model](#3)\n",
    "4. [Apply Quantization](#4)\n",
    "5. [Model calibration](#5)\n",
    "6. [Quantization Aware training](#6)\n",
    "7. [Export to Torchscript](#7)\n",
    "8. [Inference using Torch-TensorRT](#8)\n",
    "8. [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79655ea8",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "##  1. Requirements\n",
    "Please install the <a href=\"https://github.com/pytorch/TensorRT/tree/master/examples/int8/training/vgg16#prequisites\">required dependencies</a> and import these libraries accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a72941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.0.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.13.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (8.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (21.3)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.3.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.29)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (59.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.12.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.10.0)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /opt/conda/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->ipykernel>=4.5.1->ipywidgets) (3.0.8)\n",
      "Requirement already satisfied: executing in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6493e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(pytorch_quantization.__version__)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from vgg16 import vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5060a",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "##  2. VGG16 Overview\n",
    "### Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "VGG is one of the earliest family of image classification networks that first used small (3x3) convolution filters and achieved significant improvements on ImageNet recognition challenge. The network architecture looks as follows\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5afc49",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "##  3. Training a baseline VGG16 model\n",
    "We train VGG16 on CIFAR10 dataset. Define training and testing datasets and dataloaders. This will download the CIFAR 10 data in your `data` directory. Data preprocessing is performed using `torchvision` transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2c4c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.023574352264404297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 170498071,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c056f42c5ea344459273776d681655f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# ========== Define Training dataset and dataloaders =============#\n",
    "training_dataset = datasets.CIFAR10(root='./data',\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.RandomCrop(32, padding=4),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                        ]))\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                                      batch_size=32,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=2)\n",
    "\n",
    "# ========== Define Testing dataset and dataloaders =============#\n",
    "testing_dataset = datasets.CIFAR10(root='./data',\n",
    "                                   train=False,\n",
    "                                   download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                   ]))\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset,\n",
    "                                                 batch_size=16,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd092b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, crit, opt, epoch):\n",
    "#     global writer\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, (data, labels) in enumerate(dataloader):\n",
    "        data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = crit(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch % 500 == 499:\n",
    "            print(\"Batch: [%5d | %5d] loss: %.3f\" % (batch + 1, len(dataloader), running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "def test(model, dataloader, crit, epoch):\n",
    "    global writer\n",
    "    global classes\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0.0\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "            out = model(data)\n",
    "            loss += crit(out, labels)\n",
    "            preds = torch.max(out, 1)[1]\n",
    "            class_probs.append([F.softmax(i, dim=0) for i in out])\n",
    "            class_preds.append(preds)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "    test_preds = torch.cat(class_preds)\n",
    "\n",
    "    return loss / total, correct / total\n",
    "\n",
    "def save_checkpoint(state, ckpt_path=\"checkpoint.pth\"):\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a86cc",
   "metadata": {},
   "source": [
    "*Define the VGG model that we are going to perfom QAT on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c564b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 has 10 classes\n",
    "model = vgg16(num_classes=len(classes), init_weights=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ead013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU()\n",
      "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU()\n",
      "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU()\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc00452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Learning rate\n",
    "lr = 0.1\n",
    "state = {}\n",
    "state[\"lr\"] = lr\n",
    "\n",
    "# Use cross entropy loss for classification and SGD optimizer\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=state[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# Adjust learning rate based on epoch number\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    global state\n",
    "    new_lr = lr * (0.5**(epoch // 12)) if state[\"lr\"] > 1e-7 else state[\"lr\"]\n",
    "    if new_lr != state[\"lr\"]:\n",
    "        state[\"lr\"] = new_lr\n",
    "        print(\"Updating learning rate: {}\".format(state[\"lr\"]))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = state[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80865a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [    1 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 12.600\n",
      "Batch: [ 1000 |  1563] loss: 11.475\n",
      "Batch: [ 1500 |  1563] loss: 11.272\n",
      "Test Loss: 0.13036 Test Acc: 17.28%\n",
      "Epoch: [    2 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 10.519\n",
      "Batch: [ 1000 |  1563] loss: 10.134\n",
      "Batch: [ 1500 |  1563] loss: 10.047\n",
      "Test Loss: 0.11995 Test Acc: 18.95%\n",
      "Epoch: [    3 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 10.026\n",
      "Batch: [ 1000 |  1563] loss: 9.903\n",
      "Batch: [ 1500 |  1563] loss: 9.973\n",
      "Test Loss: 0.12047 Test Acc: 21.65%\n",
      "Epoch: [    4 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.772\n",
      "Batch: [ 1000 |  1563] loss: 9.952\n",
      "Batch: [ 1500 |  1563] loss: 9.801\n",
      "Test Loss: 0.11765 Test Acc: 23.06%\n",
      "Epoch: [    5 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.626\n",
      "Batch: [ 1000 |  1563] loss: 9.444\n",
      "Batch: [ 1500 |  1563] loss: 9.423\n",
      "Test Loss: 0.11196 Test Acc: 23.99%\n",
      "Epoch: [    6 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.381\n",
      "Batch: [ 1000 |  1563] loss: 9.334\n",
      "Batch: [ 1500 |  1563] loss: 9.238\n",
      "Test Loss: 0.10957 Test Acc: 24.43%\n",
      "Epoch: [    7 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.431\n",
      "Batch: [ 1000 |  1563] loss: 9.354\n",
      "Batch: [ 1500 |  1563] loss: 9.171\n",
      "Test Loss: 0.10747 Test Acc: 26.79%\n",
      "Epoch: [    8 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.198\n",
      "Batch: [ 1000 |  1563] loss: 9.245\n",
      "Batch: [ 1500 |  1563] loss: 9.255\n",
      "Test Loss: 0.11002 Test Acc: 25.34%\n",
      "Epoch: [    9 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.228\n",
      "Batch: [ 1000 |  1563] loss: 9.309\n",
      "Batch: [ 1500 |  1563] loss: 9.163\n",
      "Test Loss: 0.10456 Test Acc: 28.63%\n",
      "Epoch: [   10 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.213\n",
      "Batch: [ 1000 |  1563] loss: 9.230\n",
      "Batch: [ 1500 |  1563] loss: 9.121\n",
      "Test Loss: 0.10667 Test Acc: 28.13%\n",
      "Epoch: [   11 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 8.935\n",
      "Batch: [ 1000 |  1563] loss: 8.921\n",
      "Batch: [ 1500 |  1563] loss: 9.302\n",
      "Test Loss: 0.11474 Test Acc: 27.83%\n",
      "Epoch: [   12 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.166\n",
      "Batch: [ 1000 |  1563] loss: 9.073\n",
      "Batch: [ 1500 |  1563] loss: 8.978\n",
      "Test Loss: 0.11352 Test Acc: 27.16%\n",
      "Updating learning rate: 0.05\n",
      "Epoch: [   13 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 8.457\n",
      "Batch: [ 1000 |  1563] loss: 8.202\n",
      "Batch: [ 1500 |  1563] loss: 8.107\n",
      "Test Loss: 0.09345 Test Acc: 39.30%\n",
      "Epoch: [   14 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 7.859\n",
      "Batch: [ 1000 |  1563] loss: 7.768\n",
      "Batch: [ 1500 |  1563] loss: 7.674\n",
      "Test Loss: 0.10003 Test Acc: 39.10%\n",
      "Epoch: [   15 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 7.497\n",
      "Batch: [ 1000 |  1563] loss: 7.264\n",
      "Batch: [ 1500 |  1563] loss: 7.170\n",
      "Test Loss: 0.08734 Test Acc: 47.35%\n",
      "Epoch: [   16 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 6.902\n",
      "Batch: [ 1000 |  1563] loss: 6.673\n",
      "Batch: [ 1500 |  1563] loss: 6.504\n",
      "Test Loss: 0.07432 Test Acc: 58.73%\n",
      "Epoch: [   17 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 6.225\n",
      "Batch: [ 1000 |  1563] loss: 6.044\n",
      "Batch: [ 1500 |  1563] loss: 5.776\n",
      "Test Loss: 0.07222 Test Acc: 60.35%\n",
      "Epoch: [   18 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 5.783\n",
      "Batch: [ 1000 |  1563] loss: 5.548\n",
      "Batch: [ 1500 |  1563] loss: 5.512\n",
      "Test Loss: 0.05979 Test Acc: 66.31%\n",
      "Epoch: [   19 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 5.418\n",
      "Batch: [ 1000 |  1563] loss: 5.283\n",
      "Batch: [ 1500 |  1563] loss: 5.105\n",
      "Test Loss: 0.06805 Test Acc: 61.24%\n",
      "Epoch: [   20 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.958\n",
      "Batch: [ 1000 |  1563] loss: 4.802\n",
      "Batch: [ 1500 |  1563] loss: 4.843\n",
      "Test Loss: 0.05608 Test Acc: 70.55%\n",
      "Epoch: [   21 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.666\n",
      "Batch: [ 1000 |  1563] loss: 4.506\n",
      "Batch: [ 1500 |  1563] loss: 4.551\n",
      "Test Loss: 0.06779 Test Acc: 65.46%\n",
      "Epoch: [   22 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.401\n",
      "Batch: [ 1000 |  1563] loss: 4.346\n",
      "Batch: [ 1500 |  1563] loss: 4.336\n",
      "Test Loss: 0.05316 Test Acc: 70.07%\n",
      "Epoch: [   23 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.219\n",
      "Batch: [ 1000 |  1563] loss: 4.043\n",
      "Batch: [ 1500 |  1563] loss: 4.107\n",
      "Test Loss: 0.05174 Test Acc: 73.19%\n",
      "Epoch: [   24 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 3.918\n",
      "Batch: [ 1000 |  1563] loss: 3.941\n",
      "Batch: [ 1500 |  1563] loss: 3.904\n",
      "Test Loss: 0.04594 Test Acc: 76.77%\n",
      "Updating learning rate: 0.025\n",
      "Epoch: [   25 /    25] LR: 0.025000\n",
      "Batch: [  500 |  1563] loss: 3.139\n",
      "Batch: [ 1000 |  1563] loss: 2.992\n",
      "Batch: [ 1500 |  1563] loss: 2.913\n",
      "Test Loss: 0.03577 Test Acc: 81.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 25 epochs to get ~80% accuracy.\n",
    "num_epochs=25\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(opt, epoch)\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, state[\"lr\"]))\n",
    "\n",
    "    train(model, training_dataloader, crit, opt, epoch)\n",
    "    test_loss, test_acc = test(model, testing_dataloader, crit, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': opt.state_dict(),\n",
    "                 'state': state},\n",
    "                ckpt_path=\"./data/vgg16_base_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1044537",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "##  4. Apply Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b7f4e",
   "metadata": {},
   "source": [
    "`quant_modules.initialize()` will ensure quantized version of modules will be called instead of original modules. For example, when you define a model with convolution, linear, pooling layers, `QuantConv2d`, `QuantLinear` and `QuantPooling` will be called. `QuantConv2d` basically wraps quantizer nodes around inputs and weights of regular `Conv2d`. Please refer to all the <a href=\"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization/pytorch_quantization/nn/modules\">quantized modules</a> in pytorch-quantization toolkit for more information. A `QuantConv2d` is represented in `pytorch-quantization` toolkit as follows.\n",
    "\n",
    "```\n",
    "def forward(self, input):\n",
    "        # the actual quantization happens in the next level of the class hierarchy\n",
    "        quant_input, quant_weight = self._quant(input)\n",
    "\n",
    "        if self.padding_mode == 'circular':\n",
    "            expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\n",
    "                                (self.padding[0] + 1) // 2, self.padding[0] // 2)\n",
    "            output = F.conv2d(F.pad(quant_input, expanded_padding, mode='circular'),\n",
    "                              quant_weight, self.bias, self.stride,\n",
    "                              _pair(0), self.dilation, self.groups)\n",
    "        else:\n",
    "            output = F.conv2d(quant_input, quant_weight, self.bias, self.stride, self.padding, self.dilation,\n",
    "                              self.groups)\n",
    "\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "985dc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "164ce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regular conv, FC layers will be converted to their quantozed counterparts due to quant_modules.initialize()\n",
    "qat_model = vgg16(num_classes=len(classes), init_weights=False)\n",
    "qat_model = qat_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10216920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU()\n",
      "    (37): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU()\n",
      "    (40): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU()\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): QuantAdaptiveAvgPool2d(\n",
      "    output_size=(1, 1)\n",
      "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantLinear(\n",
      "      in_features=512, out_features=4096, bias=True\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): QuantLinear(\n",
      "      in_features=4096, out_features=4096, bias=True\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): QuantLinear(\n",
      "      in_features=4096, out_features=10, bias=True\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=dynamic calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(qat_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16_base_ckpt is the checkpoint generated from Step 3 : Training a baseline VGG16 model.\n",
    "ckpt = torch.load(\"./data/vgg16_base_ckpt\")\n",
    "modified_state_dict={}\n",
    "for key, val in ckpt[\"model_state_dict\"].items():\n",
    "    # Remove 'module.' from the key names\n",
    "    if key.startswith('module'):\n",
    "        modified_state_dict[key[7:]] = val\n",
    "    else:\n",
    "        modified_state_dict[key] = val\n",
    "\n",
    "# Load the pre-trained checkpoint\n",
    "qat_model.load_state_dict(modified_state_dict)\n",
    "opt.load_state_dict(ckpt[\"opt_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a74e8",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "##  5. Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a321f9",
   "metadata": {},
   "source": [
    "The quantizer nodes introduced in the model around desired layers capture the dynamic range (min_value, max_value) that is observed by the layer. Calibration is the process of computing the dynamic range of these layers by passing calibration data, which is usually a subset of training or validation data. There are different ways of calibration: `max`, `histogram` and `entropy`. We use `max` calibration technique as it is simple and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "039423dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    model.cuda()\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistics\"\"\"\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # Feed data to the network for collecting stats\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "def calibrate_model(model, model_name, data_loader, num_calib_batch, calibrator, hist_percentile, out_dir):\n",
    "    \"\"\"\n",
    "        Feed data to the network and calibrate.\n",
    "        Arguments:\n",
    "            model: classification model\n",
    "            model_name: name to use when creating state files\n",
    "            data_loader: calibration data set\n",
    "            num_calib_batch: amount of calibration passes to perform\n",
    "            calibrator: type of calibration to use (max/histogram)\n",
    "            hist_percentile: percentiles to be used for historgram calibration\n",
    "            out_dir: dir to save state files in\n",
    "    \"\"\"\n",
    "\n",
    "    if num_calib_batch > 0:\n",
    "        print(\"Calibrating model\")\n",
    "        with torch.no_grad():\n",
    "            collect_stats(model, data_loader, num_calib_batch)\n",
    "\n",
    "        if not calibrator == \"histogram\":\n",
    "            compute_amax(model, method=\"max\")\n",
    "            calib_output = os.path.join(\n",
    "                out_dir,\n",
    "                F\"{model_name}-max-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "            torch.save(model.state_dict(), calib_output)\n",
    "        else:\n",
    "            for percentile in hist_percentile:\n",
    "                print(F\"{percentile} percentile calibration\")\n",
    "                compute_amax(model, method=\"percentile\")\n",
    "                calib_output = os.path.join(\n",
    "                    out_dir,\n",
    "                    F\"{model_name}-percentile-{percentile}-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "                torch.save(model.state_dict(), calib_output)\n",
    "\n",
    "            for method in [\"mse\", \"entropy\"]:\n",
    "                print(F\"{method} calibration\")\n",
    "                compute_amax(model, method=method)\n",
    "                calib_output = os.path.join(\n",
    "                    out_dir,\n",
    "                    F\"{model_name}-{method}-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "                torch.save(model.state_dict(), calib_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78504a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 54.30it/s]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0105 16:22:59.012063 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.012982 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.013417 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.013848 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.014188 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.014520 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.014871 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.015420 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.015875 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.016216 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.016549 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.016973 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.018018 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.018653 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.019019 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.019340 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.019648 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.019994 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.020331 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.020666 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.021041 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.021396 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.021696 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.022050 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.022384 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.022658 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.022965 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.023393 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.023696 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.024188 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.024507 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.024896 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.025251 140003311585088 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0105 16:22:59.026388 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.026653 140003311585088 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0105 16:22:59.027194 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0105 16:22:59.027676 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.028087 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0105 16:22:59.029247 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.029631 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([128, 1, 1, 1]).\n",
      "W0105 16:22:59.030036 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.030384 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([128, 1, 1, 1]).\n",
      "W0105 16:22:59.030905 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.031265 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0105 16:22:59.031659 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.032048 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0105 16:22:59.032476 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.032852 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0105 16:22:59.033656 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.034152 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0105 16:22:59.034693 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.035173 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0105 16:22:59.035744 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.036162 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0105 16:22:59.036667 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.037264 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0105 16:22:59.037728 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.038208 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0105 16:22:59.039165 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.039711 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0105 16:22:59.040401 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.040855 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.041368 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([4096, 1]).\n",
      "W0105 16:22:59.041900 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.042467 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([4096, 1]).\n",
      "W0105 16:22:59.043008 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0105 16:22:59.043403 140003311585088 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([10, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=2.7537 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.0._weight_quantizer            : TensorQuantizer(8bit fake axis=0 amax=[0.0339, 3.5808](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.3._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=24.6682 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.3._weight_quantizer            : TensorQuantizer(8bit fake axis=0 amax=[0.0651, 1.7107](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.7._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=14.7277 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.7._weight_quantizer            : TensorQuantizer(8bit fake axis=0 amax=[0.0144, 0.8101](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.10._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=7.0920 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.10._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.1105, 0.6563](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.14._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=12.2738 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.14._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0503, 0.6807](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.17._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=9.4448 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.17._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0314, 0.6356](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.20._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=8.7171 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.20._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0252, 0.4910](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.24._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=9.3120 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.24._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0233, 0.2781](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.27._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=4.3597 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.27._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0074, 0.1538](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.30._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=3.1043 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.30._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0101, 0.1478](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.34._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=3.1177 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.34._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0059, 0.2416](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.37._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=3.1375 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.37._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0064, 0.2194](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.40._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=2.9322 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.40._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0025, 0.4907](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "avgpool._input_quantizer                : TensorQuantizer(8bit fake per-tensor amax=5.4621 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.4621 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0027, 0.5788](4096) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.3._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=7.9150 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.3._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0018, 0.5537](4096) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.6._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=9.5122 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.6._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.3101, 0.5174](10) calibrator=MaxCalibrator scale=1.0 quant)\n"
     ]
    }
   ],
   "source": [
    "#Calibrate the model using max calibration technique.\n",
    "with torch.no_grad():\n",
    "    calibrate_model(\n",
    "        model=qat_model,\n",
    "        model_name=\"vgg16\",\n",
    "        data_loader=training_dataloader,\n",
    "        num_calib_batch=32,\n",
    "        calibrator=\"max\",\n",
    "        hist_percentile=[99.9, 99.99, 99.999, 99.9999],\n",
    "        out_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa0c109",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "##  6. Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8ec11",
   "metadata": {},
   "source": [
    "In this phase, we finetune the model weights and leave the quantizer node values frozen. The dynamic ranges for each layer obtained from the calibration are kept constant while the weights of the model are finetuned to be close to the accuracy of original FP32 model (model without quantizer nodes) is preserved. Usually the finetuning of QAT model should be quick compared to the full training of the original model. Use QAT to fine-tune for around 10% of the original training schedule with an annealing learning-rate. Please refer to <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT</a> for detailed recommendations. For this VGG model, it is enough to finetune for 1 epoch to get acceptable accuracy. \n",
    "During finetuning with QAT, the quantization is applied as a composition of `max`, `clamp`, `round` and `mul` ops. \n",
    "```\n",
    "# amax is absolute maximum value for an input\n",
    "# The upper bound for integer quantization (127 for int8)\n",
    "max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
    "scale = max_bound / amax\n",
    "outputs = torch.clamp((inputs * scale).round_(), min_bound, max_bound)\n",
    "```\n",
    "<a href=\"https://github.com/NVIDIA/TensorRT/blob/8.0.1/tools/pytorch-quantization/pytorch_quantization/tensor_quant.py\">tensor_quant function</a> in `pytorch_quantization` toolkit is responsible for the above tensor quantization. Usually, per channel quantization is recommended for weights, while per tensor quantization is recommended for activations in a network.\n",
    "During inference, we use `torch.fake_quantize_per_tensor_affine` and `torch.fake_quantize_per_channel_affine` to perform quantization as this is easier to convert into corresponding TensorRT operators. Please refer to next sections for more details on how these operators are exported in torchscript and converted in Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f28d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate: 0.1\n",
      "Epoch: [    1 /     1] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 3.029\n",
      "Batch: [ 1000 |  1563] loss: 2.957\n",
      "Batch: [ 1500 |  1563] loss: 2.882\n",
      "Test Loss: 0.03429 Test Acc: 82.51%\n",
      "Checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "# Finetune the QAT model for 1 epoch\n",
    "num_epochs=1\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(opt, epoch)\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, state[\"lr\"]))\n",
    "\n",
    "    train(qat_model, training_dataloader, crit, opt, epoch)\n",
    "    test_loss, test_acc = test(qat_model, testing_dataloader, crit, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': qat_model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': opt.state_dict(),\n",
    "                 'state': state},\n",
    "                ckpt_path=\"./data/vgg16_qat_ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22b793eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.7537 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0339, 3.5808](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=24.6682 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0651, 1.7107](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=14.7277 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0144, 0.8101](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=7.0920 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.1105, 0.6563](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=12.2738 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0503, 0.6807](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=9.4448 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0314, 0.6356](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU()\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=8.7171 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0252, 0.4910](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU()\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=9.3120 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0233, 0.2781](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU()\n",
      "    (27): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=4.3597 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0074, 0.1538](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (29): ReLU()\n",
      "    (30): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.1043 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0101, 0.1478](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (32): ReLU()\n",
      "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (34): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.1177 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0059, 0.2416](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (36): ReLU()\n",
      "    (37): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=3.1375 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0064, 0.2194](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (39): ReLU()\n",
      "    (40): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=2.9322 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0025, 0.4907](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (42): ReLU()\n",
      "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): QuantAdaptiveAvgPool2d(\n",
      "    output_size=(1, 1)\n",
      "    (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=5.4621 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantLinear(\n",
      "      in_features=512, out_features=4096, bias=True\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=5.4621 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0027, 0.5788](4096) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): QuantLinear(\n",
      "      in_features=4096, out_features=4096, bias=True\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=7.9150 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.0018, 0.5537](4096) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): QuantLinear(\n",
      "      in_features=4096, out_features=10, bias=True\n",
      "      (_input_quantizer): TensorQuantizer(8bit fake per-tensor amax=9.5122 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "      (_weight_quantizer): TensorQuantizer(8bit fake axis=0 amax=[0.3101, 0.5174](10) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(qat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4dcaa2",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "##  7. Export to Torchscript\n",
    "Export the model to Torch script. Trace the model and convert it into torchscript for deployment. To learn more about Torchscript, please refer to https://pytorch.org/docs/stable/jit.html. Setting `quant_nn.TensorQuantizer.use_fb_fake_quant = True` enables the QAT model to use `torch.fake_quantize_per_tensor_affine` and `torch.fake_quantize_per_channel_affine` operators instead of `tensor_quant` function to export quantization operators. In torchscript, they are represented as `aten::fake_quantize_per_tensor_affine` and `aten::fake_quantize_per_channel_affine`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d34f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1109 04:02:37.101168 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.102248 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.107194 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.107625 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.115269 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.115740 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.117969 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.118358 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.126382 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.126834 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.128674 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.129518 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.135453 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.135936 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.137858 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.138366 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.145539 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.146053 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.147871 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.148353 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.154252 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.154685 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.156558 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.157159 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.163197 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.163676 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.165549 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.165991 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.173305 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.173926 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.176034 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.176697 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.182843 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.183426 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.185377 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.185962 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.191966 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.192424 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.194325 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.194817 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.201988 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.202665 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.204763 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.205461 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.211393 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.211987 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.213899 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.214450 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.220892 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.221533 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.223519 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.224037 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.233809 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.234434 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.238212 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.239042 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.241022 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.241654 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.247820 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.248445 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.250366 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.250959 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.257248 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.257854 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.259968 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.260660 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "W1109 04:02:37.268160 139704147265344 tensor_quantizer.py:280] Use Pytorch's native experimental fake quantization.\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  inputs, amax.item() / bound, 0,\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1109 04:02:37.329273 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.330212 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.332529 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.333365 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.339547 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.340248 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.342257 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.342890 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.350619 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.351372 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.353470 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.354121 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.360090 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.360806 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.362803 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.363274 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.370369 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.371057 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.373071 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.373766 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.379890 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.380538 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.382532 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.383128 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.389077 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.389760 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.391815 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.392399 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.399809 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.400472 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.402399 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.402939 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.408818 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.409424 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.411513 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.412097 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.418537 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.419128 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.421343 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.421946 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.429382 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.430156 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.432259 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.433079 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.439297 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.440027 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.442149 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.442826 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.449377 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.449968 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.452122 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.452754 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.462532 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.463295 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.466963 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.467725 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.469692 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.470336 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.476204 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.476738 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.478809 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.479375 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.485666 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.486219 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.488416 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E1109 04:02:37.488986 139704147265344 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n"
     ]
    }
   ],
   "source": [
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "with torch.no_grad():\n",
    "    data = iter(testing_dataloader)\n",
    "    images, _ = data.next()\n",
    "    jit_model = torch.jit.trace(qat_model, images.to(\"cuda\"))\n",
    "    torch.jit.save(jit_model, \"trained_vgg16_qat.jit.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341418a",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "##  8. Inference using Torch-TensorRT\n",
    "In this phase, we run the exported torchscript graph of VGG QAT using Torch-TensorRT. Torch-TensorRT is a Pytorch-TensorRT compiler which converts Torchscript graphs into TensorRT. TensorRT 8.0 supports inference of quantization aware trained models and introduces new APIs; `QuantizeLayer` and `DequantizeLayer`. We can observe the entire VGG QAT graph quantization nodes from the debug log of Torch-TensorRT. To enable debug logging, you can set `torch_tensorrt.logging.set_reportable_log_level(torch_tensorrt.logging.Level.Debug)`. For example, `QuantConv2d` layer from `pytorch_quantization` toolkit is represented as follows in Torchscript\n",
    "```\n",
    "%quant_input : Tensor = aten::fake_quantize_per_tensor_affine(%x, %636, %637, %638, %639)\n",
    "%quant_weight : Tensor = aten::fake_quantize_per_channel_affine(%394, %640, %641, %637, %638, %639)\n",
    "%input.2 : Tensor = aten::_convolution(%quant_input, %quant_weight, %395, %687, %688, %689, %643, %690, %642, %643, %643, %644, %644)\n",
    "```\n",
    "`aten::fake_quantize_per_*_affine` is converted into `QuantizeLayer` + `DequantizeLayer` in Torch-TensorRT internally. Please refer to <a href=\"https://github.com/pytorch/TensorRT/blob/master/core/conversion/converters/impl/quantization.cpp\">quantization op converters</a> in Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa7495e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Cannot infer input type from calcuations in graph for input x.2. Assuming it is Float32. If not, specify input type explicity\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Detected invalid timing cache, setup a local cache instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG QAT accuracy using TensorRT: 82.97%\n"
     ]
    }
   ],
   "source": [
    "qat_model = torch.jit.load(\"trained_vgg16_qat.jit.pt\").eval()\n",
    "\n",
    "compile_spec = {\"inputs\": [torch_tensorrt.Input([16, 3, 32, 32])],\n",
    "                \"enabled_precisions\": torch.int8,\n",
    "                }\n",
    "trt_mod = torch_tensorrt.compile(qat_model, **compile_spec)\n",
    "\n",
    "test_loss, test_acc = test(trt_mod, testing_dataloader, crit, 0)\n",
    "print(\"VGG QAT accuracy using TensorRT: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5a90e",
   "metadata": {},
   "source": [
    "### Performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb2cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Helper function to benchmark the model\n",
    "def benchmark(model, input_shape=(1024, 1, 32, 32), dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            output = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%100==0:\n",
    "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2514ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 4.83 ms\n",
      "Iteration 200/1000, avg batch time 4.83 ms\n",
      "Iteration 300/1000, avg batch time 4.83 ms\n",
      "Iteration 400/1000, avg batch time 4.83 ms\n",
      "Iteration 500/1000, avg batch time 4.83 ms\n",
      "Iteration 600/1000, avg batch time 4.83 ms\n",
      "Iteration 700/1000, avg batch time 4.83 ms\n",
      "Iteration 800/1000, avg batch time 4.83 ms\n",
      "Iteration 900/1000, avg batch time 4.83 ms\n",
      "Iteration 1000/1000, avg batch time 4.83 ms\n",
      "Input shape: torch.Size([16, 3, 32, 32])\n",
      "Output shape: torch.Size([16, 10])\n",
      "Average batch time: 4.83 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(jit_model, input_shape=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5378ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 1.87 ms\n",
      "Iteration 200/1000, avg batch time 1.84 ms\n",
      "Iteration 300/1000, avg batch time 1.85 ms\n",
      "Iteration 400/1000, avg batch time 1.83 ms\n",
      "Iteration 500/1000, avg batch time 1.82 ms\n",
      "Iteration 600/1000, avg batch time 1.81 ms\n",
      "Iteration 700/1000, avg batch time 1.81 ms\n",
      "Iteration 800/1000, avg batch time 1.80 ms\n",
      "Iteration 900/1000, avg batch time 1.80 ms\n",
      "Iteration 1000/1000, avg batch time 1.79 ms\n",
      "Input shape: torch.Size([16, 3, 32, 32])\n",
      "Output shape: torch.Size([16, 10])\n",
      "Average batch time: 1.79 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_mod, input_shape=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5ec1c",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "##  9. References\n",
    "* <a href=\"https://arxiv.org/pdf/1409.1556.pdf\">Very Deep Convolution Networks for large scale Image Recognition</a>\n",
    "* <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT</a>\n",
    "* <a href=\"https://github.com/pytorch/TensorRT/tree/master/examples/int8/training/vgg16#quantization-aware-fine-tuning-for-trying-out-qat-workflows\">QAT workflow for VGG16</a>\n",
    "* <a href=\"https://github.com/pytorch/TensorRT/tree/master/examples/int8/qat\">Deploying VGG QAT model in C++ using Torch-TensorRT</a>\n",
    "* <a href=\"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization\">Pytorch-quantization toolkit from NVIDIA</a>\n",
    "* <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html\">Pytorch quantization toolkit userguide</a>\n",
    "* <a href=\"https://arxiv.org/pdf/2004.09602.pdf\">Quantization basics</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
