{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantized Neural Network\n",
    "## 07 Efficient Integer Inference\n",
    "by [Soon Yau Cheong](http://www.linkedin.com/in/soonyau)\n",
    "\n",
    "One advantage of Google's quantization scheme is that it can run efficiently on existing processors including a fixed-point (inter) only processor. Compare with floating point processor, the former is usually smaller in size and more power efficient. However, from computation complexity, it actually require a lot more arithemtic operations than running in full floating point mode due to the need to de-quantize the weights and activation, perform the computation, then requantize it back into low precision integers. Although in general the inference speed should improve due to reduce memory traffic but this additional computation is a big set back. In this tutorial, we'll look at tricks to reduce the computation complexity.\n",
    "\n",
    "### Requantization\n",
    "Now let's revisit the quantization equation\n",
    "\\begin{equation}\n",
    "x_q = \\frac{x}{\\Delta_x} + z_x\n",
    "\\end{equation}\n",
    "and dequantization equation\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x} = (x_q - z_x )\\Delta_x\n",
    "\\end{equation}\n",
    "\n",
    "(Simplified) convolutional equations:\n",
    "\\begin{equation}\n",
    "y = \\sum_N^i w_i x_i +b\n",
    "\\end{equation}\n",
    "\n",
    "Now dequantize $x_q$, $w_q$ and $b_q$ to perform convolution in integers\n",
    "\\begin{equation}\n",
    "y_q = \\sum_N^i (w_{q,i} - z_w )\\Delta_w (x_{q,i} - z_x )\\Delta_x + (b_q - z_b )\\Delta_b\n",
    "\\end{equation}\n",
    "\n",
    "Let's rearrange the equation and denote the signed integer $w'_{i} = w_{q,i} - z_w$ \n",
    "\\begin{equation}\n",
    "y_q = \\Delta_w \\Delta_x \\sum_N^i w'_i x'_i + \\Delta_b b'\n",
    "\\end{equation}\n",
    "\n",
    "Last step is to quantize the result back into integer\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\frac{1}{\\Delta_y}(\\Delta_w \\Delta_x \\sum_N^i w'_i x'_i + \\Delta_b b') +  z_y \\\\\n",
    "\\hat{y} = \\frac{\\Delta_w \\Delta_x}{\\Delta_y} \\sum_N^i w'_i x'_i + \\frac{\\Delta_b}{\\Delta_y} b' +  z_y \\\\\n",
    "\\end{equation}\n",
    "\n",
    "That looks a bit complex, now we'll begin simplification. As opposed to weights that are quantized to uint8, biases are quantized to int32 and we don't really worry about losing resolution due to scaling. Therefore, $\\Delta_b$ is chosen to be equal to $\\Delta_w \\Delta_x$, and the above equation becomes\n",
    "\\begin{equation}\n",
    "\\hat{y} = (M \\sum_N^i w'_i x'_i) +  (Mb' +  z_y )\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "M = \\frac{\\Delta_w \\Delta_x}{\\Delta_y}\n",
    "\\end{equation}\n",
    "which can be simplified again to \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = (M \\sum_N^i w'_i x'_i) +  B\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "B = (Mb' +  z_y )\n",
    "\\end{equation}\n",
    "\n",
    "Instead of performing de-quantization for every activation, weights, and biases, and quantize the results, we now only need to perform 2 additional operations compared with the original convolutional equations (note that M and B can be pre-calculated from constants)\n",
    "- minus offset from quantized integer to get signed integer\n",
    "- multiply with dot product with M\n",
    "\n",
    "### Removing Offset\n",
    "One potential improvement to the scheme is to avoid the computation of signed integer, the way to do it would be to get rid of the offsets altogether which is exactly the case of biases where the offset is always 0 and quantized to int32. If we were to do that to weights, we will need to add another signed bit which grow the bidwith, for example, instead of using uint8, we now use int9 and remove the offset. Alternatively, we reduce 1-bit in our quantization scheme to int8, as we have shown previously with Cifar10, there is no accuracy loss going from 8-bit to 7-bit. \n",
    "\n",
    "### Multiplier\n",
    "All variables in the equations are now integer-only, apart from M which can be floating point and how are we going to deal with that. One popular trick to port floating point to fixed point in embedded system is to decompose the floating point to a pair of fixed integer and shift operations. In [Google's paper](https://arxiv.org/pdf/1712.05877.pdf), they use the term\n",
    "\\begin{equation}\n",
    "M = 2^{-n}M_o\n",
    "\\end{equation}\n",
    "where $M_o$ is integer and n is the number of right shift operation. For those who are not familiar with bitshift operation, shifting integer by 1 bit to the right is equivalent to dividing by 2, 2-bits is divided by 4 and so on. Now let's jump into some example to demonstrate. I won't write the full routine to compute convolution but will use actual quantization parameters of Mobilenet to demonstrate the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow 1.10.0\n",
      "Python 3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n",
      "6 MobilenetV1/MobilenetV1/Conv2d_0/Conv2D_Fold_bias\n",
      "7 MobilenetV1/MobilenetV1/Conv2d_0/Relu6\n",
      "8 MobilenetV1/MobilenetV1/Conv2d_0/weights_quant/FakeQuantWithMinMaxVars\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils\n",
    "print(\"Tensorflow\", tf.__version__)\n",
    "print(\"Python\", sys.version)\n",
    "\n",
    "model_path = 'models/mobilenet_v1/mobilenet_v1_1.0_224_quant.tflite'\n",
    "\n",
    "interpreter = tf.contrib.lite.Interpreter(model_path=model_path)\n",
    "                                         \n",
    "interpreter.allocate_tensors()\n",
    "num_layer = 89\n",
    "for i in range(num_layer):\n",
    "    detail = interpreter._get_tensor_details(i)\n",
    "    # Let's pick the first convolutional layer where input is RGB image\n",
    "    if \"Conv2d_0\" in detail['name']:\n",
    "        print(i, detail['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias 0.00017052092880476266 0\n",
      "y 0.023528477177023888 0\n",
      "w 0.02182667888700962 151\n",
      "x 0.0078125 127\n"
     ]
    }
   ],
   "source": [
    "scale_b, offset_b = interpreter._get_tensor_details(6)['quantization']\n",
    "print(\"bias\", scale_b, offset_b)\n",
    "scale_y, offset_y = interpreter._get_tensor_details(7)['quantization']\n",
    "print(\"y\", scale_y, offset_y)\n",
    "scale_w, offset_w = interpreter._get_tensor_details(8)['quantization']\n",
    "print(\"w\", scale_w, offset_w)\n",
    "scale_x, offset_x = 1./128, 127\n",
    "print(\"x\", scale_x, offset_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if $\\Delta_b == \\Delta_w \\Delta_x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00017052092880476266\n",
      "0.00017052092880476266\n"
     ]
    }
   ],
   "source": [
    "print(scale_w*scale_x)\n",
    "print(scale_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective now is to find n and corresponding $Mo$ using equation\n",
    "\\begin{equation}\n",
    "Mo = round(2^n \\times M)\n",
    "\\end{equation}\n",
    "The multiplication between M and say P will now becomes\n",
    "\\begin{equation}\n",
    "M\\times P \\approx (M_o \\times P) >> n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 14 -11 -13]\n",
      "  [ 24 -14 -16]\n",
      "  [ 10  -2  -6]]\n",
      "\n",
      " [[ 28 -13 -14]\n",
      "  [ 39 -13 -15]\n",
      "  [  9  -8  -8]]\n",
      "\n",
      " [[ 13  -6  -9]\n",
      "  [ 14  -9  -8]\n",
      "  [ -3   0  -4]]]\n"
     ]
    }
   ],
   "source": [
    "# We now take the weights of one of the filter\n",
    "w = interpreter.get_tensor(8)\n",
    "w_int = w[2,:,:,:].astype(np.int) - int(offset_w)\n",
    "print(w_int)\n",
    "\n",
    "# Generate random input\n",
    "x_int = np.random.randint(0,255,w_int.shape) - int(offset_x)\n",
    "\n",
    "# Perform dot product\n",
    "P = np.sum(np.multiply(x_int, w_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=0.0072474273418460 P=7091, MxP=51.391507\n",
      "n=1, Mo=0, approx=0.000000, error=51.391507\n",
      "n=2, Mo=0, approx=0.000000, error=51.391507\n",
      "n=3, Mo=0, approx=0.000000, error=51.391507\n",
      "n=4, Mo=0, approx=0.000000, error=51.391507\n",
      "n=5, Mo=0, approx=0.000000, error=51.391507\n",
      "n=6, Mo=0, approx=0.000000, error=51.391507\n",
      "n=7, Mo=1, approx=55.000000, error=-3.608493\n",
      "n=8, Mo=2, approx=55.000000, error=-3.608493\n",
      "n=9, Mo=4, approx=55.000000, error=-3.608493\n",
      "n=10, Mo=7, approx=48.000000, error=3.391507\n",
      "n=11, Mo=15, approx=51.000000, error=0.391507\n",
      "n=12, Mo=30, approx=51.000000, error=0.391507\n",
      "n=13, Mo=59, approx=51.000000, error=0.391507\n",
      "n=14, Mo=119, approx=51.000000, error=0.391507\n",
      "n=15, Mo=237, approx=51.000000, error=0.391507\n"
     ]
    }
   ],
   "source": [
    "# Now calculate M\n",
    "M = scale_w*scale_x/scale_y\n",
    "print(\"M=%.16f P=%d, MxP=%f\"%(M, P, M*P))\n",
    "\n",
    "def multiply(n, M, P):\n",
    "    # floting point operation\n",
    "    result = M*P\n",
    "    \n",
    "    # calculate Mo\n",
    "    Mo = int(round(2**n*M))\n",
    "    \n",
    "    # integer arithmetic only\n",
    "    approx_result = (Mo*P)>>n\n",
    "    \n",
    "    print(\"n=%d, Mo=%d, approx=%f, error=%f\"%\\\n",
    "          (n, Mo, approx_result, result-approx_result))\n",
    "    return approx_result\n",
    "\n",
    "# Our objective now is to find n and Mo\n",
    "for n in range(1, 16):\n",
    "    multiply(n, M, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go. We have shown that by decomposing M to n and Mo, we can approximate floating point multiplication using only integer multiplication and bit shifting. We notice that higher n gives better approximation with lesser error. However, the choice of n is constrained by the bitwidth of hardware processor and the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n"
     ]
    }
   ],
   "source": [
    "#For a 3x3x3 conv kernel, the dot product must accomodate maximum value of\n",
    "max_val = 3*3*3*255*255\n",
    "\n",
    "# which require bit numbers of\n",
    "numbits = round(np.log2(max_val)+1) # +1 for signed bit\n",
    "print(numbits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a processor internal registers could only hold 32-bit value at a time, then we will only have maximum 32-22 = 10 bits left for Mo, from there we can select our n. We'll need to perform system-wise analysis to find out the optimal values that do not cause arithemtic operations to overflow. In CNN, the worst case scenario would be the convolutional kernel with largest dimension, in Mobilenet, that would the last conv layer with 1x1x1024 weights which I'll leave as exercise to calculate the number of bits required. Anyway, in the most unlikely even of having both activations and weights to be all 255 which is the worst case scenario we assume, something has gone terribly wrong. Therefore, we can relax our requirement by 1 bit or 2 in calculating for normal operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "You now have all the skills to train a quantized network, convert the graph and export the quantized weights for inference. For motivated readers, you can now implement the inference by writing own software/FPGA code which isn't very difficult given there really are only 2 type of layers in Mobilenet - depthwise and pointwise convolution. Next time, we'll look at how to run TFLite on Android."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
