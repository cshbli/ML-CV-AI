{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Quantization in PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNDAMENTALS OF QUANTIZATION\n",
    "\n",
    "Quantization has roots in information compression; in deep networks it refers to reducing the numerical precision of its weights and/or activations.\n",
    "\n",
    "Overparameterized DNNs have more degrees of freedom and this makes them good candidates for information compression. When you quantize a model, two things generally happen - the model gets smaller and runs with better efficiency. \n",
    "\n",
    "- Hardware vendors explicitly allow for faster processing of 8-bit data (than 32-bit data) resulting in higher throughput. \n",
    "- A smaller model has lower memory footprint and power consumption, crucial for deployment at the edge."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping function\n",
    "\n",
    "The mapping function is what you might guess - a function that maps values from floating-point to integer space. A commonly used mapping function is a linear transformation given by:\n",
    "\n",
    "$$Q(r)=round(r/S + Z)$$\n",
    "\n",
    "where `r` is the input and `S` are `Z` `quantization parameters`.\n",
    "\n",
    "To reconvert to floating point space, the inverse function is given by:\n",
    "\n",
    "$$\\hat r=(Q(r) - Z)*S$$\n",
    "\n",
    "$\\hat r \\neq r$, and their difference constitutes the quantization error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Parameters\n",
    "\n",
    "The mapping function is parameterized by the scaling factor `S` and zero-point `Z`.\n",
    "\n",
    " `S` is simply the ratio of the input range to the output range:\n",
    "\n",
    " $$ S = \\dfrac {\\beta - \\alpha} {\\beta_q - \\alpha_q}$$\n",
    "\n",
    "where [$\\alpha$, $\\beta$] is the clipping range of the input, i.e. the boundaries of permissible inputs. [$\\alpha_q$, $\\beta_q$] is the range in quantized output space that it is mapped to. For 8-bit quantization, the output range:\n",
    "$$ \\beta_q - \\alpha_q <= (2^8 - 1)$$\n",
    "\n",
    "`Z` acts as a bias to ensure that a 0 in the input space maps perfectly to a 0 in the quantized space:\n",
    "$$Z=-(\\dfrac {\\alpha}{S} - \\alpha_q)$$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration\n",
    "\n",
    "The process of choosing the input clipping range is known as calibration. The simplest technique (also the default in PyTorch) is to record the running mininmum and maximum values and assign them to $\\alpha$ and $\\beta$. `TensorRT` also uses entropy minimization (KL divergence), mean-square-error minimization, or percentiles of the input range.\n",
    "\n",
    "In PyTorch, `Observer` modules collect statistics on the input values and calculate the qparams `S`, `Z`. Different calibration schemes result in different quantized outputs, and it’s best to empirically verify which scheme works best for your application and architecture (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-1.3661,  0.6686, -0.7628, -0.1713],\n",
      "        [ 0.5461, -0.9687,  0.5894, -0.1433],\n",
      "        [ 0.0744,  1.8963, -0.9171,  0.0334]]), tensor([[ 0.3307, -1.0928, -0.8456,  1.2337],\n",
      "        [ 0.6180, -1.6796, -0.9699,  0.8957],\n",
      "        [ 0.5770, -1.1889,  0.9793,  1.6308]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, HistogramObserver\n",
    "\n",
    "C, L = 3, 4\n",
    "normal = torch.distributions.normal.Normal(0, 1)\n",
    "inputs = [normal.sample((C, L)), normal.sample((C, L))]\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxObserver (tensor([0.0185]), tensor([152], dtype=torch.int32))\n",
      "MovingAverageMinMaxObserver (tensor([0.0139]), tensor([137], dtype=torch.int32))\n",
      "HistogramObserver (tensor([0.0128]), tensor([159], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "observers = [MinMaxObserver(), MovingAverageMinMaxObserver(), HistogramObserver()]\n",
    "for obs in observers:\n",
    "  for x in inputs:\n",
    "    obs(x)\n",
    "  print(obs.__class__.__name__, obs.calculate_qparams())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine and Symmetric Quantization Schemes\n",
    "\n",
    "__Affine or asymmetric quantization__ schemes assign the input range to the min and max observed values. Affine schemes generally offer tighter clipping ranges and are useful for quantizing non-negative activations (you don’t need the input range to contain negative values if your input tensors are never negative). The range is calculated as $\\alpha=min(r)$, $\\beta=max(r)$. Affine quantization leads to more computationally expensive inference when used for weight tensors.\n",
    "\n",
    "__Symmetric quantization__ schemes center the input range around 0, eliminating the need to calculate a zero-point offset. The range is calculated as:\n",
    "\n",
    "$$ -\\alpha = \\beta = max(|max(r)|, |min(r)|)$$\n",
    "\n",
    " For skewed signals (like non-negative activations) this can result in bad quantization resolution because the clipping range includes values that never show up in the input.\n",
    "\n",
    " In PyTorch, you can specify affine or symmetric schemes while initializing the Observer. Note that not all observers support both schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qscheme: torch.per_tensor_affine | (tensor([0.0128]), tensor([107], dtype=torch.int32))\n",
      "Qscheme: torch.per_tensor_symmetric | (tensor([0.0149]), tensor([128]))\n"
     ]
    }
   ],
   "source": [
    "for qscheme in [torch.per_tensor_affine, torch.per_tensor_symmetric]:\n",
    "  obs = MovingAverageMinMaxObserver(qscheme=qscheme)\n",
    "  for x in inputs:\n",
    "    obs(x)\n",
    "  print(f\"Qscheme: {qscheme} | {obs.calculate_qparams()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per-Tensor and Per-Channel Quantization Schemes\n",
    "\n",
    "Quantization parameters can be calculated for the layer’s entire weight tensor as a whole, or separately for each channel. In per-tensor, the same clipping range is applied to all the channels in a layer\n",
    "\n",
    "<img src=\"fig/per-channel-tensor.svg\">\n",
    "\n",
    "For weights quantization, symmetric-per-channel quantization provides better accuracies; per-tensor quantization performs poorly, possibly due to high variance in conv weights across channels from batchnorm folding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0080, 0.0061, 0.0110]), tensor([171, 159,  83], dtype=torch.int32))\n"
     ]
    }
   ],
   "source": [
    "from torch.quantization.observer import MovingAveragePerChannelMinMaxObserver\n",
    "# Calculate qparams for all 'C' channels separately\n",
    "obs = MovingAveragePerChannelMinMaxObserver(ch_axis=0)\n",
    "for x in inputs:\n",
    "  obs(x)\n",
    "print(obs.calculate_qparams())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backend Engine\n",
    "\n",
    "Currently, quantized operators run on x86 machines via the `FBGEMM` backend, or use `QNNPACK` primitives on ARM machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = 'fbgemm'\n",
    "qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QConfig\n",
    "\n",
    "The `QConfig` NamedTuple stores the `Observers` and the quantization schemes used to quantize activations and weights.\n",
    "\n",
    "Be sure to pass the `Observer` class (not the instance), or a callable that can return Observer instances. Use `with_args()` to override the default arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_qconfig = torch.quantization.QConfig(\n",
    "    activation=MovingAverageMinMaxObserver.with_args(qscheme=torch.per_tensor_affine), \n",
    "    weight=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.qint8)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Quantization\n",
    "\n",
    "PyTorch allows you a few different ways to quantize your model depending on\n",
    "\n",
    "- if you prefer a flexible but manual, or a restricted automagic process (Eager Mode v/s FX Graph Mode)\n",
    "- if qparams for quantizing activations (layer outputs) are precomputed for all inputs, or calculated afresh with each input (static v/s dynamic),\n",
    "- if qparams are computed with or without retraining (quantization-aware training v/s post-training quantization)\n",
    "\n",
    "FX Graph Mode automatically fuses eligible modules, inserts Quant/DeQuant stubs, calibrates the model and returns a quantized module - all in two method calls - but only for networks that are `symbolic traceable`. The examples below contain the calls using Eager Mode and FX Graph Mode for comparison.\n",
    "\n",
    "In DNNs, eligible candidates for quantization are the FP32 weights (layer parameters) and activations (layer outputs). Quantizing weights reduces the model size. Quantized activations typically result in faster inference.\n",
    "\n",
    "As an example, the 50-layer ResNet network has ~26 million weight parameters and computes ~16 million activations in the forward pass."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Static Quantization (PTQ)\n",
    "\n",
    "PTQ also pre-quantizes model weights but instead of calibrating activations on-the-fly, the clipping range is pre-calibrated and fixed (“static”) using validation data. Activations stay in quantized precision between operations during inference. About 100 mini-batches of representative data are sufficient to calibrate the observers. The examples below use random data in calibration for convenience - using that in your application will result in bad qparams.\n",
    "\n",
    "<img src=\"fig/ptq-flowchart.svg\">\n",
    "\n",
    "`Module fusion` combines multiple sequential modules (eg: [Conv2d, BatchNorm, ReLU]) into one. Fusing modules means the compiler needs to only run one kernel instead of many; this speeds things up and improves accuracy by reducing quantization error.\n",
    "\n",
    "- (+) Static quantization has faster inference than dynamic quantization because it eliminates the float<->int conversion costs between layers.\n",
    "\n",
    "- (-) Static quantized models may need regular re-calibration to stay robust against distribution-drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongbing/venv/torch1.13.0/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Static quantization of a model consists of the following steps:\n",
    "\n",
    "#     Fuse modules\n",
    "#     Insert Quant/DeQuant Stubs\n",
    "#     Prepare the fused module (insert observers before and after layers)\n",
    "#     Calibrate the prepared module (pass it representative data)\n",
    "#     Convert the calibrated module (replace with quantized version)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "# running on a x86 CPU, Use \"qnnpack\" if running on ARM\n",
    "backend = \"fbgemm\"\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=2, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# EAGER MODE\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "\n",
    "# Fuse modules\n",
    "# fuse first Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)\n",
    "# fuse second Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, ['2', '3'], inplace=True)\n",
    "\n",
    "# Insert stubs\n",
    "m = nn.Sequential(torch.quantization.QuantStub(),\n",
    "                  *m,\n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "# Prepare\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare(m, inplace=True)\n",
    "\n",
    "# Calibarate\n",
    "# This example uses random data for convenience. Use representation (validation) data instead\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        x = torch.rand(1, 2, 28, 28)\n",
    "        m(x)\n",
    "\n",
    "# convert\n",
    "torch.quantization.convert(m, inplace=True)\n",
    "\n",
    "# Check\n",
    "# 1 byte instead of 4 bytes for FP32\n",
    "print(m[1].weight().element_size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FX GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "prepare_fx() missing 1 required positional argument: 'example_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m qconfig_dict \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:torch\u001b[39m.\u001b[39mquantization\u001b[39m.\u001b[39mget_default_qconfig(backend)}\n\u001b[1;32m      8\u001b[0m \u001b[39m# Prepare\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model_prepared \u001b[39m=\u001b[39m quantize_fx\u001b[39m.\u001b[39;49mprepare_fx(m, qconfig_dict)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Calibrate - Use representative (validation) data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n",
      "\u001b[0;31mTypeError\u001b[0m: prepare_fx() missing 1 required positional argument: 'example_inputs'"
     ]
    }
   ],
   "source": [
    "from torch.quantization import quantize_fx\n",
    "\n",
    "m = copy.deepcopy(model)\n",
    "m.eval()\n",
    "\n",
    "qconfig_dict = {\"\":torch.quantization.get_default_qconfig(backend)}\n",
    "\n",
    "# Prepare\n",
    "model_prepared = quantize_fx.prepare_fx(m, qconfig_dict)\n",
    "\n",
    "# Calibrate - Use representative (validation) data\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        x = torch.rand(1, 2, 28, 28)\n",
    "        model_prepared(x)\n",
    "\n",
    "# Quantize\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "\n",
    "print(model_prepared[1].weight().element_size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization-Aware Training (QAT)\n",
    "\n",
    "<img src=\"fig/qat-flowchart.svg\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PTQ approach is great for large models, but accuracy suffers in smaller models. QAT tackles this by including this quantization error in the training loss, thereby training an INT8-first model.\n",
    "\n",
    "All weights and biases are stored in FP32, and backpropagation happens as usual. However in the forward pass, quantization is internally simulated via FakeQuantize modules. They are called fake because they quantize and immediately dequantize the data, adding quantization noise similar to what might be encountered during quantized inference. The final loss thus accounts for any expected quantization errors. Optimizing on this allows the model to identify FP32 parameters such that quantizing them to INT8 does not significantly affect accuracy.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"fig/qat-fake-quantization.png\">\n",
    "</p>\n",
    "\n",
    "- (+) QAT yields higher accuracies than PTQ.\n",
    "\n",
    "- (+) Qparams can be learned during model training for more fine-grained accuracy\n",
    "\n",
    "- (-) Computational cost of retraining a model in QAT can be several hundred epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Quantize(scale=tensor([0.0081]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (1): QuantizedConvReLU2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.012540503405034542, zero_point=0)\n",
       "  (2): Identity()\n",
       "  (3): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0053305355831980705, zero_point=0)\n",
       "  (4): Identity()\n",
       "  (5): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QAT follows the same steps as PTQ, with the exception of the training loop before you actually convert the model to its quantized version\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "backend = \"fbgemm\"\n",
    "\n",
    "m = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "    nn.ReLU()\n",
    ")\n",
    "\n",
    "# Fuse\n",
    "# Fuse first Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, [\"0\", \"1\"], inplace=True)\n",
    "# Fuse second Conv-ReLU pair\n",
    "torch.quantization.fuse_modules(m, [\"2\", \"3\"], inplace=True)\n",
    "\n",
    "# Insert stubs\n",
    "m = nn.Sequential(torch.quantization.QuantStub(),\n",
    "                  *m,\n",
    "                  torch.quantization.DeQuantStub())\n",
    "\n",
    "# Prepare\n",
    "m.train()\n",
    "m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.quantization.prepare_qat(m, inplace=True)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "opt = torch.optim.SGD(m.parameters(), lr=0.1)\n",
    "loss_fn = lambda out, tgt: torch.pow(tgt-out, 2).mean()\n",
    "for epoch in range(n_epochs):\n",
    "    x = torch.rand(10, 3, 28, 28)\n",
    "    out = m(x)\n",
    "    loss = loss_fn(out, torch.rand_like(out))\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "# Convert\n",
    "m.eval()\n",
    "torch.quantization.convert(m, inplace=True)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
