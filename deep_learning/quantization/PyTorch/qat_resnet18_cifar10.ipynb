{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Static quantization allows the user to generate quantized integer model that is highly efficient during inference. However, sometimes, even with careful post-training calibration, the model accuracies might be sacrificed to some extent that is not acceptable. If this is the case, we will train the model in a way so that the quantization effect has been taken into account. Quantization aware training is capable of modeling the quantization effect during training.\n",
    "\n",
    "The mechanism of quantization aware training is simple, it places fake quantization modules, i.e., quantization and dequantization modules, at the places where quantization happens during floating-point model to quantized integer model conversion, to simulate the effects of clamping and rounding brought by integer quantization. The fake quantization modules will also monitor scales and zero points of the weights and activations. Once the quantization aware training is finished, the floating point model could be converted to quantized integer model immediately using the information stored in the fake quantization modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Quantization Aware Training\n",
    "\n",
    "The workflow could be as easy as loading a pre-trained floating point model and apply a quantization aware training wrapper. However, without doing layer fusion, sometimes such kind of easy manipulation would not result in good model performances.\n",
    "\n",
    "In this case, I will also use the ResNet18 from TorchVision models as an example. All the steps prior, to the quantization aware training steps, including layer fusion and skip connections replacement, are the same as to post-training quantization\n",
    "\n",
    "The quantization aware training steps are also very similar to post-training calibration and quantization:\n",
    "\n",
    "1. Train a floating point model or load a pre-trained floating point model.\n",
    "2. Move the model to CPU and switch model to evaluation mode. Layer fusion only works in evaluation mode.\n",
    "3. Apply layer fusion.\n",
    "4. Check if the layer fusion results in correct model, and switch back to training mode.\n",
    "5. Apply `torch.quantization.QuantStub()` and `torch.quantization.QuantStub()` to the inputs and outputs, respectively.\n",
    "6. Specify quantization configurations, such as symmetric quantization or asymmetric quantization, etc.\n",
    "7. Prepare quantization model for quantization aware training.\n",
    "8. Move the model to CUDA and run quantization aware training using CUDA.\n",
    "9. Move the model to CPU and convert the quantization aware trained floating point model to quantized integer model.\n",
    "10. [Optional] Verify accuracies and inference performance gain.\n",
    "11. Save the quantized integer model."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch1.13.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
