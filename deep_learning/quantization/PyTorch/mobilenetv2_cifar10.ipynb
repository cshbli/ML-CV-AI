{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "490680cb",
   "metadata": {},
   "source": [
    "# MobileNetV2 Quantization Aware Training (QAT) on CIFAR10\n",
    "\n",
    "```Note: As of today (01/09/2023), PyTorch Quantization Model Conversion can only be done CPU.```\n",
    "\n",
    "<img src=\"fig/pytorch_qat_flow.png\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f281eaca",
   "metadata": {},
   "source": [
    "## 0. Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1e918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429cfa84",
   "metadata": {},
   "source": [
    "## 1. Model Modificiations: Replace Functionals with Modules\n",
    "\n",
    "- Replacing addition `+` with `nn.quantized.FloatFunctional().add()` module function\n",
    "\n",
    "- Insert `QuantStub` and `DeQuantStub` at the beginning and end of the network.\n",
    "\n",
    "- Replace ReLU6 with ReLU\n",
    "\n",
    "- Replace last `torch.nn.Linear` layer with `torch.nn.Conv2d` 1x1 kernel, as 1x1 `Conv2d` has better performance than `Linear`.\n",
    "\n",
    "- Define `fuse_module()` functions to specify how to fuse modules. \n",
    "\n",
    "Please check [mobilenetv2.py](./mobilenetv2.py) for all the details.    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba09b0e7",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f41b183",
   "metadata": {},
   "source": [
    "### 2.1 Define dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e694c8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import (DataLoader, TensorDataset)\n",
    "\n",
    "def prepare_data_loaders(data_path, train_batch_size, eval_batch_size, dry_run):\n",
    "    IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224    \n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    if dry_run:\n",
    "        batch_size = 1        \n",
    "        dummy_dataset = TensorDataset(torch.rand(batch_size, 3, 224, 224), torch.randint(0, 10, (batch_size,)))\n",
    "        train_loader = DataLoader(dummy_dataset,\n",
    "                                batch_size=batch_size)\n",
    "        test_loader = DataLoader(dummy_dataset,\n",
    "                                batch_size=1)\n",
    "    else:\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data',train=True, download=True, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data',train=False, download=True, transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23c81e5a",
   "metadata": {},
   "source": [
    "### 2.2 Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ef36bb",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, train_loader, optimizer, criterion, ntrain_batches):    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if batch_idx % 10 == 0:            \n",
    "            print('.', end='')\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        if batch_idx >= ntrain_batches - 1:\n",
    "            print('\\nTraining: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (batch_idx+1, len(train_loader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            return\n",
    "    \n",
    "    print('Full training set: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (batch_idx+1, len(train_loader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return\n",
    "\n",
    "def eval(model, device, test_loader, criterion, neval_batches):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('.', end='')\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            if batch_idx >= neval_batches - 1:\n",
    "                acc = 100.*correct/total\n",
    "                loss = test_loss/(batch_idx+1)\n",
    "\n",
    "                print('\\nEval: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                    % (batch_idx+1, len(test_loader), loss, acc, correct, total))\n",
    "                \n",
    "                return loss, acc\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ddaa1c",
   "metadata": {},
   "source": [
    "### 2.3 Train\n",
    "\n",
    "- It will take long time if using the current settings of epochs, ntrain_batches and neval_batches.\n",
    "\n",
    "- In order to verify the whole flow, you don't have to train and evaluate for large epochs for the model to converge.\n",
    "\n",
    "    - Define smaller epochs, or smaller ntrain_batches, or smaller neval_batches\n",
    "    \n",
    "    - use `dry_run=1`, it will use one random noise sample to train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0b4534d",
   "metadata": {},
   "source": [
    "#### 2.3.1 Define the same data loaders and criterion for float model and QAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bf424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available or not\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Prepare data loader\n",
    "data_path = './data'\n",
    "train_loader, test_loader = prepare_data_loaders(data_path, train_batch_size=32, eval_batch_size=50, dry_run=0)\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61f85bc1",
   "metadata": {},
   "source": [
    "#### 2.3.2 Training\n",
    "\n",
    "- If your float model already trained, you can skip this step and load the pretrained model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c42cc6",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.790 | Acc: 35.296% (17620/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 1.427 | Acc: 47.100% (4710/10000)\n",
      "\n",
      "Epoch: 1\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.373 | Acc: 50.355% (25137/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 1.172 | Acc: 58.250% (5825/10000)\n",
      "\n",
      "Epoch: 2\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.157 | Acc: 58.808% (29357/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 1.049 | Acc: 62.900% (6290/10000)\n",
      "\n",
      "Epoch: 3\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.010 | Acc: 64.465% (32181/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.940 | Acc: 67.730% (6773/10000)\n",
      "\n",
      "Epoch: 4\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.893 | Acc: 68.758% (34324/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.855 | Acc: 70.690% (7069/10000)\n",
      "\n",
      "Epoch: 5\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.791 | Acc: 72.480% (36182/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.722 | Acc: 75.220% (7522/10000)\n",
      "\n",
      "Epoch: 6\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.724 | Acc: 74.816% (37348/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.671 | Acc: 77.410% (7741/10000)\n",
      "\n",
      "Epoch: 7\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.661 | Acc: 77.119% (38498/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.641 | Acc: 78.210% (7821/10000)\n",
      "\n",
      "Epoch: 8\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.613 | Acc: 78.792% (39333/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.579 | Acc: 80.410% (8041/10000)\n",
      "\n",
      "Epoch: 9\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.581 | Acc: 79.862% (39867/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.575 | Acc: 80.350% (8035/10000)\n",
      "\n",
      "Epoch: 10\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.543 | Acc: 81.118% (40494/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.566 | Acc: 80.760% (8076/10000)\n",
      "\n",
      "Epoch: 11\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.520 | Acc: 82.035% (40952/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.544 | Acc: 81.540% (8154/10000)\n",
      "\n",
      "Epoch: 12\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.489 | Acc: 83.157% (41512/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.503 | Acc: 82.990% (8299/10000)\n",
      "\n",
      "Epoch: 13\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.472 | Acc: 83.734% (41800/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.493 | Acc: 83.280% (8328/10000)\n",
      "\n",
      "Epoch: 14\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.453 | Acc: 84.543% (42204/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.469 | Acc: 84.200% (8420/10000)\n",
      "\n",
      "Epoch: 15\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.427 | Acc: 85.144% (42504/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.476 | Acc: 83.730% (8373/10000)\n",
      "\n",
      "Epoch: 16\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.413 | Acc: 85.741% (42802/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.454 | Acc: 84.940% (8494/10000)\n",
      "\n",
      "Epoch: 17\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.396 | Acc: 86.244% (43053/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.445 | Acc: 84.980% (8498/10000)\n",
      "\n",
      "Epoch: 18\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.383 | Acc: 86.717% (43289/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.466 | Acc: 84.220% (8422/10000)\n",
      "\n",
      "Epoch: 19\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.367 | Acc: 87.282% (43571/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.464 | Acc: 84.850% (8485/10000)\n"
     ]
    }
   ],
   "source": [
    "from mobilenetv2 import mobilenet_v2\n",
    "\n",
    "epochs = 20\n",
    "ntrain_batches = 1560  # 50000/32\n",
    "neval_batches = 200  # 10000/50\n",
    "\n",
    "# Construct the mode\n",
    "fp32_model = mobilenet_v2(pretrained=False, progress=True)\n",
    "fp32_model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(fp32_model.parameters(), lr=0.01)\n",
    "\n",
    "for nepoch in range(epochs):    \n",
    "    print(\"\\nEpoch: {}\".format(nepoch))\n",
    "    train_one_epoch(fp32_model, device, train_loader, optimizer, criterion, ntrain_batches=ntrain_batches)\n",
    "\n",
    "    eval(fp32_model, device, test_loader, criterion, neval_batches=neval_batches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23a07fb8",
   "metadata": {},
   "source": [
    "### 2.4 Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae128f8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(fp32_model.state_dict(), \"data/mobilenetv2_cifar10_fp_state_dict.pt\")\n",
    "torch.save(fp32_model, \"data/mobilenetv2_cifar10_fp.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e97fc58",
   "metadata": {},
   "source": [
    "## 3. Model Fusing for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd978b81",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNActivation(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNActivation(\n",
      "    (0): ConvReLU2d(\n",
      "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      (1): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from mobilenetv2 import quantizable_mobilenet_v2\n",
    "\n",
    "# Load pretrained FP32 model\n",
    "qat_model = quantizable_mobilenet_v2(pretrained=False, progress=True).to(device)\n",
    "loaded_dict_enc = torch.load(\"data/mobilenetv2_cifar10_fp_state_dict.pt\", map_location=device)\n",
    "qat_model.load_state_dict(loaded_dict_enc, strict=False)\n",
    "\n",
    "# use CPU on input_tensor as our backend for parsing GraphTopology forced model to be on CPU\n",
    "print('\\n Inverted Residual Block: Before fusion \\n\\n', qat_model.features[1].conv)\n",
    "qat_model.eval()\n",
    "\n",
    "# Fuses modules\n",
    "qat_model.fuse_model()\n",
    "\n",
    "# Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "print('\\n Inverted Residual Block: After fusion\\n\\n',qat_model.features[1].conv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6aa7f6a2",
   "metadata": {},
   "source": [
    "The fused model structure is [fused model structure](./mobilenetv2_cifar10_fused_model.txt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b8bcb84",
   "metadata": {},
   "source": [
    "## 4. Model Preparation for QAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6afcc952",
   "metadata": {},
   "source": [
    "- Try default QConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "054e8b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): ConvBNActivation(\n",
      "    (0): ConvReLU2d(\n",
      "      32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "      )\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "    )\n",
      "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], device='cuda:0'), observer_enabled=tensor([1], device='cuda:0'), scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.ao.quantization as quantizer\n",
    "\n",
    "qat_model.qconfig = quantizer.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# prepare qat model using qconfig settings\n",
    "qat_model.train()\n",
    "quantizer.prepare_qat(qat_model, inplace=True)\n",
    "print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e85c69e8",
   "metadata": {},
   "source": [
    "- Try customerized QConfig\n",
    "\n",
    "    ```Note: the qat_model has to be reset. We can't do prepare_qat() twice. It may have unpredictable behaviours```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9adbdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): ConvBNActivation(\n",
      "    (0): ConvReLU2d(\n",
      "      32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "      (weight_fake_quant): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "      )\n",
      "      (activation_post_process): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): Conv2d(\n",
      "    32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "    (weight_fake_quant): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "    )\n",
      "    (activation_post_process): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "activation_quant = quantizer.FakeQuantize.with_args(\n",
    "            observer=quantizer.MovingAverageMinMaxObserver.with_args(dtype=torch.qint8), \n",
    "            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, reduce_range=False)\n",
    "weight_quant = quantizer.FakeQuantize.with_args(\n",
    "            observer=quantizer.MovingAveragePerChannelMinMaxObserver.with_args(dtype=torch.qint8), \n",
    "            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False)\n",
    "\n",
    "# assign qconfig to model\n",
    "qat_model.qconfig = torch.ao.quantization.QConfig(activation=activation_quant, weight=weight_quant)\n",
    "\n",
    "# prepare qat model using qconfig settings\n",
    "qat_model.train()\n",
    "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e2b015d",
   "metadata": {},
   "source": [
    "The prepared model structure is [prepared model structure](./mobilenetv2_cifar10_qat_prepared.txt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f37ade3f",
   "metadata": {},
   "source": [
    "## 5. Model Tuning with QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76eac8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.306 | Acc: 89.531% (2865/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.457 | Acc: 85.330% (8533/10000)\n",
      "\n",
      "Epoch: 1\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.279 | Acc: 90.219% (2887/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.460 | Acc: 85.210% (8521/10000)\n",
      "\n",
      "Epoch: 2\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.286 | Acc: 89.625% (2868/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.432 | Acc: 85.970% (8597/10000)\n",
      "\n",
      "Epoch: 3\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.277 | Acc: 90.656% (2901/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.429 | Acc: 85.920% (8592/10000)\n",
      "\n",
      "Epoch: 4\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.265 | Acc: 90.906% (2909/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.423 | Acc: 86.100% (8610/10000)\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "ntrain_batches = 100\n",
    "neval_batches = 200  # 10000/50\n",
    "\n",
    "# Redefine optimizer by using smaller learning rate here\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr=0.001)\n",
    "\n",
    "# The \"device\", \"train_loader\", \"test_loader\", \"criterion\" are the same as training the float model\n",
    "qat_fine_tune(qat_model, device, train_loader, test_loader, \n",
    "            optimizer, criterion, \n",
    "            ntrain_batches, neval_batches, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b79d3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state_dict()\n",
    "torch.save(qat_model.state_dict(), \"data/mobilenetv2_cifar10_qat_state_dict.pt\")\n",
    "\n",
    "# Please note we can't save the model structure, since the local observer object can't be serialized\n",
    "# torch.save(qat_model.eval(), \"data/mobilenetv2_cifar10_qat.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71d4d649",
   "metadata": {},
   "source": [
    "## 6. Model Conversion\n",
    "\n",
    "```Note that Model Conversion is currently only supported on CPUs.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "169270d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After quantization and conversion done \n",
      " Sequential(\n",
      "  (0): ConvBNActivation(\n",
      "    (0): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.043570488691329956, zero_point=-128, padding=(1, 1), groups=32)\n",
      "    (1): Identity()\n",
      "    (2): Identity()\n",
      "  )\n",
      "  (1): QuantizedConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.06476081162691116, zero_point=7)\n",
      "  (2): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongbing/venv/torch1.13.0/lib/python3.8/site-packages/torch/ao/quantization/utils.py:287: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quantized_model = torch.ao.quantization.convert(qat_model.to('cpu').eval(), inplace=False)\n",
    "print('Inverted Residual Block: After quantization and conversion done \\n',quantized_model.features[1].conv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "747948a7",
   "metadata": {},
   "source": [
    "## 7. Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87da56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(quantized_model, \"./data/mobilenetv2_cifar10_quantized.pth\")\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"./data/mobilenetv2_cifar10_quantized_jit.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
