{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "490680cb",
   "metadata": {},
   "source": [
    "# MobileNetV2 with CIFAR10\n",
    "\n",
    "\n",
    "<img src=\"fig/pytorch_qat_flow.png\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f281eaca",
   "metadata": {},
   "source": [
    "## 0 Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1e918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.ao.quantization'\n",
    ")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "429cfa84",
   "metadata": {},
   "source": [
    "## 1. Model Modificiations: Replace Functionals with Modules\n",
    "\n",
    "- Replacing addition `+` with `nn.quantized.FloatFunctional().add()` module function\n",
    "\n",
    "- Insert `QuantStub` and `DeQuantStub` at the beginning and end of the network.\n",
    "\n",
    "- Replace ReLU6 with ReLU\n",
    "\n",
    "- Replace last `torch.nn.Linear` layer with `torch.nn.Conv2d` 1x1 kernel, as 1x1 `Conv2d` has better performance than `Linear`.\n",
    "\n",
    "- Define `fuse_module()` functions to specify how to fuse modules. \n",
    "\n",
    "Please check [mobilenetv2.py](./mobilenetv2.py) for all the details.    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba09b0e7",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f41b183",
   "metadata": {},
   "source": [
    "### 2.1 Define dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e694c8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import (DataLoader, TensorDataset)\n",
    "\n",
    "def prepare_data_loaders(data_path, train_batch_size, eval_batch_size, dry_run):\n",
    "    IMAGE_HEIGHT, IMAGE_WIDTH = 224, 224    \n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    if dry_run:\n",
    "        batch_size = 1        \n",
    "        dummy_dataset = TensorDataset(torch.rand(batch_size, 3, 224, 224), torch.randint(0, 10, (batch_size,)))\n",
    "        train_loader = DataLoader(dummy_dataset,\n",
    "                                batch_size=batch_size)\n",
    "        test_loader = DataLoader(dummy_dataset,\n",
    "                                batch_size=1)\n",
    "    else:\n",
    "        trainset = torchvision.datasets.CIFAR10(root='./data',train=True, download=True, transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "        testset = torchvision.datasets.CIFAR10(root='./data',train=False, download=True, transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset, batch_size=eval_batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23c81e5a",
   "metadata": {},
   "source": [
    "### 2.2 Define training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ef36bb",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, device, train_loader, optimizer, criterion, ntrain_batches):    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        if batch_idx % 10 == 0:            \n",
    "            print('.', end='')\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        if batch_idx >= ntrain_batches - 1:\n",
    "            print('\\nTraining: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (batch_idx+1, len(train_loader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            return\n",
    "    \n",
    "    print('Full training set: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (batch_idx+1, len(train_loader), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    return\n",
    "\n",
    "def eval(model, device, test_loader, criterion, neval_batches):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('.', end='')\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            if batch_idx >= neval_batches - 1:\n",
    "                acc = 100.*correct/total\n",
    "                loss = test_loss/(batch_idx+1)\n",
    "\n",
    "                print('\\nEval: [%d/%d] Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                    % (batch_idx+1, len(test_loader), loss, acc, correct, total))\n",
    "                \n",
    "                return loss, acc\n",
    "    \n",
    "    return loss, acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93ddaa1c",
   "metadata": {},
   "source": [
    "### 2.3 Train\n",
    "\n",
    "- It will take long time if using the current settings of epochs, ntrain_batches and neval_batches.\n",
    "\n",
    "- In order to verify the whole flow, you don't have to train and evaluate for large epochs for the model to converge.\n",
    "\n",
    "    - Define smaller epochs, or smaller ntrain_batches, or smaller neval_batches\n",
    "    \n",
    "    - use `dry_run=1`, it will use one random noise sample to train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0b4534d",
   "metadata": {},
   "source": [
    "#### 2.3.1 Define the same data loaders and criterion for float model and QAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bf424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available or not\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Prepare data loader\n",
    "data_path = './data'\n",
    "train_loader, test_loader = prepare_data_loaders(data_path, train_batch_size=32, eval_batch_size=50, dry_run=0)\n",
    "\n",
    "# Define a loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61f85bc1",
   "metadata": {},
   "source": [
    "#### 2.3.2 Training\n",
    "\n",
    "- If your float model already trained, you can skip this step and load the pretrained model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92c42cc6",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.790 | Acc: 35.296% (17620/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 1.427 | Acc: 47.100% (4710/10000)\n",
      "\n",
      "Epoch: 1\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.373 | Acc: 50.355% (25137/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 1.172 | Acc: 58.250% (5825/10000)\n",
      "\n",
      "Epoch: 2\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.157 | Acc: 58.808% (29357/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 1.049 | Acc: 62.900% (6290/10000)\n",
      "\n",
      "Epoch: 3\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 1.010 | Acc: 64.465% (32181/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.940 | Acc: 67.730% (6773/10000)\n",
      "\n",
      "Epoch: 4\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.893 | Acc: 68.758% (34324/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.855 | Acc: 70.690% (7069/10000)\n",
      "\n",
      "Epoch: 5\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.791 | Acc: 72.480% (36182/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.722 | Acc: 75.220% (7522/10000)\n",
      "\n",
      "Epoch: 6\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.724 | Acc: 74.816% (37348/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.671 | Acc: 77.410% (7741/10000)\n",
      "\n",
      "Epoch: 7\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.661 | Acc: 77.119% (38498/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.641 | Acc: 78.210% (7821/10000)\n",
      "\n",
      "Epoch: 8\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.613 | Acc: 78.792% (39333/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.579 | Acc: 80.410% (8041/10000)\n",
      "\n",
      "Epoch: 9\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.581 | Acc: 79.862% (39867/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.575 | Acc: 80.350% (8035/10000)\n",
      "\n",
      "Epoch: 10\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.543 | Acc: 81.118% (40494/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.566 | Acc: 80.760% (8076/10000)\n",
      "\n",
      "Epoch: 11\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.520 | Acc: 82.035% (40952/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.544 | Acc: 81.540% (8154/10000)\n",
      "\n",
      "Epoch: 12\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.489 | Acc: 83.157% (41512/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.503 | Acc: 82.990% (8299/10000)\n",
      "\n",
      "Epoch: 13\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.472 | Acc: 83.734% (41800/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.493 | Acc: 83.280% (8328/10000)\n",
      "\n",
      "Epoch: 14\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.453 | Acc: 84.543% (42204/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.469 | Acc: 84.200% (8420/10000)\n",
      "\n",
      "Epoch: 15\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.427 | Acc: 85.144% (42504/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.476 | Acc: 83.730% (8373/10000)\n",
      "\n",
      "Epoch: 16\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.413 | Acc: 85.741% (42802/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.454 | Acc: 84.940% (8494/10000)\n",
      "\n",
      "Epoch: 17\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.396 | Acc: 86.244% (43053/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.445 | Acc: 84.980% (8498/10000)\n",
      "\n",
      "Epoch: 18\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.383 | Acc: 86.717% (43289/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.466 | Acc: 84.220% (8422/10000)\n",
      "\n",
      "Epoch: 19\n",
      "............................................................................................................................................................\n",
      "Training: [1560/1563] Loss: 0.367 | Acc: 87.282% (43571/49920)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.464 | Acc: 84.850% (8485/10000)\n"
     ]
    }
   ],
   "source": [
    "from mobilenetv2 import mobilenet_v2\n",
    "\n",
    "epochs = 20\n",
    "ntrain_batches = 1560  # 50000/32\n",
    "neval_batches = 200  # 10000/50\n",
    "\n",
    "# Construct the mode\n",
    "fp32_model = mobilenet_v2(pretrained=False, progress=True)\n",
    "fp32_model.to(device)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(fp32_model.parameters(), lr=0.01)\n",
    "\n",
    "for nepoch in range(epochs):    \n",
    "    print(\"\\nEpoch: {}\".format(nepoch))\n",
    "    train_one_epoch(fp32_model, device, train_loader, optimizer, criterion, ntrain_batches=ntrain_batches)\n",
    "\n",
    "    eval(fp32_model, device, test_loader, criterion, neval_batches=neval_batches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23a07fb8",
   "metadata": {},
   "source": [
    "### 2.4 Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae128f8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(fp32_model.state_dict(), \"data/mobilenetv2_cifar10_fp_state_dict.pt\")\n",
    "torch.save(fp32_model, \"data/mobilenetv2_cifar10_fp.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e97fc58",
   "metadata": {},
   "source": [
    "## 3. Model Fusing for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd978b81",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    }
   },
   "outputs": [],
   "source": [
    "from mobilenetv2 import quantizable_mobilenet_v2\n",
    "\n",
    "# Load pretrained FP32 model\n",
    "qat_model = quantizable_mobilenet_v2(pretrained=False, progress=True).to(device)\n",
    "loaded_dict_enc = torch.load(\"data/mobilenetv2_cifar10_fp_state_dict.pt\", map_location=device)\n",
    "qat_model.load_state_dict(loaded_dict_enc, strict=False)\n",
    "\n",
    "# use CPU on input_tensor as our backend for parsing GraphTopology forced model to be on CPU\n",
    "qat_model.eval()\n",
    "qat_model.fuse_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49093a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizableMobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNActivation(\n",
      "      (0): ConvReLU2d(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "    )\n",
      "    (1): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (10): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (11): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (13): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (14): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (16): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (17): QuantizableInvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNActivation(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (18): ConvBNActivation(\n",
      "      (0): ConvReLU2d(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Reshape()\n",
      "    (2): Conv2d(1280, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): Reshape()\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(qat_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b8bcb84",
   "metadata": {},
   "source": [
    "## 4. Model Preparation for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "054e8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizableMobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): ConvReLU2d(\n",
       "        3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (1): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): Conv2d(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (2): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): ConvReLU2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNActivation(\n",
       "      (0): ConvReLU2d(\n",
       "        320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Reshape()\n",
       "    (2): Conv2d(\n",
       "      1280, 1000, kernel_size=(1, 1), stride=(1, 1)\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (3): Reshape()\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0', dtype=torch.int32)\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_quant = torch.ao.quantization.FakeQuantize.with_args(\n",
    "            observer=quantizer.MovingAverageMinMaxObserver.with_args(dtype=torch.qint8), \n",
    "            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, reduce_range=False)\n",
    "weight_quant = torch.ao.quantization.FakeQuantize.with_args(\n",
    "            observer=quantizer.MovingAveragePerChannelMinMaxObserver.with_args(dtype=torch.qint8), \n",
    "            quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_affine, reduce_range=False)            \n",
    "\n",
    "# assign qconfig to model\n",
    "qat_model.qconfig = torch.ao.quantization.QConfig(activation=activation_quant, weight=weight_quant)\n",
    "\n",
    "# prepare qat model using qconfig settings\n",
    "qat_model.train()\n",
    "torch.ao.quantization.prepare_qat(qat_model, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f37ade3f",
   "metadata": {},
   "source": [
    "## 5. Model Tuning with QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6479c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qat_fine_tune(model, device, train_loader, test_loader, \n",
    "                  optimizer, criterion, \n",
    "                  ntrain_batches, neval_batches, epochs):\n",
    "    print(\"\\nEpoch: 0\")\n",
    "    # train one epoch first\n",
    "    train_one_epoch(model, device, train_loader, optimizer, criterion, ntrain_batches=ntrain_batches)\n",
    "\n",
    "    # Freeze quantizer parameters\n",
    "    model.apply(torch.quantization.disable_observer)\n",
    "        \n",
    "    # Freeze batch norm mean and variance estimates\n",
    "    model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    eval(model, device, test_loader, criterion, neval_batches=neval_batches)\n",
    "\n",
    "    # QAT takes time and one needs to train over a few epochs.\n",
    "    # Train and check accuracy after each epoch\n",
    "    for epoch in range(1, epochs):\n",
    "        print(\"\\nEpoch: {}\".format(epoch))\n",
    "        train_one_epoch(model, device, train_loader, optimizer, criterion, ntrain_batches=ntrain_batches)\n",
    "\n",
    "        eval(model, device, test_loader, criterion, neval_batches=neval_batches)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76eac8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.304 | Acc: 89.469% (2863/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.455 | Acc: 85.320% (8532/10000)\n",
      "\n",
      "Epoch: 1\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.277 | Acc: 90.000% (2880/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.451 | Acc: 85.480% (8548/10000)\n",
      "\n",
      "Epoch: 2\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.288 | Acc: 89.594% (2867/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.436 | Acc: 85.840% (8584/10000)\n",
      "\n",
      "Epoch: 3\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.278 | Acc: 90.312% (2890/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.429 | Acc: 85.990% (8599/10000)\n",
      "\n",
      "Epoch: 4\n",
      "..........\n",
      "Training: [100/1563] Loss: 0.267 | Acc: 90.969% (2911/3200)\n",
      "....................\n",
      "Eval: [200/200] Loss: 0.427 | Acc: 85.880% (8588/10000)\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "ntrain_batches = 100\n",
    "neval_batches = 200  # 10000/50\n",
    "\n",
    "# Redefine optimizer by using smaller learning rate here\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr=0.001)\n",
    "\n",
    "# The \"device\", \"train_loader\", \"test_loader\", \"criterion\" are the same as training the float model\n",
    "qat_fine_tune(qat_model, device, train_loader, test_loader, \n",
    "            optimizer, criterion, \n",
    "            ntrain_batches, neval_batches, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
