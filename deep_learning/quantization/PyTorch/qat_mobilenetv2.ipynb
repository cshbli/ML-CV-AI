{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "490680cb",
   "metadata": {},
   "source": [
    "# MobileNetV2 QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c663477e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1+cu102\n"
     ]
    }
   ],
   "source": [
    "# Both torch 1.9.1 and 1.13.0 should work\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d4e5ce2",
   "metadata": {},
   "source": [
    "## 1. FP32 Model Baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14a096dc",
   "metadata": {},
   "source": [
    "### 1.1 Create Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d785b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from typing import Callable, Any, Optional, List\n",
    "\n",
    "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_planes: int,\n",
    "        out_planes: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        dilation: int = 1,\n",
    "    ) -> None:\n",
    "        padding = (kernel_size - 1) // 2 * dilation\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.ReLU6\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
    "                      bias=False),\n",
    "            norm_layer(out_planes),\n",
    "            activation_layer(inplace=True)\n",
    "        )\n",
    "        self.out_channels = out_planes\n",
    "\n",
    "\n",
    "# necessary for backwards compatibility\n",
    "ConvBNReLU = ConvBNActivation\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp: int,\n",
    "        oup: int,\n",
    "        stride: int,\n",
    "        expand_ratio: int,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            norm_layer(oup),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.out_channels = oup\n",
    "        self._is_cn = stride > 1\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 1000,\n",
    "        width_mult: float = 1.0,\n",
    "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
    "        round_nearest: int = 8,\n",
    "        block: Optional[Callable[..., nn.Module]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "            block: Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer: Module specifying the normalization layer to use\n",
    "\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features: List[nn.Module] = [ConvBNReLU(3, input_channel, stride=2, norm_layer=norm_layer)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                # QAT Modification, pass kwargs\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer, **kwargs))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
    "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
    "        x = self.features(x)\n",
    "        # Cannot use \"squeeze\" as batch-size can be 1 => must use reshape with x.shape[0]        \n",
    "        # x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.mean([2, 3])\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24d603c8",
   "metadata": {},
   "source": [
    "### 1.2 Define dataset and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4c28a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def prepare_data_loaders(data_path, train_batch_size, eval_batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    # This ImageNet wrapper function expects the data to be one zip file                                     \n",
    "    # dataset = torchvision.datasets.ImageNet(\n",
    "    #     data_path, split=\"train\", transform=transforms.Compose([\n",
    "    #         transforms.RandomResizedCrop(224),\n",
    "    #         transforms.RandomHorizontalFlip(),\n",
    "    #         transforms.ToTensor(),\n",
    "    #         normalize,\n",
    "    #     ]))\n",
    "    dataset = torchvision.datasets.ImageFolder(root=data_path + \"/train\", \n",
    "        transform=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    # dataset_test = torchvision.datasets.ImageNet(\n",
    "    #     data_path, split=\"val\", transform=transforms.Compose([\n",
    "    #         transforms.Resize(256),\n",
    "    #         transforms.CenterCrop(224),\n",
    "    #         transforms.ToTensor(),\n",
    "    #         normalize,\n",
    "    #     ]))\n",
    "    dataset_test = torchvision.datasets.ImageFolder(root=data_path + \"/val\",\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8adf5ab",
   "metadata": {},
   "source": [
    "### 1.3 Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb43490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e884a13",
   "metadata": {},
   "source": [
    "### 1.4 Get Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9763e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 50\n",
    "\n",
    "# Prepare test data loader\n",
    "data_path = '/data/datasets/ImageNet/ilsvrc_2012'\n",
    "train_dataloader, test_dataloader = prepare_data_loaders(data_path, train_batch_size=32, eval_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9a2536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 71.87\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "fp_model = MobileNetV2()\n",
    "state_dict = torch.load(\"data/mobilenet_v2-b0353104.pth\")\n",
    "fp_model.load_state_dict(state_dict)\n",
    "fp_model.to('cpu')\n",
    "\n",
    "num_eval_batches = 1000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "top1, top5 = evaluate(fp_model, criterion, test_dataloader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(fp_model), \"data/mobilenet_v2_scripted.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04ddf064",
   "metadata": {},
   "source": [
    "## 2 PyTorch QAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99dd9ca6",
   "metadata": {},
   "source": [
    "### 2.1 Define Model Architecture\n",
    "\n",
    "- Replacing addition with nn.quantized.FloatFunctional\n",
    "- Insert QuantStub and DeQuantStub at the beginning and end of the network.\n",
    "- Replace ReLU6 with ReLU, otherwise PyTorch QAT has errors while do `fuse_models()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            # Replace with ReLU\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup, momentum=0.1),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return self.skip_add.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "            if type(m) == InvertedResidual:\n",
    "                for idx in range(len(m.conv)):\n",
    "                    if type(m.conv[idx]) == nn.Conv2d:\n",
    "                        torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f76f94",
   "metadata": {},
   "source": [
    "### 2.2 Define dataset and data loaders\n",
    "\n",
    "- The same as FP32."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b94f4150",
   "metadata": {},
   "source": [
    "### 2.3 Define Helper Functions\n",
    "\n",
    "- The same as FP32."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5203800",
   "metadata": {},
   "source": [
    "### 2.4 Training and Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b145c42",
   "metadata": {},
   "source": [
    "#### 2.4.1 Load pretrained float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2370f03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNReLU(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU()\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (18): ConvBNReLU(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qat_model = MobileNetV2()\n",
    "state_dict = torch.load(\"data/mobilenet_v2-b0353104.pth\")\n",
    "qat_model.load_state_dict(state_dict)\n",
    "print(qat_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "023e891c",
   "metadata": {},
   "source": [
    "#### 2.4.2 Fuse Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a10b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNReLU(\n",
      "      (0): ConvReLU2d(\n",
      "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
      "            (1): ReLU()\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (18): ConvBNReLU(\n",
      "      (0): ConvReLU2d(\n",
      "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "qat_model.eval()\n",
    "qat_model.fuse_model()\n",
    "print(qat_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72451733",
   "metadata": {},
   "source": [
    "#### 2.4.3 Prepare for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9a68ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.fake_quantize.FakeQuantize'>, observer=<class 'torch.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, reduce_range=True), weight=functools.partial(<class 'torch.quantization.fake_quantize.FakeQuantize'>, observer=<class 'torch.quantization.observer.MovingAveragePerChannelMinMaxObserver'>, quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0))\n"
     ]
    }
   ],
   "source": [
    "# Using default qconfig\n",
    "quantization_config = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "qat_model.qconfig = quantization_config\n",
    "print(qat_model.qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311f2926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): ConvBNReLU(\n",
      "      (0): ConvReLU2d(\n",
      "        3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (weight_fake_quant): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "        )\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): Conv2d(\n",
      "          32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (2): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (1): ConvBNReLU(\n",
      "          (0): ConvReLU2d(\n",
      "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
      "            (weight_fake_quant): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "            )\n",
      "            (activation_post_process): FakeQuantize(\n",
      "              fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "            )\n",
      "          )\n",
      "          (1): Identity()\n",
      "          (2): Identity()\n",
      "        )\n",
      "        (2): Conv2d(\n",
      "          960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (weight_fake_quant): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "          )\n",
      "          (activation_post_process): FakeQuantize(\n",
      "            fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "          )\n",
      "        )\n",
      "        (3): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): ConvBNReLU(\n",
      "      (0): ConvReLU2d(\n",
      "        320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (weight_fake_quant): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "        )\n",
      "        (activation_post_process): FakeQuantize(\n",
      "          fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "        )\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Identity()\n",
      "    )\n",
      "  )\n",
      "  (quant): QuantStub(\n",
      "    (activation_post_process): FakeQuantize(\n",
      "      fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (dequant): DeQuantStub()\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(\n",
      "      in_features=1280, out_features=1000, bias=True\n",
      "      (weight_fake_quant): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, ch_axis=0, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([], device='cuda:0'), max_val=tensor([], device='cuda:0'))\n",
      "      )\n",
      "      (activation_post_process): FakeQuantize(\n",
      "        fake_quant_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), observer_enabled=tensor([1], device='cuda:0', dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.], device='cuda:0'), zero_point=tensor([0], device='cuda:0')\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "qat_model.train()\n",
    "qat_model.to(device)\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print(qat_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0dca2a9",
   "metadata": {},
   "source": [
    "#### 2.4.4 Training and Testing\n",
    "\n",
    "- Training can be done on GPU/CUDA, testing has to be done on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba06661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "010cb880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Loss tensor(1.7670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 59.688 Acc@5 79.844\n",
      "..........Epoch 0 :Evaluation accuracy on 500 images, 75.80\n",
      "....................Loss tensor(1.4418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 66.406 Acc@5 84.219\n",
      "..........Epoch 1 :Evaluation accuracy on 500 images, 78.00\n",
      "....................Loss tensor(1.5081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 62.812 Acc@5 84.375\n",
      "..........Epoch 2 :Evaluation accuracy on 500 images, 77.40\n",
      "....................Loss tensor(1.5501, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 64.219 Acc@5 84.062\n",
      "..........Epoch 3 :Evaluation accuracy on 500 images, 74.80\n",
      "....................Loss tensor(1.4629, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 65.000 Acc@5 85.625\n",
      "..........Epoch 4 :Evaluation accuracy on 500 images, 74.80\n",
      "....................Loss tensor(1.3761, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 65.938 Acc@5 87.031\n",
      "..........Epoch 5 :Evaluation accuracy on 500 images, 77.00\n",
      "....................Loss tensor(1.4489, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 67.031 Acc@5 85.938\n",
      "..........Epoch 6 :Evaluation accuracy on 500 images, 75.80\n",
      "....................Loss tensor(1.3387, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 67.969 Acc@5 86.562\n",
      "..........Epoch 7 :Evaluation accuracy on 500 images, 74.80\n"
     ]
    }
   ],
   "source": [
    "num_train_batches = 20\n",
    "num_eval_batches = 10\n",
    "eval_batch_size = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "\n",
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(8):\n",
    "    qat_model.to(device)\n",
    "    train_one_epoch(qat_model, criterion, optimizer, train_dataloader, device, num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    qat_model.to('cpu')\n",
    "    quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
    "    quantized_model.eval()    \n",
    "    top1, top5 = evaluate(quantized_model, criterion, test_dataloader, neval_batches=num_eval_batches)\n",
    "    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebc59e46",
   "metadata": {},
   "source": [
    "## 3 BST QAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f617889",
   "metadata": {},
   "source": [
    "### 3.0 Make sure bstnnx_training package is in the system python path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d22b8ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.3.2\n"
     ]
    }
   ],
   "source": [
    "# Make sure bstnnx_training is in the system Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/home/hongbing/Projects/bst-study/bstnnx_training\")\n",
    "\n",
    "import bstnnx_training\n",
    "\n",
    "print(bstnnx_training.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c835820",
   "metadata": {},
   "source": [
    "### 3.1 Define Model Architecture\n",
    "\n",
    "- Comparing to PyTorch QAT, we only need replacing addition with torch.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13809755",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from bstnnx_training.PyTorch.QAT import modules as bst\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            nn.BatchNorm2d(out_planes, momentum=0.1),\n",
    "            # Replace with ReLU\n",
    "            nn.ReLU(inplace=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup, momentum=0.1),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        # Replace torch.add with floatfunctional\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return torch.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n",
    "        \"\"\"\n",
    "        MobileNet V2 main class\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 2],\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features = [ConvBNReLU(3, input_channel, stride=2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "            if type(m) == InvertedResidual:\n",
    "                for idx in range(len(m.conv)):\n",
    "                    if type(m.conv[idx]) == nn.Conv2d:\n",
    "                        torch.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e19e57e",
   "metadata": {},
   "source": [
    "### 3.2 Define dataset and data loaders\n",
    "\n",
    "- The same as FP32 and PyTorch QAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91ec5903",
   "metadata": {},
   "source": [
    "### 3.3 Define Helper Functions\n",
    "\n",
    "- The Same as FP32 and PyTorch QAT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd2fa126",
   "metadata": {},
   "source": [
    "### 3.4 Training and Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f9aa2f7",
   "metadata": {},
   "source": [
    "#### 3.4.1 Load pretrained float model\n",
    "\n",
    "- The same as PyTorch QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04a1fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_model = MobileNetV2()\n",
    "state_dict = torch.load(\"data/mobilenet_v2-b0353104.pth\")\n",
    "bst_model.load_state_dict(state_dict)\n",
    "# print(qat_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f09e5d43",
   "metadata": {},
   "source": [
    "#### 3.4.2 Fuse Model\n",
    "\n",
    "- The same as PyTorch QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96a0aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_model.eval()\n",
    "bst_model.fuse_model()\n",
    "# print(bst_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed076ccc",
   "metadata": {},
   "source": [
    "#### 3.4.3 Prepare for BST QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6c5b9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongbing/Projects/bst-study/bstnnx_training/bstnnx_training/PyTorch/QAT/core/observer/bst_min_max_observer.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_v = torch.tensor(max(abs(self.min_val), abs(self.max_val)), device=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNReLU(\n",
       "      (0): ConvReLU2d(\n",
       "        3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): Conv2d(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (2): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (1): ConvBNReLU(\n",
       "          (0): ConvReLU2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): Identity()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNReLU(\n",
       "      (0): ConvReLU2d(\n",
       "        320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(\n",
       "      in_features=1280, out_features=1000, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch the quantization framework\n",
    "import bstnnx_training.PyTorch.QAT.core as quantizer\n",
    "\n",
    "bst_activation_quant = quantizer.FakeQuantize.with_args(observer=quantizer.MinMaxObserver.with_args(dtype=torch.qint8), \n",
    "            quant_min=-128, \n",
    "            quant_max=127, \n",
    "            dtype=torch.qint8, \n",
    "            qscheme=torch.per_tensor_affine, \n",
    "            reduce_range=False)\n",
    "bst_weight_quant = quantizer.FakeQuantize.with_args(observer=quantizer.MinMaxObserver.with_args(dtype=torch.qint8), \n",
    "            quant_min=-128, \n",
    "            quant_max=127, \n",
    "            dtype=torch.qint8, \n",
    "            qscheme=torch.per_tensor_affine, \n",
    "            reduce_range=False)\n",
    "        \n",
    "# 1) [bst_alignment] get b0 pre-bind qconfig adjusting Conv's activation quant scheme\n",
    "sample_data = torch.randn(1, 3, 224, 224, requires_grad=True)\n",
    "pre_bind_qconfig = quantizer.pre_bind(bst_model, input_tensor=sample_data.to('cpu'))\n",
    "        \n",
    "# 2) assign qconfig to model\n",
    "bst_model.qconfig = quantizer.QConfig(activation=bst_activation_quant, \n",
    "                                        weight=bst_weight_quant,\n",
    "                                        qconfig_dict=pre_bind_qconfig)\n",
    "        \n",
    "# 3) prepare qat model using qconfig settings\n",
    "bst_model.train()\n",
    "prepared_model = quantizer.prepare_qat(bst_model, inplace=False)  \n",
    "        \n",
    "# 4) [bst_alignment] link model observers\n",
    "prepared_model = quantizer.link_modules(prepared_model, auto_detect=True, input_tensor=sample_data.to('cpu'), inplace=False)\n",
    "    \n",
    "prepared_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16054a6b",
   "metadata": {},
   "source": [
    "#### 2.4.4 Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b694362a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongbing/Projects/bst-study/bstnnx_training/bstnnx_training/PyTorch/QAT/core/observer/bst_min_max_observer.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_v = torch.tensor(max(abs(self.min_val), abs(self.max_val)), device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................Loss tensor(1.9692, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 57.969 Acc@5 78.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongbing/Projects/bst-study/bstnnx_training/bstnnx_training/PyTorch/QAT/core/observer/bst_min_max_observer.py:20: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Epoch 0 :Evaluation accuracy on 500 images, 73.60\n",
      "....................Loss tensor(1.8329, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 57.812 Acc@5 81.406\n",
      "..........Epoch 1 :Evaluation accuracy on 500 images, 69.80\n",
      "....................Loss tensor(1.9330, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 58.750 Acc@5 77.969\n",
      "..........Epoch 2 :Evaluation accuracy on 500 images, 72.00\n",
      "....................Loss tensor(1.7490, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 58.906 Acc@5 82.031\n",
      "..........Epoch 3 :Evaluation accuracy on 500 images, 72.20\n",
      "....................Loss tensor(1.8002, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 57.500 Acc@5 80.938\n",
      "..........Epoch 4 :Evaluation accuracy on 500 images, 71.60\n",
      "....................Loss tensor(1.7291, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 59.531 Acc@5 80.781\n",
      "..........Epoch 5 :Evaluation accuracy on 500 images, 71.00\n",
      "....................Loss tensor(1.6262, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 61.094 Acc@5 82.812\n",
      "..........Epoch 6 :Evaluation accuracy on 500 images, 68.40\n",
      "....................Loss tensor(1.8862, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 60.625 Acc@5 78.750\n",
      "..........Epoch 7 :Evaluation accuracy on 500 images, 72.20\n"
     ]
    }
   ],
   "source": [
    "model = prepared_model\n",
    "\n",
    "num_train_batches = 20\n",
    "num_eval_batches = 10\n",
    "eval_batch_size = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)\n",
    "\n",
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(8):\n",
    "    model.to(device)\n",
    "    train_one_epoch(model, criterion, optimizer, train_dataloader, device, num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        model.apply(torch.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "    # Check the accuracy after each epoch\n",
    "    model.to('cpu')\n",
    "    quantized_model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "    quantized_model.eval()    \n",
    "    top1, top5 = evaluate(model, criterion, test_dataloader, neval_batches=num_eval_batches)\n",
    "    print('\\nEpoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c046626",
   "metadata": {},
   "source": [
    "### 3.5 Export float ONNX model and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e87c0a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conv_12's activation is not recorded in tensor qparams\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'987'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m stage_dict\u001b[39m=\u001b[39m{}\n\u001b[1;32m      4\u001b[0m stage_dict[\u001b[39m'\u001b[39m\u001b[39msimplify_onnx\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m onnx_model_path, quant_param_json_path \u001b[39m=\u001b[39m quantizer\u001b[39m.\u001b[39;49mexport_onnx(prepared_model, \n\u001b[1;32m      6\u001b[0m                                                                sample_data, \n\u001b[1;32m      7\u001b[0m                                                                stage_dict\u001b[39m=\u001b[39;49mstage_dict, \n\u001b[1;32m      8\u001b[0m                                                                result_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Projects/bst-study/bstnnx_training/bstnnx_training/PyTorch/QAT/core/converter.py:570\u001b[0m, in \u001b[0;36mexport_onnx\u001b[0;34m(prepared_model, sample_inputs, stage_dict, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     onnx\u001b[39m.\u001b[39mchecker\u001b[39m.\u001b[39mcheck_model(inferred_model)\n\u001b[1;32m    569\u001b[0m \u001b[39m# Step6: optimize model with hardware contraints\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m opt_frozen_model, opt_quant_js \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49moptimize(inferred_model, qparams, device_engine\u001b[39m=\u001b[39;49mdevice_engine)\n\u001b[1;32m    571\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mQAT Json optimization complete\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m \u001b[39m# Step7: process optimized json\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/bst-study/bstnnx_training/bstnnx_training/opt/optimizer.py:72\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m(onnx_model, quant_js, device_engine, mapping, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m optimizer \u001b[39m=\u001b[39m Optimizer(onnx_model, quant_js, device_engine, mapping, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     71\u001b[0m optimizer\u001b[39m.\u001b[39mprepare()\n\u001b[0;32m---> 72\u001b[0m optimizer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     73\u001b[0m optimizer\u001b[39m.\u001b[39mdone()\n\u001b[1;32m     75\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mmodel, optimizer\u001b[39m.\u001b[39mquant_js\n",
      "File \u001b[0;32m~/Projects/bst-study/bstnnx_training/bstnnx_training/opt/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     optimization_pass \u001b[39m=\u001b[39m opt\n\u001b[0;32m--> 140\u001b[0m optimization_pass(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquant_js)\n",
      "File \u001b[0;32m~/Projects/bst-study/bstnnx_training/bstnnx_training/opt/opt_pass/assign_per_channel_axis.py:127\u001b[0m, in \u001b[0;36massign_per_channel_axis\u001b[0;34m(model, quant_js)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m node_name \u001b[39min\u001b[39;00m graph\u001b[39m.\u001b[39mnode_list:\n\u001b[1;32m    126\u001b[0m     node \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mnode_map[node_name]\n\u001b[0;32m--> 127\u001b[0m     update_channel_axis_by_node(node, tensor_qparams)\n\u001b[1;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m input_name \u001b[39min\u001b[39;00m graph\u001b[39m.\u001b[39mget_real_inputs():\n\u001b[1;32m    130\u001b[0m     tensor_qparams[input_name][\u001b[39m'\u001b[39m\u001b[39mper_channel_axis\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m QUANT_CHANNEL_AXIS([\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Projects/bst-study/bstnnx_training/bstnnx_training/opt/opt_pass/assign_per_channel_axis.py:108\u001b[0m, in \u001b[0;36mupdate_channel_axis_by_node\u001b[0;34m(node, tensor_qparams)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_channel_axis_by_node\u001b[39m(node, tensor_qparams):\n\u001b[1;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39mop_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mConv\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m         update_conv_per_channel_axis(node, tensor_qparams)\n\u001b[1;32m    109\u001b[0m     \u001b[39melif\u001b[39;00m node\u001b[39m.\u001b[39mop_type \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mConvTranspose\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    110\u001b[0m         update_convtrans_per_channel_axis(node, tensor_qparams)\n",
      "File \u001b[0;32m~/Projects/bst-study/bstnnx_training/bstnnx_training/opt/opt_pass/assign_per_channel_axis.py:53\u001b[0m, in \u001b[0;36mupdate_conv_per_channel_axis\u001b[0;34m(node, tensor_qparams)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39moutput[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tensor_qparams:\n\u001b[1;32m     51\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnode\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39ms activation is not recorded in tensor qparams\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m tensor_qparams[node\u001b[39m.\u001b[39;49moutput[\u001b[39m0\u001b[39;49m]][\u001b[39m'\u001b[39m\u001b[39mper_channel_axis\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m QUANT_CHANNEL_AXIS([\u001b[39m'\u001b[39m\u001b[39mConv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(node\u001b[39m.\u001b[39minput)):\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m node\u001b[39m.\u001b[39minput[i] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m tensor_qparams:\n",
      "\u001b[0;31mKeyError\u001b[0m: '987'"
     ]
    }
   ],
   "source": [
    "sample_data = torch.randn(1, 3, 224, 224, requires_grad=False)\n",
    "\n",
    "stage_dict={}\n",
    "stage_dict['simplify_onnx'] = True\n",
    "onnx_model_path, quant_param_json_path = quantizer.export_onnx(prepared_model, \n",
    "                                                               sample_data, \n",
    "                                                               stage_dict=stage_dict, \n",
    "                                                               result_dir='./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7370a61f",
   "metadata": {},
   "source": [
    "### 4. BST Quantize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4e77bde",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp32_model = mobilenet_v2(pretrained=False, progress=True, use_bstnn=True).to(device)\n",
    "loaded_dict_enc = torch.load(fp32_mobilenet_v2_pt_path, map_location=device)\n",
    "fp32_model.load_state_dict(loaded_dict_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4c1f68d",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "24"
    }
   },
   "outputs": [],
   "source": [
    "float_loss, float_acc = test(fp32_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1df087fa",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "25"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0152559280395508 | Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loss: {float_loss} | Accuracy: {float_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd978b81",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "26"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zihaozhao/.local/lib/python3.7/site-packages/torch/nn/modules/module.py:1051: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "fp32_model = mobilenet_v2(pretrained=False, progress=True, use_bstnn=True).to(device)\n",
    "loaded_dict_enc = torch.load(fp32_mobilenet_v2_pt_path, map_location=device)\n",
    "fp32_model.load_state_dict(loaded_dict_enc)\n",
    "\n",
    "# define test data used for fusing model\n",
    "random_data = np.random.rand(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH).astype(\"float32\")\n",
    "sample_data = torch.from_numpy(random_data).to(device)\n",
    "\n",
    "# use CPU on input_tensor as our backend for parsing GraphTopology forced model to be on CPU\n",
    "fp32_model.eval()\n",
    "fused_model = quantizer.fuse_modules(fp32_model, auto_detect=True, input_tensor=sample_data.cpu())\n",
    "prepared_model = quantize_model(fused_model, device, backend=\"bst\", sample_data=sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5cadad95",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "27"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizableMobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): ConvBNActivation(\n",
       "      (0): Conv2d(\n",
       "        3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): Conv2d(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (2): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (14): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (15): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (16): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-1024, quant_max=1023, dtype=torch.int16, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (17): QuantizableInvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ConvBNActivation(\n",
       "          (0): Conv2d(\n",
       "            960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960\n",
       "            (weight_fake_quant): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "            (activation_post_process): FakeQuantize(\n",
       "              fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "              (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "            )\n",
       "          )\n",
       "          (1): Identity()\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): Conv2d(\n",
       "          960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (weight_fake_quant): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "          (activation_post_process): FakeQuantize(\n",
       "            fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "            (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "          )\n",
       "        )\n",
       "        (3): Identity()\n",
       "      )\n",
       "      (add): Add(\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (18): ConvBNActivation(\n",
       "      (0): Conv2d(\n",
       "        320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (weight_fake_quant): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "        (activation_post_process): FakeQuantize(\n",
       "          fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "          (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): A1000A0AvgPool2d(\n",
       "    kernel_size=(7, 7), stride=(7, 7), padding=0\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Reshape()\n",
       "    (2): Conv2d(\n",
       "      1280, 1000, kernel_size=(1, 1), stride=(1, 1)\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (3): Reshape()\n",
       "  )\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0])\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f97e28",
   "metadata": {},
   "source": [
    "### 5. Quant-aware-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3162c7ae",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "28"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2\n",
      "\n",
      "Epoch: 3\n",
      "\n",
      "Epoch: 4\n",
      "\n",
      "Epoch: 5\n",
      "\n",
      "Epoch: 6\n",
      "\n",
      "Epoch: 7\n",
      "\n",
      "Epoch: 8\n",
      "\n",
      "Epoch: 9\n",
      "\n",
      "Epoch: 10\n"
     ]
    }
   ],
   "source": [
    "quant_aware_training(prepared_model, device, train_loader, sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba82889",
   "metadata": {},
   "source": [
    "### Extra step:  Export float ONNX model and Json and optimize them with hardware constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4517f5ac",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "29"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "100%|| 102/102 [00:00<00:00, 257567.13it/s]\n"
     ]
    }
   ],
   "source": [
    "rand_in = np.random.rand(1, 1, 3, IMAGE_HEIGHT, IMAGE_WIDTH).astype(\"float32\")\n",
    "sample_in = tuple(torch.from_numpy(x) for x in rand_in)\n",
    "stage_dict={}\n",
    "stage_dict['simplify_onnx'] = True\n",
    "onnx_model_path, quant_param_json_path = quantizer.export_onnx(prepared_model, \n",
    "                                                               sample_in, \n",
    "                                                               stage_dict=stage_dict, \n",
    "                                                               result_dir=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3b21f",
   "metadata": {},
   "source": [
    "##  Model conversion flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93b122e9",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "30"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12c2d219",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "31"
    }
   },
   "outputs": [],
   "source": [
    "%%writetemplate $tmp_dir/test_e2e_qat.yaml\n",
    "\n",
    "data_reader_method: random_data_reader\n",
    "model_name: qat_frozen_model\n",
    "model_path: {onnx_model_path}\n",
    "batch_size: 1\n",
    "size_limit: 10\n",
    "non_image_input: True\n",
    "enable_in_scale: True\n",
    "orig_model_format: onnx\n",
    "device_engine: 'A1000B0'\n",
    "stage:\n",
    "  - stage_name: pre_processing_stage\n",
    "    priority: 100\n",
    "  - stage_name: graph_optimization_stage\n",
    "    run_built_in_optimization: True\n",
    "    optimization_passes:\n",
    "      - convert_gemm\n",
    "      - convert_max_pool_to_dsp\n",
    "      - convert_slice\n",
    "      - convert_eltwise_add\n",
    "      - convert_eltwise_mul\n",
    "      - convert_global_avgpool\n",
    "      - fuse_activation\n",
    "      - convert_relu\n",
    "      - fuse_conv_add\n",
    "      - convert_to_skip_node\n",
    "      - convert_resize_to_bst_resize\n",
    "    optimization_parameters:\n",
    "        fuse_conv_add:\n",
    "            quant_params_json_path: {quant_param_json_path}\n",
    "    priority: 200\n",
    "  - stage_name: quantization_stage\n",
    "    quantization_method: bst_standard_quantization_flow\n",
    "    quant_params_json_path: {quant_param_json_path}\n",
    "    priority: 300  \n",
    "  - stage_name: graph_partition_stage\n",
    "    priority: 400\n",
    "  - stage_name: section_binding_stage\n",
    "    priority: 500\n",
    "  - stage_name: code_generation_stage\n",
    "    priority: 600\n",
    "  - stage_name: code_compilation_stage\n",
    "    priority: 700\n",
    "  - stage_name: run_emulation_stage\n",
    "    profiling_mode: 2\n",
    "    priority: 800\n",
    "  - stage_name: partition_evaluation_stage\n",
    "    priority: 900\n",
    "  - stage_name: run_emulation_stage\n",
    "    profiling_mode: 0\n",
    "    priority: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c19b9699",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "32"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = os.path.join(tmp_dir, \"test_e2e_qat.yaml\")\n",
    "RESULT_DIR = output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff639b54",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "33"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.frontend.stage_flow_control_main\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  bstnnx.__version__ = 4.0.8 (\u001b[1mstage_flow_control_main.py\u001b[0m:246)\n",
      "[\u001b[1mbstnnx.frontend.stage_flow_control_main\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  bstnnx.version.git_version = 818a5f312cac177fdcd1ffdbf0e80730ee578bf7 (\u001b[1mstage_flow_control_main.py\u001b[0m:247)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Running on A1000B0 (\u001b[1mstage_flow_control_main.py\u001b[0m:253)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute PreProcessingStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/100_PreProcessingStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function PreProcessingStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.pre_processing_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute PreProcessingStage:100:run (\u001b[1mpre_processing_stage.py\u001b[0m:437)\n",
      "[\u001b[1mbstnnx.frontend.pre_processing_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  The model is valid! (\u001b[1mpre_processing_stage.py\u001b[0m:566)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main_cf._data['device_engine']=A1000B0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function PreProcessingStage.run took 0.3940918445587158 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.pre_processing_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute PreProcessingStage:100:done (\u001b[1mpre_processing_stage.py\u001b[0m:694)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute GraphOptimizationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/200_GraphOptimizationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function GraphOptimizationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.graph_optimization_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute GraphOptimizationStage:200:run (\u001b[1mgraph_optimization_stage.py\u001b[0m:71)\n",
      "[\u001b[1mbstnnx.bst_optimizer.split_group_conv\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Output_channel_unit is 16 (\u001b[1mgraph_optimization_stage.py\u001b[0m:237)\n",
      "[\u001b[1mbstnnx.bst_optimizer.split_group_conv\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Split_method is evenly (\u001b[1mgraph_optimization_stage.py\u001b[0m:237)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_26's group number changed from 32 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_67's group number changed from 96 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_108's group number changed from 144 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_156's group number changed from 144 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_197's group number changed from 192 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_245's group number changed from 192 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_293's group number changed from 192 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_334's group number changed from 384 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_382's group number changed from 384 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_430's group number changed from 384 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_478's group number changed from 384 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_519's group number changed from 576 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_567's group number changed from 576 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_615's group number changed from 576 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_656's group number changed from 960 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_704's group number changed from 960 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.defs.bst_op.fused_activation\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Conv_752's group number changed from 960 to 1 (\u001b[1mgraph_optimization_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.helper.data  \u001b[0m][\u001b[1;33mWARNING\u001b[0m]  no image_process_config parameter, use default value {'image_format': 'RGB888P', 'mean': [0.0, 0.0, 0.0], 'scale': [1.0, 1.0, 1.0]} (\u001b[1mgraph_optimization_stage.py\u001b[0m:289)\n",
      "[\u001b[1mbstnnx.bst_optimizer.convert_global_avgpool\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node AveragePool_789 is a GAP. Converting it to a bst_global_pooling on engine EDP... (\u001b[1mgraph_optimization_stage.py\u001b[0m:318)\n",
      "[\u001b[1mbstnnx.bst_optimizer.convert_to_skip_node\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Converting node Reshape_799 (Reshape) to Skip_Node... (\u001b[1mgraph_optimization_stage.py\u001b[0m:318)\n",
      "[\u001b[1mbstnnx.bst_optimizer.convert_to_skip_node\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Converting node Reshape_801 (Reshape) to Skip_Node... (\u001b[1mgraph_optimization_stage.py\u001b[0m:318)\n",
      "[\u001b[1mbstnnx.bst_optimizer.convert_to_skip_node\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Converting node Reshape_816 (Reshape) to Skip_Node... (\u001b[1mgraph_optimization_stage.py\u001b[0m:318)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function GraphOptimizationStage.run took 4.418373107910156 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.graph_optimization_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute GraphOptimizationStage:200:done (\u001b[1mgraph_optimization_stage.py\u001b[0m:354)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute QuantizationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/300_QuantizationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function QuantizationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.quantization_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute QuantizationStage:300:run (\u001b[1mquantization_stage.py\u001b[0m:75)\n",
      "[\u001b[1mbstnnx.quantization.b0_standard.bst_standard_quantization_flow\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  data reader type is random_data_reader (\u001b[1mquantization_stage.py\u001b[0m:112)\n",
      "[\u001b[1mbstnnx.quantization.b0_standard.utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Loading qparams... (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.quantization.b0_standard.utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Loading software quant params from json... (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.quantization.b0_standard.utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Converting quant params from scaler to list... (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.quantization.b0_standard.utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Inserting move mask info... (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.quantization.quantizer.quantizer_base\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Saving quant params to /bsnn/users/zihaozhao/QAT/alan/results/job0/300_QuantizationStage/quant_param_dict.json (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.quantization.quantizer.quantizer_base\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Saving quant params to /bsnn/users/zihaozhao/QAT/alan/results/job0/300_QuantizationStage/node_quant_param_dict.json (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_122 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_129 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 24 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 24 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 24 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_129_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1056 (\u001b[1mquantization_stage.py\u001b[0m:113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_129_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_129_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_211 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_218 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 32 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 32 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 32 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_218_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1145 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_218_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_218_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_259 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_266 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 32 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 32 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 32 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_266_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1193 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_266_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_266_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_348 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_355 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_355_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1282 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_355_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_355_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_396 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_403 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_403_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1330 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_403_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_403_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_444 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_451 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 64 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_451_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1378 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_451_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_451_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_533 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_540 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 96 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 96 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 96 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_540_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1467 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_540_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_540_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_581 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_588 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 96 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 96 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 96 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_588_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1515 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_588_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_588_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_670 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_677 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 160 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 160 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 160 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_677_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1604 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_677_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_677_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv node name: Conv_718 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  sub node names: [] (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add node name: Add_725 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  relu node name: None (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  output_channels: 160 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_ch_num: 160 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_add: 160 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_copy: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_insert: 0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_conv_node_output: Add_725_preshift_quant_out:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  new_perm_node_output: 1652 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  conv_move_add: Add_725_conv_move_add (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  there is no relu node (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  add_output_name: Add_725_conv_move_add:0 (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.utils.quantization_utils\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start to quantize weight. (\u001b[1mquantization_stage.py\u001b[0m:113)\n",
      "100%|| 57/57 [02:09<00:00,  2.28s/it]\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function QuantizationStage.run took 175.04051327705383 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.quantization_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute QuantizationStage:300:done (\u001b[1mquantization_stage.py\u001b[0m:127)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute GraphPartitionStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/400_GraphPartitionStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function GraphPartitionStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.graph_partition_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute GraphPartitionStage:400:run (\u001b[1mgraph_partition_stage.py\u001b[0m:60)\n",
      "[\u001b[1mbstnnx.frontend.graph_partition_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Use partition_greedy method. (\u001b[1mgraph_partition_stage.py\u001b[0m:115)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Boundaries indxes = [51, 52, 53, 54, 55, 56] (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Total boundaries number = 6 (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_279 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_280 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_281 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_282 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_283 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_284 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_285 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_286 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_287 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.separate_fused_conv_add\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Fused_Conv_Add_288 is a Fused_Conv_Add node, separating it into a Conv and a Fused_Add or a Fused_Conv and an Add (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.partition_base\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Conv_348: force_non_boundary (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Boundaries indxes = [24, 54, 61, 62, 63, 64, 65, 66] (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Total boundaries number = 8 (\u001b[1mgraph_partition_stage.py\u001b[0m:119)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Boundaries indxes = [24, 54, 61, 62, 63, 64, 65, 66] (\u001b[1mgraph_partition_stage.py\u001b[0m:148)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Total boundaries number = 8 (\u001b[1mgraph_partition_stage.py\u001b[0m:148)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.remove_aligned_concat\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Reshape_799 is a aligned Concat/Skip_Node. Remove this section before codegen ... (\u001b[1mgraph_partition_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.remove_aligned_concat\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Reshape_801 is a aligned Concat/Skip_Node. Remove this section before codegen ... (\u001b[1mgraph_partition_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.partition.section_optimizers.remove_aligned_concat\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Node Reshape_816 is a aligned Concat/Skip_Node. Remove this section before codegen ... (\u001b[1mgraph_partition_stage.py\u001b[0m:242)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function GraphPartitionStage.run took 240.9465754032135 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.graph_partition_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute GraphPartitionStage:400:done (\u001b[1mgraph_partition_stage.py\u001b[0m:273)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute SectionBindingStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/500_SectionBindingStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function SectionBindingStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.section_binding_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute SectionBindingStage:500:run (\u001b[1msection_binding_stage.py\u001b[0m:64)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  FirstSecs are [0, 3, 4] (\u001b[1msection_binding_stage.py\u001b[0m:113)\n",
      "[\u001b[1mbstnnx.partition.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  LastSecs are [2, 3, 4] (\u001b[1msection_binding_stage.py\u001b[0m:113)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs={'0'}, outputs={'1761'}\n",
      "inputs={'1761'}, outputs={'1764'}\n",
      "inputs={'1764'}, outputs={'1783'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.frontend.section_binding_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  BTMD/BTMW debug information: 0_2: BTMD(802816), BTMW(1701248) (\u001b[1msection_binding_stage.py\u001b[0m:126)\n",
      "[\u001b[1mbstnnx.frontend.section_binding_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  BTMD/BTMW debug information: 3_3: BTMD(65536), BTMW(0) (\u001b[1msection_binding_stage.py\u001b[0m:126)\n",
      "[\u001b[1mbstnnx.frontend.section_binding_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  BTMD/BTMW debug information: 4_4: BTMD(4096), BTMW(1294272) (\u001b[1msection_binding_stage.py\u001b[0m:126)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function SectionBindingStage.run took 71.67428779602051 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.section_binding_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute SectionBindingStage:500:done (\u001b[1msection_binding_stage.py\u001b[0m:154)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute CodeGenerationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/600_CodeGenerationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function CodeGenerationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.code_generation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute CodeGenerationStage:600:run (\u001b[1mcode_generation_stage.py\u001b[0m:96)\n",
      "[\u001b[1mbstnnx.codegen.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  dags_backend_type=['MAC_ARRAY', 'EDP', 'MAC_ARRAY'] (\u001b[1mcode_generation_stage.py\u001b[0m:138)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mcode_generation_stage.py\u001b[0m:138)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00244400 (\u001b[1mcode_generation_stage.py\u001b[0m:138)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mcode_generation_stage.py\u001b[0m:138)\n",
      "[\u001b[1mbstnnx.frontend.code_generation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Add ref input tensors: ['1764'] (\u001b[1mcode_generation_stage.py\u001b[0m:353)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mcode_generation_stage.py\u001b[0m:177)\n",
      "[\u001b[1mbstnnx.codegen.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  net meta data is saved as /bsnn/users/zihaozhao/QAT/alan/results/job0/600_CodeGenerationStage/qat_frozen_model.20220505013418.hw_test_config.json. (\u001b[1mcode_generation_stage.py\u001b[0m:177)\n",
      "[\u001b[1mbstnnx.codegen.graph\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  net meta data for linux is saved as /bsnn/users/zihaozhao/QAT/alan/results/job0/600_CodeGenerationStage/qat_frozen_model.meta. (\u001b[1mcode_generation_stage.py\u001b[0m:206)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function CodeGenerationStage.run took 89.68550634384155 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.code_generation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute CodeGenerationStage:600:done (\u001b[1mcode_generation_stage.py\u001b[0m:562)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute CodeCompilationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function CodeCompilationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.code_compilation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute CodeCompilationStage:700:run (\u001b[1mcode_compilation_stage.py\u001b[0m:155)\n",
      "[\u001b[1mbstnnx.frontend.code_compilation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  net_fw_dir = /bsnn/work/bstnnx_release/third_party/Net-FW-xos-v0-edp-gemm (\u001b[1mcode_compilation_stage.py\u001b[0m:178)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp -f /bsnn/users/zihaozhao/QAT/alan/results/job0/600_CodeGenerationStage/qat_frozen_model.c /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/c_src_gcc_debug/qat_frozen_model.c (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cmake  /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/c_src_gcc_debug (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = make -j48 (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.frontend.code_compilation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  build_log_file=/bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/build.log (\u001b[1mcode_compilation_stage.py\u001b[0m:134)\n",
      "[\u001b[1mbstnnx.frontend.code_compilation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Successfully compiled gcc_debug with the generated c file !! (\u001b[1mcode_compilation_stage.py\u001b[0m:142)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function CodeCompilationStage.run took 14.613078117370605 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.code_compilation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute CodeCompilationStage:700:done (\u001b[1mcode_compilation_stage.py\u001b[0m:199)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute RunEmulationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function RunEmulationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute RunEmulationStage:800:run (\u001b[1mrun_emulation_stage.py\u001b[0m:552)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model.t.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model_0_2.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model_3_3.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model_4_4.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  net_am_dir = /workspace/tools/net_am (\u001b[1mrun_emulation_stage.py\u001b[0m:601)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Generating batch mode references for 1 batches.  (\u001b[1mrun_emulation_stage.py\u001b[0m:322)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mrun_emulation_stage.py\u001b[0m:371)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Generating batch mode references for 1 batches.  (\u001b[1mrun_emulation_stage.py\u001b[0m:322)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mrun_emulation_stage.py\u001b[0m:371)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Generating batch mode references for 1 batches.  (\u001b[1mrun_emulation_stage.py\u001b[0m:322)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mrun_emulation_stage.py\u001b[0m:371)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_process=24 (\u001b[1mrun_emulation_stage.py\u001b[0m:640)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = bash ./run_simulation.sh (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = bash ./run_simulation.sh (\u001b[1m__init__.py\u001b[0m:72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully run through emulator and get consistent with result with CPUBackend !!  (/bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage/sec_3_3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = bash ./run_simulation.sh (\u001b[1m__init__.py\u001b[0m:72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully run through emulator and get consistent with result with CPUBackend !!  (/bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage/sec_0_2)\n",
      "successfully run through emulator and get consistent with result with CPUBackend !!  (/bsnn/users/zihaozhao/QAT/alan/results/job0/800_RunEmulationStage/sec_4_4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function RunEmulationStage.run took 19.38118886947632 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute RunEmulationStage:800:done (\u001b[1mrun_emulation_stage.py\u001b[0m:735)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute PartitionEvaluationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/900_PartitionEvaluationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function PartitionEvaluationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.partition_evaluation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute PartitionEvaluationStage:900:run (\u001b[1mpartition_evaluation_stage.py\u001b[0m:150)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function PartitionEvaluationStage.run took 6.126962900161743 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.partition_evaluation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute PartitionEvaluationStage:900:done (\u001b[1mpartition_evaluation_stage.py\u001b[0m:224)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute RunEmulationStage:bstnnx_run (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.base   \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  successfully create stage_result_dir /bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage (\u001b[1mstage_flow_control_main.py\u001b[0m:305)\n",
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Start profiling function RunEmulationStage.run. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute RunEmulationStage:1000:run (\u001b[1mrun_emulation_stage.py\u001b[0m:552)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model.t.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model_0_2.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model_3_3.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = cp /bsnn/users/zihaozhao/QAT/alan/results/job0/700_CodeCompilationStage/build_gcc_debug/qat_frozen_model_4_4.dag /bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  net_am_dir = /workspace/tools/net_am (\u001b[1mrun_emulation_stage.py\u001b[0m:601)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Generating batch mode references for 1 batches.  (\u001b[1mrun_emulation_stage.py\u001b[0m:322)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mrun_emulation_stage.py\u001b[0m:371)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Generating batch mode references for 1 batches.  (\u001b[1mrun_emulation_stage.py\u001b[0m:322)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mrun_emulation_stage.py\u001b[0m:371)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Generating batch mode references for 1 batches.  (\u001b[1mrun_emulation_stage.py\u001b[0m:322)\n",
      "[\u001b[1mbstnnx.engine.A1000B0\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Set SharedMem/BTMD/BTMW to 0.0MB/2.0MB/2.0MB: 00144440 (\u001b[1mrun_emulation_stage.py\u001b[0m:371)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  num_process=24 (\u001b[1mrun_emulation_stage.py\u001b[0m:640)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = bash ./run_simulation.sh (\u001b[1m__init__.py\u001b[0m:72)\n",
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = bash ./run_simulation.sh (\u001b[1m__init__.py\u001b[0m:72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully run through emulator and get consistent with result with CPUBackend !!  (/bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage/sec_3_3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.tools        \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  cmd = bash ./run_simulation.sh (\u001b[1m__init__.py\u001b[0m:72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully run through emulator and get consistent with result with CPUBackend !!  (/bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage/sec_4_4)\n",
      "successfully run through emulator and get consistent with result with CPUBackend !!  (/bsnn/users/zihaozhao/QAT/alan/results/job0/1000_RunEmulationStage/sec_0_2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[1mbstnnx.utils.timer  \u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Function RunEmulationStage.run took 39.828617334365845 seconds to complete. (\u001b[1mstage_flow_control_main.py\u001b[0m:306)\n",
      "[\u001b[1mbstnnx.frontend.run_emulation_stage\u001b[0m][\u001b[1;32mINFO\u001b[0m   ]  Execute RunEmulationStage:1000:done (\u001b[1mrun_emulation_stage.py\u001b[0m:735)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFlCAYAAABMeCkPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAldElEQVR4nO3deVyVdd7/8TeHI6hgSoq7009wazQNRHFXkFFKcCMNTYvMO+2X41QWmmY55tJiUXZnNKmZLVre2phIeVduNZnjGjpJJrmQIriQLMp2uH5/+OtMjguYfDksr+fj0eMRZ7muzznXpS+u6xzPcbMsyxIAADDC5uoBAACoyggtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCi2otICBAqampkqRp06YpLi5OkrRz504NHDiwzNf3ySefaNy4cWW+XEn64IMP1KNHDwUEBCgzM9PIOm7Ua6+9pscff9zVY1zit/sAYAKhRbUxduxYrVq16pLL9uzZoxYtWlx226CgIG3YsOGG1vfzzz+rbdu2Kioqcl42ePBgLV269IaWeyWFhYV67rnntHTpUu3Zs0c+Pj5lvo6q4Hr2AaCsEFqgCjhz5ozy8/PVqlWr676vZVkqLi42MBUAidCiDKSlpWnSpEnq1q2bgoODNXv2bElScXGxFi1apJCQEHXv3l2xsbHKzs6W9O+jvY8//lj9+vVTcHCw3njjDUlSenq6OnbsqF9++cW5ju+//17BwcEqLCy86hz/eVryt0eUcXFx2rlzp2bPnq2AgADnjG3bttXRo0cvW9b27dvVp08fSVJiYqICAgKc/3Xo0EFjx46VJG3evFlDhw5VYGCg+vbtq9dee825jDFjxkiSunTpooCAAO3Zs0dr1qzRqFGjnLfZvXu3oqKi1LlzZ0VFRWn37t3O68aOHatXXnlF0dHRCggI0Lhx43T27NnLZj18+LDCw8Od67r33ntLtey4uDhFR0erU6dOVzx1mp6erj//+c/q1q2bQkNDtXz5cud1SUlJuvvuuxUUFKRevXpp9uzZKigocF7/448/6v7771fXrl3Vo0cPxcfHO68rLCxUbGysAgICNGjQIO3bt++ydUsXfwGYN2+eunfvrsDAQEVGRurgwYOSpIKCAj3//PPq16+fevTooaefflp5eXnO+37xxRcaMmSIAgMDFRYWpq1bt5ZqH8jOzlZsbKy6deumkJAQLVq0yPlLyK/b7vnnn1eXLl0UGhqqLVu2ONe5Zs0a9e/fXwEBAQoNDdUnn3xyxceFasgCbkBRUZEVGRlpzZ0718rNzbXy8vKsHTt2WJZlWatWrbLCwsKsY8eOWTk5OdbDDz9sPf7445ZlWVZqaqrVpk0ba8aMGdaFCxesAwcOWO3bt7cOHTpkWZZljR071vrwww+d63nuueesmTNnXnOWhQsXWlOmTHH+/Os6CgsLLcuyrDFjxlgfffTRJfdp06aNdeTIEcuyLGvq1KnWyy+/bFmWZX377bdW7969L1tHdna2FR4ebq1YscJ5u+TkZMvhcFgHDhywunfvbn3++edXXL9lWdbq1aut6Ohoy7IsKzMz0woKCrI+/vhjq7Cw0Fq3bp0VFBRknT171jlv//79rZ9++sm6cOGCNWbMGOvFF1+84mP/z3WVZtl9+/a1Dh48aBUWFloFBQWXLM/hcFjDhg2zXnvtNSs/P986duyYFRoaam3dutWyLMvat2+ftWfPHquwsNBKTU21wsPDrbffftv5HPXs2dNasmSJlZeXZ2VnZ1t79+51bqMOHTpYmzdvtoqKiqwFCxZYI0aMuOJj2rp1qzVs2DDr3LlzVnFxsXXo0CErPT3dsizLmjt3rjVhwgQrMzPTys7OtiZMmGAtWLDAsizL+u6776zAwEDr66+/thwOh3Xy5EnnflXSPvDEE09YEydOtLKzs63U1FRrwIABztuvXr3a+uMf/2h9+OGHVlFRkfX+++9bPXv2tIqLi63c3FwrICDASklJsSzLstLT062DBw9e8XGh+uGIFjckKSlJGRkZio2NVe3ateXp6amgoCBJ0rp16xQTE6MWLVrIy8tLjz32mBITEy95zXLSpEmqWbOm2rVrp3bt2ik5OVmSFBkZqYSEBEkXj2wSExMVGRlZ/g/wN4qLizVlyhR17dpV0dHRkqTg4GC1bdtWNptN7dq106BBg/TPf/6zVMvbvHmzbrnlFg0dOlR2u10RERHy8/PTpk2bnLcZPny4WrZsqZo1ayo8PFwHDhwos2UPGzZMrVu3lt1uV40aNS65/759+3T27FlNmjRJHh4eatGihUaOHKnExERJUocOHXT77bfLbrerefPmuvvuu7Vjxw7nuhs0aKBx48bJ09NT3t7e6tSpk3PZnTt3Vt++feXu7q4hQ4Y4t/l/stvtys3N1U8//STLsuTv76+GDRvKsix99NFHmj59uurVqydvb29NmDBB69evlyT9z//8j6KiotSzZ0/ZbDY1atRI/v7+JT5nDodDiYmJmjJliry9vdW8eXPdf//9lxyZNm3aVCNHjpS7u7uGDRumU6dO6fTp05Ikm82mH3/8UXl5eWrYsKFat25dmk2FasDu6gFQuaWlpalp06ay2y/flTIyMtSsWTPnz82aNVNRUZHOnDnjvKxBgwbO/69Vq5bOnz8vSRowYICeffZZZWRk6MiRI7LZbM6Au0pcXJxyc3P11FNPOS/77rvvtGDBAv34448qLCxUQUGB8zRuSTIyMtS0adNLLmvatKnS09OdP/v6+jr//7fPT1ksu0mTJle9//Hjx5WRkXHJc+5wOJw/Hz58WM8995z279+vCxcuyOFwqH379pIu7hN/+MMfrrrs327zmjVrKj8/X0VFRZftQ927d9c999yj2bNn6/jx4xowYICmTp2q/Px8XbhwQcOHD3fe1vrN68xpaWnq27fvVdd/NZmZmSosLLzkefvP5+w/91dJOn/+vHx9fRUXF6elS5dqxowZCgwM1NSpU0sVeFR9HNHihjRp0kRpaWmXHKX+qmHDhjp+/Ljz5xMnTshut6t+/folLrdu3brq2bOnEhMTlZCQoDvvvFNubm7XvE+tWrUueZ3u1yONsrB+/XqtX79eCxcuvOTob8qUKerfv7+2bNmiXbt2KTo6Wtb//+bJkuZt2LChTpw4ccllaWlpatSo0Q3PW5plX2u+Jk2aqHnz5tq5c6fzvz179uitt96SJM2aNUt+fn7asGGDdu/erUcffdT5uJs0aVJm/1zm3nvv1Zo1a5SYmKgjR45o8eLF8vHxUc2aNbV+/XrnbLt27dKePXuc6z927Nh1r8vHx0c1atS45Hm7nu3Ru3dvvf322/r666/l5+enmTNnXvcMqJoILW5Ix44d5evrq5deeknnz59Xfn6+du3aJUmKiIjQO++8o9TUVOXm5iouLk533HHHFY9+ryQyMlJr167Vhg0bSnXa+NZbb9WOHTt04sQJZWdn680337zk+gYNGvyuAHz//fd69tln9frrr+vmm2++5Lrc3FzVrVtXnp6eSkpKcp7ulqSbb75ZNpvtquvs27evjhw5onXr1qmoqEiJiYk6dOiQ+vXrd90zlvWyO3bsKC8vL/3tb39TXl6eHA6HDh48qKSkJEkXH7eXl5e8vLyUkpKiFStWOO/br18/nTp1SsuWLVNBQYFycnL03XffXfdjSEpK0nfffafCwkLVqlVLHh4estlsstlsGjFihObNm+c8O5Kenq6vvvpKknTXXXdpzZo12rZtm4qLi5Wenq6UlBRJ194H3N3dFR4erri4OOXk5Oj48eN6++23NXjw4BJnPX36tL744gudP39eHh4eql27tmw2/nrFRewJuCHu7u6Kj4/X0aNHFRISoj59+ujTTz+VJEVFRWnw4MEaM2aM+vfvLw8Pj+v6LT80NFRHjhxRgwYN1K5duxJv37NnT915550aPHiwhg8frpCQkEuuv/fee7VhwwZ16dJFc+bMKfUcX375pbKysjR69GjnO4/Hjx8vSXrmmWe0cOFCBQQE6PXXX9cdd9zhvF+tWrU0ceJEjRo1SkFBQdq7d+8ly/Xx8VF8fLzefvttBQcHa/HixYqPj78s5r/HjS771+2anJys/v37q1u3bnrqqaeUk5MjSZo6daoSEhIUGBiomTNn6s4773Te19vbW0uXLtWmTZvUs2dPDRw4UNu3b7/ux/DrafquXbsqJCRE9erV0wMPPCBJeuKJJ3TLLbdo5MiRCgwMVExMjA4fPizp4i8J8+fP17x589S5c2eNGTPGeZRa0j4wc+ZM1apVS2FhYRo9erQiIiIUFRVV4qzFxcVatmyZevfura5du2rHjh2aNWvWdT9mVE1u1q/newAAQJnjiBYAAIN41zEqlaefflrr1q277PLIyEjnBxAAQEXCqWMAAAzi1DEAAAYZOXV86lS2icVWCD4+tZWZWboPDUDFw/ar3Nh+lVdV33a+vnWueh1HtNfJbnd39Qi4AWy/yo3tV3lV521HaFEhFRQU6PTp08rPz3f1KABwQ3jXMSqcU6cy9MorL6plS3+1b99B3bv3knTx82xL+lhDAKhoCC0qlPz8PH311Rbde+8DOnPmtJKTD8jTs6Y6dry91B/dCAAVCaeOUaF4etbUn/4UrrZt22nfvu/k799aBw8mOz/6DwAqG0KLCiExcZ1SUg5JkurUufjuvQkTHlb9+g2UnHxAnDEGUFlxLg4ul5eXp4SEtXJzc1PNmjXVrFlzSdIPPyTr66+36OGH/6K6deu5dkgA+J0ILVyquLhYRUVF6tDhNtWuXVsJCWt1663tVVCQr7CwgWrVqrXc3avvPwsAUPlx6hguZbPZ5O3trQ4dOikl5ZD+9a99Onv2jPOf9RBZAJUdR7RwOcuylJp6VF5eXnrggYlq3LixGjVq7OqxAOPGPbexTJe3dFpomS7PpOXLl+ree8c5f544cZzi45cqLe2EYmMf0bvvfqTk5O/12Wfr9cgjT1zXstPSTmjfviQNGBAuSb97OWWF0MLl3NzcNGLEKHl4eLh6FADl5N13374ktPHxSy+7Tbt2f1S7dn+87mWnpZ3QF1985gzt711OWSG0qBB+jSwfSgGY9emnCVq58j1JbmrVqpXGj39I8+fP1rlzv6hePR89+eQzaty4sebOnSUvLy8lJx/QmTNn9H//758VEhKmZ555UgMHDlKPHhc/SGbu3Fnq0aOXQkLCLltXYuI6JSd/r8cemypJio19RNHRY7R9+zbl5+crJma0Wrb00zPPzNGf/tRbn3/+1SX33717p1aufE8vvPCKHn98sk6fPi1JSks7rkceeUK33x6oZ599Wnl5FyRJjz4aq9tu66T4+P/W0aOHFRMzWnfcMUitW7d1Licr65zmz5+tEyeOy9OzpmJjZ6hVq9ZasuRNpaef1IkTx5Wenq6RI0dpxIjoMnnOCS0qFCILmPPTTyl6552lio9fqnr16ikr65zmzJmlO+6I0B13RCghYa1effVFzZ//kiTp9OnTWrRosY4ePaJp0x5TSEiYQkMHaOPGz9WjRy8VFhZq164devzxadc1x0MP/Vlr1nykZcs+KPV9FixYKElKTj6g+fP/qt69+8lutysu7nV5enoqNfWYZs2aoSVL3tXEiZOcYZUuBvtXS5a8qdat22r+/Je0a9cOzZnzjHOOY8eOauHCeJ0/f16jR0dp2LC7yuSDcggtXKqsX6OqaCrTa2ao+nbv3qGQkP6qV6+eJOmmm+rqX/9K0rx5L0qSwsMH6Y03Fjpv36dPP9lsNrVs6aezZ89Kkrp166FXX12ggoICbd/+jTp1CpCnZ81ymf+XX37RnDlPa/bs+fL29lZOTo7i4p7Xjz8elM3mrtTUoyUuIylpr+bMeUGS1LlzF2VlnVNu7sUPxOnevac8PDzk4eEhHx8fnT17Rg0bNrrhuQktAOCKatSo8ZufLEmSp6enAgI665//3KYvv/xcYWEDrnp/d3d3FRdbzp/z8wt+9ywOh0PPPDNdMTHj5efXSpL04Yfvy8envpYtW6Hi4mL179/zdy9fkmrU+Pf7RGw2mxwOxw0tz7msMlkKAKDCCwzsok2bvtS5c79IkrKyzqlDh4764osNkqT//d9P1bFjQInL6d9/gNavX6ekpL0KDu5x1ds1btxUhw4dVHFxsdLS0nTgwL+c17m721VUVFTq2ePj/1utWrVSWNhA52W5uTmqX7+BbDabNmxIdIaxdm0vnT9/5e++7dQpQJ9//pmki6eU69atKy8v71LP8XtwRAsALlLeLy34+fnrvvvGadKkB2WzuatNm7Z69NFYzZv3V61Y8a7zzVAl6dq1m5599mn17t33P456L9WxYyc1adJUY8aMUJs2rdWmTVvndYMHD9N990WrTZt2euaZOSWuc8WKd9WypZ9iYkZLksaPn6Bhw0boqadi9dln6xUc3F21atWSJLVq1Vo2m0333TdKd94Zodat/73eceMe1Pz5s3XffdHy9KypGTP+WuK6b5SbZVlWyTe7PqdOZZf1IisMX986VfrxlTdeo8X14M9f5VXVt52vb52rXsepYwAADOLUMQDghmzfvk1vvPHaJZc1adJU8+cvcNFEFQuhBQDckODg7goO7u7qMSosTh0DAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCo1KF1OBwaOnSoJkyYYHIeAACqlFKHdvny5fL39zc5CwAAVU6pQnvy5Elt3rxZd911l+l5AACoUkoV2nnz5umJJ56QzcZLugAAXI8Sv1Rg06ZNuvnmm9WhQwdt3769VAv18aktu939hoerqK71vYPAb7GvlD2e08qrum67EkO7e/dubdy4UVu3blV+fr5ycnL0+OOPa8GCq3/9UWbm+TIdsiKp6l9ejLLFvlK2+PNXeVX1bXetXyJKDO2UKVM0ZcoUSdL27du1dOnSa0YWAAD8Gy+6AgBg0HV98XtwcLCCg4NNzQIAQJXDES0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMspd0g/z8fN1zzz0qKCiQw+HQwIEDNXny5PKYDQCASq/E0Hp4eOidd96Rl5eXCgsLNXr0aPXp00e33357OYwHAEDlVuKpYzc3N3l5eUmSioqKVFRUJDc3N+ODAQBQFZR4RCtJDodDw4cP17FjxzR69Gh16tTpmrf38aktu929TAasiHx967h6BFQS7Ctlj+e08qqu265UoXV3d9fatWuVlZWlhx9+WAcPHlSbNm2uevvMzPNlNmBF4+tbR6dOZbt6DFQS7Ctliz9/lVdV33bX+iXiut51fNNNNyk4OFhfffXVDQ8FAEB1UGJoz549q6ysLElSXl6evvnmG/n5+RkfDACAqqDEU8cZGRmaNm2aHA6HLMtSeHi4QkJCymM2AAAqvRJD265dO/39738vh1EAAKh6+GQoAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAyyl3SDtLQ0xcbG6syZM3Jzc9PIkSN13333lcdsAABUeiWG1t3dXdOmTVP79u2Vk5OjqKgo9ezZU61atSqP+QAAqNRKPHXcsGFDtW/fXpLk7e0tPz8/paenGx8MAICqoMQj2t/6+eefdeDAAXXq1Omat/PxqS273f2GBqvIfH3ruHoEVBLsK2WP57Tyqq7brtShzc3N1eTJkzV9+nR5e3tf87aZmedveLCKyte3jk6dynb1GKgk2FfKFn/+Kq+qvu2u9UtEqd51XFhYqMmTJysyMlIDBgwos8EAAKjqSgytZVmaMWOG/Pz8dP/995fHTAAAVBklhnbXrl1au3atvv32Ww0ZMkRDhgzRli1bymM2AAAqvRJfow0KCtIPP/xQHrMAAFDl8MlQAAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC2AcmdZlqtHAMoNoQVQbn7+OVVFRUVyc3Nz9ShAuSG0AMpFUtJeLVu2WN9887UKCwtdPQ5QbggtAOPy8vK0bds/ZLfblZX1i1aufE/Hj//s6rGAckFoARiTn5+vo0ePyM1N8vdvJYfDoYiIoWrUqImSk7939XhAubC7egAAVdPp06cVF/e8Wrduq7p16yk4uLtSUg5pzZpVys3Nlaenp6tHBMoFR7QAylxhYaH27t2lsWPvV/fuvXTyZJr270/SiBHRstlsatq0qUaOHOXqMYFywREtgDJXo0YN9erVVzVr1tTKle+pd+++2rNntwoKCjR0aJSrxwPKFUe0AMrM4cM/Of/fw8NDkhQdPUY2m02HD6eoRo0arhoNcBlCC+CGWZal+Pj/1gsvzNWuXTskSTbbxb9ejh07oh07tmvixEmqX7+BK8cEXILQArhhBQUFCgkJ04wZs/T3v69WevpJSdJnn63XH/7wfzR27P1q2LCRi6cEXIPXaAHcME9PT7Vt206SNHLkKL3yygJ17Hi73N3dJf376Baojtj7AZSpli39dfx4qurXb8A7iwFxRAugjOXl5WnmzGfVunUbV48CVAiEFsDvNu65jde4tnJ/xOLSaaGuHgFVBKeOAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEElhvbJJ59U9+7dFRERUR7zAABQpZQY2uHDh2vx4sXlMQsAAFVOiaHt0qWL6tatWx6zAABQ5dhNLNTHp7bsdncTi64QfH3ruHoEVBLsK5UX267sVdfn1EhoMzPPm1hsheDrW0enTmW7egxUEuwrlRfbrmxV9b87r/VLBO86BgDAIEILAIBBJYb2scceU3R0tA4fPqw+ffpo1apV5TEXAABVQomv0b788svlMQcAAFUSp44BADCI0AIAYBChBQDAoGoXWsuyXD0CAKAaqTah/fnnVBUVFcnNzc3VowAAqpFqEdqkpL1atmyxvvnmaxUWFrp6HABANVLlQ1tQUKBt2/4hu92urKxftHLlezp+/GdXjwUAqCaqbGiLioqUnPy9PDw81KRJUzkcDkVEDFWjRk2UnPy9q8cDAFQTRr5UwNWys7O1bNliubm5qW7demrTpq3OnftFa9asUm5urjw9PV09IgCgmqiSR7Q5OdnKy7ug+vUbKDn5e7333jJFRAyRzeampk2bauTIUa4eEQBQTVTRI9os5eRkq3fvvmrevIXS00+qsLBQQ4fe5erRAADVTJU5ot227R/66qvNkqQ2bdqpefM/6JNPPtamTV/o4MEfZLdXyd8pAAAVXJWoz1tvvSF3d3dJUkrKIcXEjNd//ddD2rt3t06fPqUHH3xYN99c38VTAgCqoyoR2saNmygwMEjNmjXXSy89r08/TVCNGjUUFjbQ1aMBAKq5KnHq+Kab6mr//iRJUnT0PcrNzVFeXp6LpwIAoIqEtlevPrpw4YJWrVqp119/Vf7+rRURMcTVYwEAUDVOHbu7uysycqhyc3PVuXOQ/PxauXokAAAkVYHQjntu4xUuPVbuc5iydFqoq0cAANyAKnHqGACAiorQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAGAQoQUAwCBCCwCAQYQWAACDCC0AAAYRWgAADCK0AAAYRGgBADCI0AIAYBChBQDAIEILAIBBhBYAAIMILQAABhFaAAAMIrQAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEGEFgAAgwgtAAAGEVoAAAwitAAAGERoAQAwiNACAK6ooKBAp0+fUlbWOUlScXGxiyeqnOyuHgAAUPGcPXtGL744T7ffHqiffkrRAw9MUMOGjWRZltzc3Fw9XqXCES0A4DIZGRnq2zdUd999j3r06KUlS95UUVERkf0dCC0A4DLZ2Vnav3+fJKlv31C1atVG58794tqhKilCCwCQJG3b9g999dVmSVKXLsFq0KCBlix5U5s2faH9+5Nkt/Nq4+/BswYA0FtvvSF3d3dJUkrKIcXEjFdMzHjt3r1TZ86c1kMPTVbduvVcO2QlVarQbt26VXPnzlVxcbFGjBihBx980PRcAIBy1LhxEwUGBqlZs+Z66aXn9emnCapRo4bCwga6erRKr8RTxw6HQ7Nnz9bixYu1fv16JSQk6NChQ+UxGwCgnNx0U13t358kSYqOvke5uTnKy8tz8VRVQ4mhTUpK0i233KIWLVrIw8NDgwYN0pdfflkeswEAykmvXn104cIFrVq1Uq+//qr8/VsrImKIq8eqEko8dZyenq7GjRs7f27UqJGSkpKMDgUAKF/u7u6KjByq3Nxcde4cJD+/Vq4eqcow8mYoX986JhZ7Rete4jeuyoztV7mx/aqievL3b2ZkyeXZhoqkxFPHjRo10smTJ50/p6enq1GjRkaHAgCgqigxtLfddpuOHDmi1NRUFRQUaP369QoNDS2P2QAAqPRKPHVst9v19NNPa/z48XI4HIqKilLr1q3LYzYAACo9N8uyLFcPAQBAVcVHMAIAYBChBQDAIEILAIBBfKlACVJSUvTll18qIyNDktSwYUP1799f/v7+Lp4MACqP2NhYvfDCC64ewyV4M9Q1/O1vf9P69es1aNAg578dTk9Pd17GlysA5qSkpCgjI0MdO3aUl5eX8/KtW7eqT58+LpwMJZk4ceJll23fvl3BwcGSpPj4+PIeyaU4or2G1atXKyHh4jdY/FZMTIwiIiIIbSW2evVqRUVFuXoMXMXy5cv1/vvvy9/fX8nJyZo+fbrCwsIkSXFxcYS2gktPT5e/v79GjBghNzc3WZal/fv3a9y4ca4ezSV4jfYa3NzcnKeMf+vUqVNyc3NzwUQoK6+99pqrR8A1rFq1SmvWrNGiRYu0fPlyLVq0SO+8844kiZNwFd/q1avVoUMHxcfHq06dOgoODpanp6e6du2qrl27unq8cscR7TVMnz5dMTExuuWWW9SkSRNJ0okTJ3Ts2DHNnDnTxdOhJJGRkVe97vTp0+U4Ca5XcXGx83Rx8+bN9e6772ry5Mk6ceIEoa0EbDabYmJiFB4ernnz5qlBgwZyOByuHstlCO019OnTRxs2bFBSUpLS09MlXfzs59tuu03u7u4ung4lOXPmjJYsWaKbbrrpkssty1J0dLSLpkJp1K9fXwcOHNCtt94qSfLy8tKbb76p6dOn6+DBgy6eDqXVuHFjLVy4UJs3b5a3t7erx3EZ3gyFKmv69OkaPny4goKCLrtuypQpeumll1wwFUrj5MmTcnd3l6+v72XX7dq1S507d3bBVMDvQ2gBADCIN0MBAGAQoQUAwCBCC1QTM2bM0M6dO6943bRp0/Tee++V80RA9cC7joFqYu7cua4eAaiWCC1QQVy4cEFTp07VoUOHZLfb1bJlS7366qv6+OOP9cEHH8jhcMjb21uzZs2Sn5+fJOnNN99UQkKC3NzcVLt2bX3wwQey2a58omrs2LEaN26cQkJClJ6ertjYWJ06dUrNmjW76n0A3DhCC1QQX3/9tXJzc5WYmChJOnfunHbu3KlPP/1U77//vjw8PLRlyxZNnz5dK1eu1Mcff6yNGzdqxYoV8vb2VmZmZqmDOWfOHHXp0kWTJk1SamqqBg8erN69e5t8eEC1RWiBCqJdu3ZKSUnRX//6V3Xt2lX9+vXTxo0blZycrBEjRki6+GEbWVlZkqRNmzZp1KhRzg8C8PHxKfW6tm/frqeeekqS1KJFC3Xv3r2MHw2AXxFaoIJo0aKFEhIS9O2332rr1q2Ki4tT//79FRUVpb/85S+uHg/A78QLM0AF8eunIYWFhenJJ5/U2bNnFRoaqrVr1+rkyZOSJIfDof3790uSQkJCtGLFCuXk5EiSMjMzS72ubt26afXq1ZKk1NRUbdu2rYwfDYBfcUQLVBA//PCD82Mhi4uL9eCDD6pLly565JFH9NBDD8nhcKiwsFDh4eHq0KGDhg4dqvT0dN19992y2+2qXbu23n///VK9TjtjxgzFxsYqISFBzZs3d35PKICyx0cwAgBgEKeOAQAwiFPHQBWyZcsWvfzyy5dd/thjj6lv374umAgAp44BADCIU8cAABhEaAEAMIjQAgBgEKEFAMAgQgsAgEH/D6xHBpBEIAWrAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfkAAAFlCAYAAADoEpHcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1p0lEQVR4nO3deVhUdf//8efACC6AoMKgiZbmlisuCKGoIKC55Nre7VJf29XbstyzNOvutjS1biW1srvucsNKLElckDJb1TDN1EhcGEwRV7bh/P7wbn55uxGgA4fX47q6ruZzzvmc95kPXq85n3PmjMUwDAMRERExHTdXFyAiIiLXhkJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS+mMm7cOGbNmuXqMoqkSZMm/Pbbb9d1n3PnzuWpp54q1T4Nw2D8+PF06NCBQYMGlWrfpen+++9n2bJlri7D6dtvvyU2NtbVZYjJWV1dgIiUb9999x1ffPEFmzZtomrVqq4up8xq0qQJiYmJ1K9fH4D27duzdu1aF1clZqczeakwCgoKKtR+r5dDhw5xww03FCvgzf7eiLiaQl7KtZ9++on+/fsTHBzM6NGjyc3NdS7bunUrERERxMXFER4ezvjx48nOzuahhx4iNDSUDh068NBDD5GRkeHc5v7772fWrFncddddBAcH8/DDD5OVlcWTTz5J27ZtGThwIAcPHrxqXU2aNOG9994jJiaGmJgYABYuXEinTp3o1KkTy5cvL9Lx5eTk8NJLL9GtWzfatWvH3XffTU5ODiNGjODdd9+9YN0+ffrw+eefA/DLL78wbNgwQkJCuPXWW5k/f/4l+9+2bRt33XUX7du3p2/fvmzdutW5bOXKlURFRREcHExkZCQff/zxRdsvW7aMSZMmsW3bNoKDg5kzZw4AS5cuJTo6mpCQEB5++GHsdvsV35u/UteKFSvo2bMnwcHBREVF8cEHH1yw7bp167j99ttp27Yt3bt3Jzk52bns0KFDzrEdPnw4x48fv+T+jx8/zkMPPUT79u0JCQnhnnvuobCwEAC73c4TTzxBaGgokZGRLFmyxLmdw+Fg/vz5dO/eneDgYAYMGMCRI0e49957Abj99tsJDg5mzZo1zr/PP+zbt4/777+f9u3b06tXL5KSkpzLxo0bx3PPPceIESMIDg5m8ODBHDhwADh/uWTGjBmEhYXRtm1b+vTpw549ey55XFIBGSLlVG5urtG1a1fjrbfeMvLy8oxPP/3UuOWWW4xXX33VMAzD+Oqrr4xmzZoZL7/8spGbm2ucO3fOOH78uPHZZ58ZZ8+eNU6dOmU88cQTxiOPPOLs87777jO6d+9u/Pbbb8bJkyeNnj17GjExMcYXX3xh5OfnG2PHjjXGjRt31doaN25sDB061MjKyjLOnTtnbNq0yQgLCzN+/vln48yZM8aYMWOMxo0bG2lpaVfsZ+rUqcZ9991nZGRkGAUFBcZ3331n5ObmGgkJCcagQYOc6+3atcsICQkxcnNzjVOnThnh4eHGokWLjJycHOPUqVPGtm3bDMMwjDlz5hhPPvmkYRiGkZGRYYSEhBgbN240HA6HkZKSYoSEhBjHjh0zzpw5YwQHBxv79u0zDMMw7Ha7sWfPnkvWuGLFCuOuu+5yvv7yyy+NkJAQIzU11cjNzTWef/5545577rnse/O/rlSXYRjGhg0bjN9++80oLCw0tm7darRq1cpITU01DMMwtm/fbrRt29ZISUkxHA6HkZGRYezdu9cwjPNjGxUVZezfv984d+6ccd999xn//Oc/L3lMM2fONCZPnmzk5eUZeXl5xjfffGMUFhYaDofD6N+/vzF37lwjNzfXOHDggBEZGWkkJycbhmEYb775ptG7d29j3759RmFhobFr1y7j+PHjzuP+83h/9dVXRufOnQ3DMIy8vDyje/fuxr/+9S8jNzfX+PLLL402bdo43/9nnnnGCAkJMbZv327k5+cbY8aMMUaPHm0YhmEkJycb/fv3N7Kzs43CwkJj7969ht1uv+RxScWjM3kpt7Zv305+fj5DhgyhUqVK9OjRg5YtW16wjpubGyNHjsTDw4PKlSvj5+dHbGwsVapUwcvLi0ceeYRvvvnmgm0GDBhAvXr18Pb2JiIigqCgIG699VasVis9evTgp59+KlJ9I0aMwNfXl8qVK/Ppp58yYMAAGjduTNWqVXn88cevun1hYSErVqxg4sSJ2Gw23N3dadu2LR4eHkRFRZGWlkZaWhoAH330ET179sTDw4ONGzdSq1Ythg8fjqenJ15eXrRu3fqi/j/66CMiIiLo0qULbm5uhIeH06JFCzZt2uR873755RdycnIICAigUaNGRTruTz75hIEDB9K8eXM8PDwYM2YM27Ztu2AG5M/vzV+tq2vXrtSrVw+LxUJISAjh4eF8++23ACxfvpyBAwcSHh6Om5sbNpuNhg0bOvseMGAAN910E5UrV6ZHjx7s2rXrksdgtVo5evQohw8fplKlSrRv3x6LxcKPP/7I8ePHefzxx/Hw8CAoKIg77riDNWvWAOdnNkaNGkWDBg2wWCw0bdoUPz+/q75n27dv5+zZs4wYMQIPDw/CwsLo1q0bCQkJznW6d+9Oq1atsFqt9O3b11m71WrlzJkz7N+/H8MwaNiwIQEBAVfdp1QMuvFOyq3MzExsNhsWi8XZVqdOnQvW8fPzw9PT0/n63LlzvPjii2zevJns7GwAzpw5g8PhwN3dHYBatWo51/f09LzgdeXKlTl79myR6qtdu/YFtbZo0cL5+oYbbrjq9llZWeTm5hIUFHTRMk9PT3r27MnHH3/M448/zurVq51T5UeOHKFevXpX7f/w4cN89tlnbNiwwdlWUFBAx44dqVq1KrNmzWLx4sVMnDiRtm3b8swzz1wQmJeTmZlJ8+bNna+rVauGr68vdrudunXrAhe+N3+lLoBNmzbx+uuvk5aWRmFhITk5OTRu3Nh57F26dLls3/7+/s7/r1KlymXH8oEHHmDevHkMHz4cgDvvvJMRI0Zw6NAhMjMzad++vXNdh8PhfJ2RkVGk9/5/ZWZmEhgYiJvb/z/vqlOnzgWXOS73dxgWFsa9997L888/z6FDh4iJieGZZ57By8vrL9ch5qOQl3LL398fu92OYRjOoD98+PAFofjnDwAAixcv5tdff2Xp0qX4+/uza9cu+vXrh3ENfozxz/sOCAjgyJEjzteHDx++6vZ/fEBJT0+nadOmFy3v378/Tz/9NO3ataNKlSoEBwcD5wP0jzPLK6lduza3334706dPv+Tyzp0707lzZ3Jycpg9ezaTJ0/m/fffv2q/AQEBHDp0yPn67NmznDhxApvN5mz733Epal15eXmMHDmSf/zjH0RFRVGpUiUeffRR5/jVrl3bea26JLy8vBg3bhzjxo1jz549DBkyhJYtW1K7dm3q1q1LYmLiJbcLDAzkwIEDzg8dRRUQEEBGRgaFhYXOoD9y5Ag33nhjkbb/29/+xt/+9jeOHTvG6NGjWbhwIaNHj/5LNYg5abpeyq02bdpgtVpZsmQJ+fn5JCYm8uOPP15xmzNnzuDp6YmPjw8nTpxg3rx5pVLLmTNn+Prrry+7vEePHsTHx7N3717OnTtXpP26ubkxcOBAXnzxRex2Ow6Hgx9++IG8vDwAgoODcXNz46WXXqJv377O7bp27crRo0d5++23ycvL4/Tp02zfvv2i/vv27cuGDRvYvHkzDoeD3Nxctm7dSkZGBr///jvr1q3j7NmzeHh4ULVq1QvOMq+kd+/erFy5kl27dpGXl8err75Kq1atnGfxV3OluvLy8sjLy6NGjRpYrVY2bdrEF1984dx20KBBrFy5ki1btlBYWIjdbmffvn1F2u+fbdiwgd9++w3DMPD29sbd3R2LxUKrVq2oVq0acXFx5OTk4HA42LNnDzt27ABg8ODBvPbaa6SlpWEYBrt37yYrKws4fyaenp5+yf21atWKypUrs3DhQvLz89m6dSvr16/ntttuu2qtO3bscF66qlKlCh4eHkUeKzE//SVIueXh4cHcuXOJj48nJCSENWvWEB0dfcVthgwZQm5uLqGhodx555107ty5xHWcPn2auXPn8uGHH7Jz505++eWXi9bp0qULQ4YMYciQIURHRxMaGlqkvp955hkaN27MoEGDCAkJYebMmc67vOH83dp79uzh9ttvd7Z5eXmxePFiNmzYQHh4OLGxsRfcnf6H2rVr88Ybb7BgwQLCwsLo0qULixYtorCwkMLCQt5++206d+5MSEgI33zzDVOnTi1SzbfeeiujRo3iiSeeoFOnTqSnp/+lBxRdqS4vLy8mTZrE6NGj6dChA6tXryYyMtK5batWrXjxxReZMWMG7dq147777ivSrMn/+u233xg2bBjBwcHceeed3H333YSGhuLu7s78+fPZvXs3UVFRhIaGMmnSJE6fPg3AsGHD6NmzJ8OHD6dt27ZMnDjR+Y2Pxx9/nHHjxtG+ffuLZlo8PDyYP38+ycnJhIaG8txzz/Hyyy8X6fLImTNnmDRpEiEhIXTr1g1fX18eeOCBv3zMYk4W41rMU4pUIPv37ycuLo7Q0FC8vLzYs2cPPXv25Kabbrrm+161ahUffvgh//nPf675vkSk/NGZvEgx/TH1Wrt2bY4cOYKnpyfdu3e/7F3jpe3cuXO8//773Hnnndd8XyJSPulMXqQYJk2axIoVK6hUqRLu7u4UFhaSl5eH1WolKiqKcePGERgYWKS+evXqdckp5eeee+6Ca+1/tnnzZp544gnCwsKYO3cuVqvuoRWRiynkRf6i3NxcUlNTqVy5Mm+++Sbjx4/HZrORnp7OgQMHaNWqFd7e3q4uU0REIS9SVPv27SMpKYmOHTtyyy23UKlSJb788kuWLVtGq1atGDhwID4+Pq4uU0TESdfkRYpg//79vPXWW9SuXZtdu3ZRqVIl4Pyd5Hv37gVQwItImWO6C3lHj55ydQnXlJ9fVbKyivbENSk9O3f+Alhxd6/Ce+8t5Ny5fGrW9KdRo8aMHPkUbdq0verfnsaufNP4lV9mHzt//8tfHtSZfDljtbq7uoQK5dtvzz/gJjT0Vpo3b8FXX33JnDnzcTgKcTgKqFXLnzZt2hapL41d+abxK78q8tgp5EUuobCwkLfeepPFi+NYuvT8o1y7d4/lzJnTLFmyiJ9/3kXz5i2v0ouIiGsp5EUuwTAMYmJ68sYbC9m79xe++upLAEaNeorWrdsyduwE/PxquLhKEZErU8iL/Mkvv/zM0aOZuLu7c8MN55+1PmrUkyxd+h9eeGEqJ09mEx7e+Yo/sCIiUlYo5EX+a9eunbzwwnN8+ulqfv/9qLPd3d2Kw1FA+/Yh2GxFe8CNiEhZoJAXAQ4dOkizZs2ZOXMOPj4+bN/+AwUFBZw5c/6HR555ZhKxsVf/RTARkbJEIS8VmmEYzJ8/j+nTp/DNN1upVasWXbt258iRw7z00jQ2bFiHh4cHderc4OpSRUT+MtN9T17kr8jLy6Nbt+707n07Cxa8TlBQfQIDA52/yR0b20u/zS0i5ZZCXio0T09PmjRpCsAdd9zNa6/9k1tuaUHr1sHcccc9Lq5ORKRkFPIi/3XTTQ05fPgwERHdaNWqjavLEbmmhr+0vlT7Wzwu8i9vs2jRAqpUqco999xfqrX8Vd9//y0ffPBvXn559jXbh6uOVSEv8l85OTlMmvQcjRo1dnUpIhVWQUHBNf/pZIfDgbt7xXgKnkJe5L9q1apFrVq1XF2GiKm9884iPv00AT8/PwICbDRp0ozHHx9Bo0ZN2LFjG927xxIUVI933llEQUE+Pj6+PPvsNGrUqMmiRQs4cuQwhw8fwm7PYOTIMezc+SNfffUltWoF8PLLsy77AWHQoD5ERkbz7bdbueeev+Hl5c2cOa9QuXLlq87cnT17ltmz/8nu3T9hsVgYNuz/OH36NPv27WXUqCcB+PjjeNLS9jNy5JN8+ulqPvjg34CFm2++mcmTp13Q36FDB3nllX9w4kQWlStX5plnJlG//o2sX7+Ot96Kw83NHS8vL15//c0Sv98KeRERuS52795FUlIib7/9Pg5HAcOH30eTJs0AyM/PZ9GidwE4efIkcXFvY7FY+OSTVbz33hKeeOLvwPmAnDt3Ab/+up+HHx7G9Okv8+ijoxg//im+/DKFiIiul91/9erVWbz4PXJzc7n77gG89tq/qFs3iClTxl+x7rffXki1al4sWfKhsz6r1cqSJYt57LFRWK1W1qz5hLFjJ7B//z7eeWcx8+cvxtfXl5Mnsy/q7+WXX+Cpp8YTFFSPnTtTeeWVl5gzZz5vv/0mr746D3//AE6dKp0fW1PIS4VV2tcky5riXCMVuZZ27PiBiIhuVK5cGYBOnSKcy6Kiop3/f/RoJs8+O55jx34nPz+f2rX//1dYQ0NvxWq10rDhzRQWFhIaeisADRveTEbG4SvuPyoqBoADB9KoXbsOQUH1AIiN7cnHH8dfdrtvv/2a556b4Xz9x89Kt2vXgS++2MyNN95EQUEBDRvezPLlH9CtWxS+vr7/Xbf6BX2dPXuWH3/cweTJ45xt+fl5ALRs2ZoXXphKZGQ0Xbp0u+KxFJVCXkREXK5KlSrO/58162XuuuteOnXqwvfff8vixXHOZZUqeQDg5uaG1Wp1PmLaYrFQUOC44j4qV65yxeV/Ve/e/Xj33cXUq3cjt93Wp0jbGEYh3t5evP32+xctGzt2Ajt3prJlSwoPPHA/ixa9S/XqviWqUV8AFhGR66J167Zs3ryR3Nwczp49wxdfbL7kemfOnKZWrQAAPvssodTrqFfvRo4cOcyhQwcB+PzztVdcv0OHjqxcucz5+uTJkwA0b96CzEw769atpXv3WADatu3Ahg1JZGef+O+6F07XV6vmRe3aN7B+/Trg/AO5fvllD3D+UkTz5i148MGH8fX1IzPTXuJj1Zm8iEgF5IrLOU2aNCUyMpohQ+7Bz8+Ppk1vueR6w4ePYPLkcXh7e9OuXQcOHz5UqnV4enry9NMTGTt21H9vvAvm3Lmzl11/yJAHePXVf3D//Xfg5ubO8OH/R5cu59+/bt2i2bv3Z+cUfoMGDRkyZDiPPz4CNzd3GjduwsSJUy/ob8qUacyc+RLvvLMIh6OAqKgYGjVqzOuvv8bBgwcwDIN27UK4+eaSf9PHYhiGUeJeypCjR0vnZoWyyt/f2/THeL3omrz8Ffq3V35dy7F7+unR3HHHPbRvH3JN+i8Kf3/vyy7TdL2IiMhfdOrUKe66awAeHp4uDfir0XS9iIiYxvjxT3HkyIV32Y8b9zRNm7a56rYJCR+zbNkHF7S1bNmaJ5985qJ1vb29+eCDlSWq9XpQyIuIiGm8+OLMi9qKOl3fq1dfevXqey3KchlN14uIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImddWQHz9+PGFhYfTu3dvZduLECYYNG0ZMTAzDhg0jOzsbAMMwmD59OtHR0fTp04edO3c6t4mPjycmJoaYmBji4+Od7ampqfTp04fo6GimT5+OYRhX3IeIiIgUzVVDfsCAASxcuPCCtri4OMLCwkhMTCQsLIy4uDgAkpOTSUtLIzExkWnTpjF16lTgfGDPmzePpUuXsmzZMubNm+cM7alTpzJt2jQSExNJS0sjOTn5ivsQERGRorlqyHfo0IHq1atf0JaUlES/fv0A6NevH+vWrbug3WKx0KZNG06ePElmZiYpKSmEh4fj6+tL9erVCQ8PZ/PmzWRmZnL69GnatGmDxWKhX79+JCUlXXEfIiIiUjTW4mx07NgxAgICAPD39+fYsWMA2O12AgMDnesFBgZit9svarfZbJds/2P9K+3javz8qmK1uhfnsMoNf39vV5cg5YD+Tkqf3tPyq6KOXbFC/s8sFgsWi6U0aimVfWRlnb2mtbiav783R4+ecnUZUg7o76R06d9e+WX2sbvSB5hi3V1fs2ZNMjMzAcjMzKRGjRrA+TP0jIwM53oZGRnYbLaL2u12+yXb/1j/SvsQERGRoilWyEdGRrJq1SoAVq1aRVRU1AXthmGwbds2vL29CQgIoFOnTqSkpJCdnU12djYpKSl06tSJgIAAvLy82LZtG4ZhXLKv/92HiIiIFM1Vp+vHjBnD119/TVZWFhERETzxxBOMGDGC0aNHs3z5curUqcPs2bMB6NKlC5s2bSI6OpoqVaowY8YMAHx9fXn00UcZNGgQAI899hi+vr4APPvss4wfP56cnBwiIiKIiIgAuOw+REREpGgsxh9fTDcJM193AfNfW7qehr+03tUlXFOLx0W6ugRT0b+98svsY1fq1+RFRESk7FPIi4iImJRCXkRExKQU8iIiIialkBcRETEphbyIiIhJKeRFRERMSiEvIhXWtm3f8+OP211dhsg1o5AXkQopKSmRFSuWcvBgOlu2fOHqckSuCYW8iFRIDocDq9VK48ZNWbduLUlJn7u6JJFSp5AXkQojLe1X3n33LXbuTMXT0xN/f39uuKEuI0Y8yt69ezh79oyrSxQpVQp5EakQfvstjQ8/fA+bLZADB9Jo1y6EmjVr8dlnCbz55r9o2vQWqlat5uoyRUrVVX+FTkSkvCsoKODEiSw8PSvj51eDN954DQ8PD2rUqEn79iHcfHMjWrRo5eoyRUqdQl5ETG3TpvW4u1vp1CmC06dP88UXm5kzZwHr13+On18N538iZqTpehExraSkRD7+eBWHDx9kxYoPCQ/vjMViYcmSxfz88y5attTZu5ibzuRFxHROnsymatVq+PhUx9/fny5dItm4MYkPP3yPRx55gm++2cqtt3bCYrG4ulSRa0pn8iJiKqmpO5gwYSx79uwmIMCGl5c3p06dIjq6B7///ju5ubnOM3oRs1PIi4hpfPPNVlJSkmnevCUbN66nfv0badbsFnbs2Mbs2TPp0KEj3t7eri5T5LrRdL2ImEJKyiZ27NjGsGEjqFKlCv/+99ucPXuGrl2jcHd3Jzy8MzZboKvLFLmudCYvIuWaYRjMmfMKBw+mU7VqNXbv/gmHw8GJEyfYvXsX7u7uAAp4qZAU8iJSrlksFmy2QG6+uTF2ewY//PAdW7duITKyu358Rio8hbyIlHsdOoSyc+eP3HjjTfTtO4DExE+pX/9Ghgx5wNWlibiUQl5Eyr0GDRrSuHETPD09eeON1+jePYZq1bxcXZaIy+nGOxExhbZtO5CW9itNmjSjWbPmri5HrjPDMPS1yEtQyIuIKXh6etKkSVNXlyHXUWrqDqzWSjRt2kwBfxkKeREpl4a/tN7VJVxTi8dFurqEMu3dd9/i+PHjVK5cmXPnzhIc3M7VJZVJuiYvIiLlRkFBATt3phIQYGPUqCepXbsO/v4Bri6rzFLIi4hIuXDgwG/MmvUyNWvWJDb2NgAqV67CqVMnSUz8jOzsE64tsAxSyIuISJmXmvojy5Z9wJEjR/jyyxRn+65dO3n99dfw8KhE9eq+riuwjFLIi4hImedwOLj33iG8+upcsrKOk519gsLCQqpXr85dd91L165Rri6xTNKNdyIiUmatW7eW/Px8wsM74+NTnd9/P0p+fj4WiwU3NzfuuONuqlat5uoyyyydyYuISJljGAZxcW/w++9HcXNz45VX/gFArVr+eHp68vHH8QAK+KtQyIuISJnicDiwWCzUrFmLFi1akZV1HIejgAULXgdg2LD/4957h7i4yvJB0/UiIlJmbN++jY0bk4iI6ArAuXNnadasObffPpDPPkugsLAQNzc3PfymiHQmLyIiZUJmpp2vv97CrbeGk5Nz7r8B34KbbmrAiy8+T61atXBzU2z9FTqTFxERl9uwYR2//LKHrKwsWrdui4eHB9nZ2WRkHMHf358HH3yIevVudHWZ5Y4+EomIiMsYhsGcOa9gt2cQEBCAYRQyb94sAH79dT9Hj2ZSvbqvAr6YFPIiIuIyFosFmy2Qhg0b8fPPP9OoURNSU39k6dL38fDwoH37EFeXWK5pul5ERFyqQ4dQNm/eSFBQPbp3j2H37p9o3z6EBg1udnVp5Z7O5EVExKUaNGhI48ZNqFq1CrNm/ZMuXSIV8KVEZ/IiIuJybdt2IC3tV5o0aUazZs1dXY5plOhM/u2336ZXr1707t2bMWPGkJubS3p6OoMHDyY6OprRo0eTl5cHQF5eHqNHjyY6OprBgwdz8OBBZz8LFiwgOjqa2NhYNm/e7GxPTk4mNjaW6Oho4uLiSlKqiIiUYZ6enjRp0lQBX8qKfSZvt9tZsmQJa9asoXLlyowaNYqEhAQ2bdrE0KFD6dWrF1OmTGH58uXcc889LFu2DB8fHz7//HMSEhKYOXMms2fPZu/evSQkJJCQkIDdbmfYsGGsXbsWgOeff5633noLm83GoEGDiIyM5OabNYUjIlLeDX9pvatLuGYWj4t0dQlOJTqTdzgc5OTkUFBQQE5ODv7+/nz11VfExsYC0L9/f5KSkgBYv349/fv3ByA2NpYtW7ZgGAZJSUn06tULDw8PgoKCqF+/Pjt27GDHjh3Ur1+foKAgPDw86NWrl7MvERERubpih7zNZmP48OF069aNTp064eXlRfPmzfHx8cFqPT9BEBgYiN1uB86f+deuXRsAq9WKt7c3WVlZ2O12AgMDL+jXbrdftl1ERESKptjT9dnZ2SQlJZGUlIS3tzejRo264Hq6q/j5VcVqdXd1GdeUv7+3q0uQckB/J+Wbxq/8KktjV+yQ//LLL6lbty41atQAICYmhu+//56TJ09SUFCA1WolIyMDm80GnD8TP3LkCIGBgRQUFHDq1Cn8/Pyw2WxkZGQ4+7Xb7c5tLtd+JVlZZ4t7SOWCv783R4+ecnUZUg7o76R80/iVX9d77K70oaLY0/V16tRh+/btnDt3DsMw2LJlCzfffDMdO3Z03jgXHx9PZOT5GxAiIyOJjz//+79r164lNDQUi8VCZGQkCQkJ5OXlkZ6eTlpaGq1ataJly5akpaWRnp5OXl4eCQkJzr5ERETk6op9Jt+6dWtiY2Pp378/VquVZs2aceedd9K1a1f+/ve/M3v2bJo1a8bgwYMBGDRoEGPHjiU6Oprq1asza9b5ZxM3atSInj17ctttt+Hu7s6UKVNwdz8/3T5lyhQefPBBHA4HAwcOpFGjRqVwyCIiIhWDxTAMw9VFlCazT3Fpur70mPkrPFC2vsZzLWj8yjczj9/1HrtrMl0vIiIiZZtCXkRExKQU8iIiIialkBcRETEphbyIiIhJKeRFRERMSiEvIiJiUgp5ERERk1LIi4iImJRCXkRExKQU8iIiIialkBcRETEphbyIiIhJKeRFRERMSiHvIqmpP7J//z5XlyEiIiamkHeBzz5L4IMP3uXgwXS+//5bV5cjIiImpZB3gTNnzlCjRk0CAmx88skqkpI+d3VJIiJiQgr56+TgwXRWrlxGTk4OYFC9ui9NmzZjxIhHycy0k5+f7+oSRUTEZBTy10Fq6g7effctDh1K54svkomO7sGpUydZu3YNcXFvULduXSpVquTqMkVExGSsri6gInA4HPzf/z3KkSOHePvtRVSpUpWYmJ54eXlhswXSpk1bV5coIiImpDP5ayglJZnU1B20bh1MrVq1yM3N5e9/H8t3332Np2dl6tW7UQEvIiLXjEL+Glmz5hNWrlzK3r2/sGrVcgDatw/hp59SAQv+/gGuLVBERExPIV/KCgoKALBaK3HzzY259dZOnD17lsWL48jNzeHUqVM89tgofHx8XFypiIiYnUK+FKWl/cp7771DYWEhnp6eOBwF+PhUp2fP3lSqVAl3dysDB96Bm5vedhERufZ0410p2bLlCz75ZBVVqlRh7do1NG7clD17dvPpp6v55put9O3bH6tVb7eIiFw/Sp1S8MMP3/Hrr/s5cSKLPn36kZ19gm3bvqdPn37UrFmLjh3DqFPnBleXKSIiFYxCvoQSEj5m8+aN9O07AKvVnbCwcABmznyRevXqExhYWwEvIiIuoZAvITc3N3x8qnPjjTexcWMSn366mqNHM6lVy5/g4HauLk9ERCow3QFWQv7+Afj51cDPrwZDhjxAZqadwMA6DB36oK7Bi4iISymFSig4uB2HDh3kk0/iSU39kX79BtK2bXtXlyUiIqKQLyl3d3d6976ds2fP0r59Rxo0aOjqkkRERACFfIkNf2n9/7T85pI6rpXF4yJdXYKIiBSTrsmLiIiYlEJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS8iImJSCnkRERGTUsiLiIiYlEJeRETEpEoU8idPnmTkyJH06NGDnj178sMPP3DixAmGDRtGTEwMw4YNIzs7GwDDMJg+fTrR0dH06dOHnTt3OvuJj48nJiaGmJgY4uPjne2pqan06dOH6Ohopk+fjmEYJSlXRESkQilRyL/wwgt07tyZzz77jI8++oiGDRsSFxdHWFgYiYmJhIWFERcXB0BycjJpaWkkJiYybdo0pk6dCsCJEyeYN28eS5cuZdmyZcybN8/5wWDq1KlMmzaNxMRE0tLSSE5OLtnRioiIVCDFDvlTp07xzTffMGjQIAA8PDzw8fEhKSmJfv36AdCvXz/WrVsH4Gy3WCy0adOGkydPkpmZSUpKCuHh4fj6+lK9enXCw8PZvHkzmZmZnD59mjZt2mCxWOjXrx9JSUklP2IREZEKoti/Qnfw4EFq1KjB+PHj2b17N82bN2fixIkcO3aMgIAAAPz9/Tl27BgAdrudwMBA5/aBgYHY7faL2m022yXb/1hfREREiqbYIV9QUMBPP/3E5MmTad26NdOnT3dOzf/BYrFgsVhKXORf4edXFavV/bru08z8/b1dXYIUk8aufNP4lV9laeyKHfKBgYEEBgbSunVrAHr06EFcXBw1a9YkMzOTgIAAMjMzqVGjBnD+DD0jI8O5fUZGBjabDZvNxtdff+1st9vthISEXHb9q8nKOlvcQ5JLOHr0lKtLkGLS2JVvGr/y63qP3ZU+VBT7mry/vz+BgYHs378fgC1bttCwYUMiIyNZtWoVAKtWrSIqKgrA2W4YBtu2bcPb25uAgAA6depESkoK2dnZZGdnk5KSQqdOnQgICMDLy4tt27ZhGMYFfYmIiMjVFftMHmDy5Mk89dRT5OfnExQUxIsvvkhhYSGjR49m+fLl1KlTh9mzZwPQpUsXNm3aRHR0NFWqVGHGjBkA+Pr68uijjzpv4Hvsscfw9fUF4Nlnn2X8+PHk5OQQERFBREREScoVERGpUEoU8s2aNWPlypUXtb/zzjsXtVksFp599tlL9jNo0CBnyP9Zy5YtWb16dUlKFBERqbD0xDsRERGTUsiLiIiYlEJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS8iImJSCnkRERGTUsiLiIiYlEJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS8iImJSCnkRERGTUsiLiIiYlEJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS8iImJSCnkRERGTUsiLiIiYlEJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS8iImJSCnkRERGTUsiLiIiYlEJeRETEpBTyIiIiJqWQFxERMSmFvIiIiEkp5EVERExKIS8iImJSCnkRERGTUsiLiIiYVIlD3uFw0K9fPx566CEA0tPTGTx4MNHR0YwePZq8vDwA8vLyGD16NNHR0QwePJiDBw86+1iwYAHR0dHExsayefNmZ3tycjKxsbFER0cTFxdX0lJFREQqlBKH/JIlS2jYsKHz9cyZMxk6dCiff/45Pj4+LF++HIBly5bh4+PD559/ztChQ5k5cyYAe/fuJSEhgYSEBBYuXMhzzz2Hw+HA4XDw/PPPs3DhQhISEli9ejV79+4tabkiIiIVRolCPiMjg40bNzJo0CAADMPgq6++IjY2FoD+/fuTlJQEwPr16+nfvz8AsbGxbNmyBcMwSEpKolevXnh4eBAUFET9+vXZsWMHO3bsoH79+gQFBeHh4UGvXr2cfYmIiMjVlSjkZ8yYwdixY3FzO99NVlYWPj4+WK1WAAIDA7Hb7QDY7XZq164NgNVqxdvbm6ysLOx2O4GBgc4+bTYbdrv9su0iIiJSNNbibrhhwwZq1KhBixYt2Lp1a2nWVCJ+flWxWt1dXYZp+Pt7u7oEKSaNXfmm8Su/ytLYFTvkv//+e9avX09ycjK5ubmcPn2aF154gZMnT1JQUIDVaiUjIwObzQacPxM/cuQIgYGBFBQUcOrUKfz8/LDZbGRkZDj7tdvtzm0u134lWVlni3tIcglHj55ydQlSTBq78k3jV35d77G70oeKYk/XP/nkkyQnJ7N+/XpeffVVQkNDeeWVV+jYsSNr164FID4+nsjISAAiIyOJj48HYO3atYSGhmKxWIiMjCQhIYG8vDzS09NJS0ujVatWtGzZkrS0NNLT08nLyyMhIcHZl4iIiFxdsc/kL2fs2LH8/e9/Z/bs2TRr1ozBgwcDMGjQIMaOHUt0dDTVq1dn1qxZADRq1IiePXty22234e7uzpQpU3B3Pz/dPmXKFB588EEcDgcDBw6kUaNGpV2uiIiIaZVKyHfs2JGOHTsCEBQU5Pza3J95enoyZ86cS27/yCOP8Mgjj1zU3qVLF7p06VIaJYqIiFQ4euKdiIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETKrYIX/kyBHuv/9+brvtNnr16sU777wDwIkTJxg2bBgxMTEMGzaM7OxsAAzDYPr06URHR9OnTx927tzp7Cs+Pp6YmBhiYmKIj493tqemptKnTx+io6OZPn06hmEUt1wREZEKp9gh7+7uzrhx41izZg0ffvgh77//Pnv37iUuLo6wsDASExMJCwsjLi4OgOTkZNLS0khMTGTatGlMnToVOP+hYN68eSxdupRly5Yxb9485weDqVOnMm3aNBITE0lLSyM5ObnkRywiIlJBFDvkAwICaN68OQBeXl40aNAAu91OUlIS/fr1A6Bfv36sW7cOwNlusVho06YNJ0+eJDMzk5SUFMLDw/H19aV69eqEh4ezefNmMjMzOX36NG3atMFisdCvXz+SkpJKfsQiIiIVRKlckz948CC7du2idevWHDt2jICAAAD8/f05duwYAHa7ncDAQOc2gYGB2O32i9ptNtsl2/9YX0RERIrGWtIOzpw5w8iRI5kwYQJeXl4XLLNYLFgslpLu4i/x86uK1ep+XfdpZv7+3q4uQYpJY1e+afzKr7I0diUK+fz8fEaOHEmfPn2IiYkBoGbNmmRmZhIQEEBmZiY1atQAzp+hZ2RkOLfNyMjAZrNhs9n4+uuvne12u52QkJDLrn81WVlnS3JI8j+OHj3l6hKkmDR25ZvGr/y63mN3pQ8VxZ6uNwyDiRMn0qBBA4YNG+Zsj4yMZNWqVQCsWrWKqKioC9oNw2Dbtm14e3sTEBBAp06dSElJITs7m+zsbFJSUujUqRMBAQF4eXmxbds2DMO4oC8RERG5umKfyX/33Xd89NFHNG7cmNtvvx2AMWPGMGLECEaPHs3y5cupU6cOs2fPBqBLly5s2rSJ6OhoqlSpwowZMwDw9fXl0UcfZdCgQQA89thj+Pr6AvDss88yfvx4cnJyiIiIICIiogSHKiIiUrEUO+Tbt2/Pzz//fMllf3xn/s8sFgvPPvvsJdcfNGiQM+T/rGXLlqxevbq4JYqIiFRoeuKdiIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETEohLyIiYlIKeREREZNSyIuIiJiUQl5ERMSkFPIiIiImpZAXERExKYW8iIiISSnkRURETKrMh3xycjKxsbFER0cTFxfn6nJERETKjTId8g6Hg+eff56FCxeSkJDA6tWr2bt3r6vLEhERKRfKdMjv2LGD+vXrExQUhIeHB7169SIpKcnVZYmIiJQLZTrk7XY7gYGBztc2mw273e7CikRERMoPq6sLKG3+/t7XdX+fvHL7dd2flB6NXfmm8SvfNH7XR5k+k7fZbGRkZDhf2+12bDabCysSEREpP8p0yLds2ZK0tDTS09PJy8sjISGByMhIV5clIiJSLpTp6Xqr1cqUKVN48MEHcTgcDBw4kEaNGrm6LBERkXLBYhiG4eoiREREpPSV6el6ERERKT6FvIiIiEkp5EVEREyqTN94V9Ht27ePpKQkMjMzAQgICCAqKoqGDRu6uDIRkfLj6aef5uWXX3Z1GS6hG+/KqLi4OBISEujVq5fz2QB2u93ZNmLECBdXKGJu+/btIzMzk1atWlGtWjVne3JyMhERES6sTK7k4Ycfvqht69atdOzYEYD58+df75JcSmfyZdSKFStYvXo1lSpVuqB96NCh9O7dWyFfjq1YsYKBAwe6ugy5giVLlvDee+/RsGFDdu/ezYQJE+jevTsAs2bNUsiXYXa7nYYNGzJ48GAsFguGYZCamsrw4cNdXZpL6Jp8GWWxWJzT9H929OhRLBaLCyqS0jJ37lxXlyBXsWzZMlauXMkbb7zBkiVLeOONN3jnnXcA0ORn2bZixQpatGjB/Pnz8fb2pmPHjnh6ehISEkJISIiry7vudCZfRk2YMIGhQ4dSv359ateuDcDhw4c5cOAAkydPdnF1cjV9+vS57LLff//9OlYixVFYWOicoq9bty7vvvsuI0eO5PDhwwr5Ms7NzY2hQ4fSo0cPZsyYQa1atXA4HK4uy2UU8mVUREQEa9euZceOHc5f3rPZbLRs2RJ3d3cXVydXc+zYMRYtWoSPj88F7YZhcNddd7moKimqmjVrsmvXLpo1awZAtWrVWLBgARMmTGDPnj0urk6KIjAwkDlz5rBx40a8vLxcXY7L6MY7kWtgwoQJDBgwgPbt21+07Mknn+SVV15xQVVSVBkZGbi7u+Pv73/Rsu+++4527dq5oCqRv04hLyIiYlK68U5ERMSkFPIiIiImpZAXkWtu4sSJfPvtt5dcNm7cOP79739f54pEKgbdXS8i19wLL7zg6hJEKiSFvIhw7tw5nnnmGfbu3YvVauWmm27itddeIz4+nvfffx+Hw4GXlxdTp06lQYMGACxYsIDVq1djsVioWrUq77//Pm5ul54cvP/++xk+fDjdunXDbrfz9NNPc/ToUW644YbLbiMiJaeQFxFSUlI4c+YMa9asASA7O5tvv/2WTz/9lPfeew8PDw82bdrEhAkT+OCDD4iPj2f9+vX85z//wcvLi6ysrCKH9fTp0+nQoQOPP/446enp9O3bl86dO1/LwxOpsBTyIkLTpk3Zt28fzz33HCEhIXTt2pX169eze/duBg8eDJx/kM/JkycB2LBhA3fffbfzISN+fn5F3tfWrVuZNGkSAEFBQYSFhZXy0YjIHxTyIkJQUBCrV6/mq6++Ijk5mVmzZhEVFcXAgQMZNWqUq8sTkWLSxTARcT7hrXv37owfP57jx48TGRnJRx99REZGBgAOh4PU1FQAunXrxn/+8x9Onz4NQFZWVpH3FRoayooVKwBIT09ny5YtpXw0IvIHncmLCD///LPzUbuFhYWMGDGCDh06MHr0aB555BEcDgf5+fn06NGDFi1a0K9fP+x2O3feeSdWq5WqVavy3nvvFem6/MSJE3n66adZvXo1devWdf7Ot4iUPj3WVkRExKQ0XS8iImJSmq4XkVKxadMmXn311Yvax4wZQ5cuXVxQkYhoul5ERMSkNF0vIiJiUgp5ERERk1LIi4iImJRCXkRExKQU8iIiIib1/wDGMJuAtjqRoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bstnnx.frontend.stage_flow_control_main\n",
    "RESULT_DIR = output_dir\n",
    "extra_option = {\n",
    "    \"logging_level\": \"INFO\",\n",
    "    \"priority_range\": \"100-1000,1200\",\n",
    "    \"build_type\": \"html\",\n",
    "    \"BSTNNX_NET_FW_DIR\": \"/bsnn/work/bstnnx_release/third_party/Net-FW-xos-v0-edp-gemm\"\n",
    "}\n",
    "\n",
    "bstnnx.frontend.stage_flow_control_main.bstnnx_run(config=CONFIG_FILE_PATH, \\\n",
    "                                                      result_dir=RESULT_DIR, \\\n",
    "                                                      extra=extra_option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86511ca",
   "metadata": {},
   "source": [
    "# Sanity check with reference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "515322ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_reference_model_path = os.path.join(output_dir, \"300_QuantizationStage/quant_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bde8fab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3476797342300415 100.0\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "custom_bst_op_path = bstnnx.backend.custom_op.get_custom_op_lib_path()\n",
    "onnx_rep = runtime_helper.OnnxRep(bst_reference_model_path, custom_op_lib=custom_bst_op_path)\n",
    "bst_refernce_loss, bst_reference_acc  = test(onnx_rep, device, test_loader)\n",
    "print(bst_refernce_loss, bst_reference_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408cae7",
   "metadata": {},
   "source": [
    "```{.python .input}\n",
    "print(f\"Float model - Loss: {float_loss} | Accuracy: {float_acc}\")\n",
    "print(f\"Torch QAT model - Loss: {torch_qat_loss} | Accuracy: {torch_qat_acc}\")\n",
    "print(f\"BST Reference model - Loss: {bst_refernce_loss} | Accuracy: {bst_reference_acc}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.9.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "29481b19616862e67e4bae0fd078da766e713ba66d65398c80d88a2e079c89da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
