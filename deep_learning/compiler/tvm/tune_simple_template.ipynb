{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nWriting tunable template and Using auto-tuner\n=============================================\n**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_\n\nThis is an introduction tutorial to the auto-tuning module in TVM.\n\nThere are two steps in auto-tuning.\nThe first step is defining a search space.\nThe second step is running a search algorithm to explore through this space.\nIn this tutorial, you can learn how to perform these two steps in TVM.\nThe whole workflow is illustrated by a matrix multiplication example.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install dependencies\n",
        "--------------------\n",
        "To use autotvm package in TVM, we need to install some extra dependencies.\n",
        "This step (installing xgboost) can be skipped as it doesn't need XGBoost\n",
        "(change \"3\" to \"2\" if you use python2):\n",
        "\n",
        ".. code-block:: bash\n",
        "\n",
        "  pip3 install --user psutil xgboost\n",
        "\n",
        "To make TVM run faster in tuning, it is recommended to use cython\n",
        "as FFI of TVM. In the root directory of TVM, execute\n",
        "(change \"3\" to \"2\" if you use python2):\n",
        "\n",
        ".. code-block:: bash\n",
        "\n",
        "  pip3 install --user cython\n",
        "  sudo make cython3\n",
        "\n",
        "Now return to python code. Import packages.\n",
        "\n",
        "# Jupyter notebook has some conflicts with Tornado asynio, this notebook will have errors, but python code OK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm import te\n",
        "\n",
        "# the module is called `autotvm`\n",
        "from tvm import autotvm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1:  Define the search space\n--------------------------------\nIn this section, we will rewrite a deterministic TVM schedule code to a\ntunable schedule template. You can regard the process of search space definition\nas the parameterization of our existing schedule code.\n\nTo begin with, here is how we implement a blocked matrix multiplication in TVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Matmul V0: Constant tiling factor\n",
        "def matmul_v0(N, L, M, dtype):\n",
        "    A = te.placeholder((N, L), name='A', dtype=dtype)\n",
        "    B = te.placeholder((L, M), name='B', dtype=dtype)\n",
        "\n",
        "    k = te.reduce_axis((0, L), name='k')\n",
        "    C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name='C')\n",
        "    s = te.create_schedule(C.op)\n",
        "\n",
        "    # schedule\n",
        "    y, x = s[C].op.axis\n",
        "    k = s[C].op.reduce_axis[0]\n",
        "\n",
        "    yo, yi = s[C].split(y, 8)\n",
        "    xo, xi = s[C].split(x, 8)\n",
        "\n",
        "    s[C].reorder(yo, xo, k, yi, xi)\n",
        "\n",
        "    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parametrize the schedule\n^^^^^^^^^^^^^^^^^^^^^^^^\nIn the previous schedule code, we use a constant \"8\" as tiling factor.\nHowever, it might not be the best one because the best tiling factor depends\non real hardware environment and input shape.\n\nIf you want the schedule code to be portable across a wider range of input shapes\nand target hardware, it is better to define a set of candidate values and\npick the best one according to the measurement results on target hardware.\n\nIn autotvm, we can define a tunable parameter, or a \"knob\" for such kind of value.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Matmul V1: List candidate values\n",
        "@autotvm.template(\"tutorial/matmul_v1\")  # 1. use a decorator\n",
        "def matmul_v1(N, L, M, dtype):\n",
        "    A = te.placeholder((N, L), name='A', dtype=dtype)\n",
        "    B = te.placeholder((L, M), name='B', dtype=dtype)\n",
        "\n",
        "    k = te.reduce_axis((0, L), name='k')\n",
        "    C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name='C')\n",
        "    s = te.create_schedule(C.op)\n",
        "\n",
        "    # schedule\n",
        "    y, x = s[C].op.axis\n",
        "    k = s[C].op.reduce_axis[0]\n",
        "\n",
        "    # 2. get the config object\n",
        "    cfg = autotvm.get_config()\n",
        "\n",
        "    # 3. define search space\n",
        "    cfg.define_knob(\"tile_y\", [1, 2, 4, 8, 16])\n",
        "    cfg.define_knob(\"tile_x\", [1, 2, 4, 8, 16])\n",
        "\n",
        "    # 4. schedule according to config\n",
        "    yo, yi = s[C].split(y, cfg['tile_y'].val)\n",
        "    xo, xi = s[C].split(x, cfg['tile_x'].val)\n",
        "\n",
        "    s[C].reorder(yo, xo, k, yi, xi)\n",
        "\n",
        "    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we make four modifications to the previous schedule code and get\na tunable \"template\". We can explain the modifications one by one.\n\n1. Use a decorator to mark this function as a simple template.\n2. Get a config object:\n   You can regard this :code:`cfg` as an argument of this function but\n   we obtain it in a different way. With this argument, this function is no longer\n   a deterministic schedule code. Instead, we can pass different configurations to\n   this function and get different schedules, so this function is a \"template\".\n\n   To make the template function more compact, we do two things in a single function.\n   (1) define a search space and (2) schedule according to an entity in this space.\n   To achieve this, we make :code:`cfg` be either\n   a :any:`ConfigSpace` or a :any:`ConfigEntity` object.\n\n   When it is a :any:`ConfigSpace`, it will collect all tunable knobs in this function and\n   build the search space.\n   When it is a :any:`ConfigEntity`, it will ignore all space definition API\n   (namely, :code:`cfg.define_XXXXX(...)`).   Instead, it stores deterministic values for\n   all tunable knobs, and we schedule according to these values.\n\n   During auto-tuning, we will first call this template with a :any:`ConfigSpace`\n   object to build the search space. Then we call this template with different :any:`ConfigEntity`\n   in the built space to get different schedules. Finally we will measure the code generated by\n   different schedules and pick the best one.\n\n3. Define two tunable knobs. The first one is :code:`tile_y` with\n   5 possible values. The second one is :code:`tile_x` with a same\n   list of possible values. These two knobs are independent, so they\n   span a search space with size = 5x5 = 25\n4. Schedule according to the deterministic values in :code:`cfg`\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use better space definition API\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIn the previous template, we manually list all possible values for a knob.\nThis is the lowest level API to define the space.\nHowever, we also provide another set of API to make the space definition\neasier and smarter. It is recommended to use this set of high level API.\n\nIn the following example, we use :any:`ConfigSpace.define_split` to define a split\nknob. It will enumerate all the possible ways to split an axis and construct\nthe space.\n\nWe also have :any:`ConfigSpace.define_reorder` for reorder knob and\n:any:`ConfigSpace.define_annotate` for annotation like unroll, vectorization,\nthread binding.\nWhen the high level API cannot meet your requirement, you can always fall\nback to use low level API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@autotvm.template(\"tutorial/matmul\")\n",
        "def matmul(N, L, M, dtype):\n",
        "    A = te.placeholder((N, L), name='A', dtype=dtype)\n",
        "    B = te.placeholder((L, M), name='B', dtype=dtype)\n",
        "\n",
        "    k = te.reduce_axis((0, L), name='k')\n",
        "    C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name='C')\n",
        "    s = te.create_schedule(C.op)\n",
        "\n",
        "    # schedule\n",
        "    y, x = s[C].op.axis\n",
        "    k = s[C].op.reduce_axis[0]\n",
        "\n",
        "    ##### define space begin #####\n",
        "    cfg = autotvm.get_config()\n",
        "    cfg.define_split(\"tile_y\", y, num_outputs=2)\n",
        "    cfg.define_split(\"tile_x\", x, num_outputs=2)\n",
        "    ##### define space end #####\n",
        "\n",
        "    # schedule according to config\n",
        "    yo, yi = cfg[\"tile_y\"].apply(s, C, y)\n",
        "    xo, xi = cfg[\"tile_x\"].apply(s, C, x)\n",
        "\n",
        "    s[C].reorder(yo, xo, k, yi, xi)\n",
        "\n",
        "    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>More Explanation on :code:`cfg.defile_split`</p></div>\n\n In this template, :code:`cfg.define_split(\"tile_y\", y, num_outputs=2)` will enumerate\n all possible combinations that can split axis y into two axes with factors of the length of y.\n For example, if the length of y is 32 and we want to split it into two axes\n using factors of 32, then there are 6 possible values for\n (length of outer axis, length of inner axis) pair, namely\n (32, 1), (16, 2), (8, 4), (4, 8), (2, 16) or (1, 32).\n They are just the 6 possible values of `tile_y`.\n\n During schedule, :code:`cfg[\"tile_y\"]` is a :code:`SplitEntity` object.\n We stores the lengths of outer axes and inner axes in :code:`cfg['tile_y'].size`\n (a tuple with two elements).\n In this template, we apply it by using :code:`yo, yi = cfg['tile_y'].apply(s, C, y)`.\n Actually, this is equivalent to\n :code:`yo, yi = s[C].split(y, cfg[\"tile_y\"].size[1])`\n or  :code:`yo, yi = s[C].split(y, nparts=cfg['tile_y\"].size[0])`\n\n The advantage of using cfg.apply API is that it makes multi-level split\n (when num_outputs >= 3) easier.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2:  Search through the space\n---------------------------------\nIn step 1, we build the search space by extending our old schedule code\ninto a template. The next step is to pick a tuner and explore in this space.\n\nAuto-tuners in TVM\n^^^^^^^^^^^^^^^^^^\nThe job for a tuner can be described by following pseudo code\n\n  .. code-block:: c\n\n   ct = 0\n   while ct < max_number_of_trials:\n       propose a batch of configs\n       measure this batch of configs on real hardware and get results\n       ct += batch_size\n\nWhen proposing the next batch of configs, the tuner can take different strategies. We\nprovide four tuners with different strategies in autotvm.\n\n* :any:`RandomTuner`: Enumerate the space in a random order\n* :any:`GridSearchTuner`: Enumerate the space in a grid search order\n* :any:`GATuner`: Using genetic algorithm to search through the space\n* :any:`XGBTuner`: Uses a model based method. Train a XGBoost model to predict the speed of lowered IR and pick the next batch according to the prediction.\n\nYou can choose the tuner according to the size of your space, your time budget and other factors.\nFor example, if your space is very small (less than 1000), a gridsearch tuner or a\nrandom tuner is good enough. If your space is at the level of 10^9 (this is the space\nsize of a conv2d operator on CUDA GPU), XGBoostTuner can explore more efficiently\nand find better configs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin tuning\n^^^^^^^^^^^^\nHere we continue our matrix multiplication example.\nFirst we should create a tuning task.\nWe can also inspect the initialized search space.\nIn this case, for a 512x512 square matrix multiplication, the space size\nis 10x10=100\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ConfigSpace (len=100, space_map=\n   0 tile_y: Split(policy=factors, product=512, num_outputs=2) len=10\n   1 tile_x: Split(policy=factors, product=512, num_outputs=2) len=10\n)\n"
        }
      ],
      "source": [
        "N, L, M = 512, 512, 512\n",
        "task = autotvm.task.create(\"tutorial/matmul\", args=(N, L, M, 'float32'), target='llvm')\n",
        "print(task.config_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to define how to measure the generated code and pick a tuner.\nSince our space is small, a random tuner is just okay.\n\nWe only make 10 trials in this tutorial for demonstration. In practice,\nyou can do more trials according to your time budget.\nWe will log the tuning results into a log file. This file can be\nused to get the best config later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Process Process-1:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/hongbing/Projects/tvm/python/tvm/rpc/tracker.py\", line 355, in _tracker_server\n    handler.run()\n  File \"/home/hongbing/Projects/tvm/python/tvm/rpc/tracker.py\", line 351, in run\n    self._ioloop.start()\n  File \"/home/hongbing/.local/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 425, in run_forever\n    raise RuntimeError('This event loop is already running')\nRuntimeError: This event loop is already running\n"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot get remote devices from the tracker. Please check the status of tracker by 'python -m tvm.exec.query_rpc_tracker --port [THE PORT YOU USE]' and make sure you have free devices on the queue status.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1ea9cf31de74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m tuner.tune(n_trial=10,\n\u001b[1;32m     16\u001b[0m            \u001b[0mmeasure_option\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasure_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m            callbacks=[autotvm.callback.log_to_file('matmul.log')])\n\u001b[0m",
            "\u001b[0;32m~/Projects/tvm/python/tvm/autotvm/tuner/tuner.py\u001b[0m in \u001b[0;36mtune\u001b[0;34m(self, n_trial, measure_option, early_stopping, callbacks)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mevery\u001b[0m \u001b[0mmeasurement\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSee\u001b[0m \u001b[0mautotvm\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \"\"\"\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mmeasure_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_measure_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasure_option\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mn_parallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_parallel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1e9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/autotvm/measure/measure.py\u001b[0m in \u001b[0;36mcreate_measure_batch\u001b[0;34m(task, option)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'runner'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mattach_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;31m# feed device related information from runner to builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/autotvm/measure/measure_methods.py\u001b[0m in \u001b[0;36mset_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLocalRunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/autotvm/measure/measure_methods.py\u001b[0m in \u001b[0;36mset_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get devices for measurement successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             raise RuntimeError(\"Cannot get remote devices from the tracker. \"\n\u001b[0m\u001b[1;32m    216\u001b[0m                                \u001b[0;34m\"Please check the status of tracker by \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                                \u001b[0;34m\"'python -m tvm.exec.query_rpc_tracker --port [THE PORT YOU USE]' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot get remote devices from the tracker. Please check the status of tracker by 'python -m tvm.exec.query_rpc_tracker --port [THE PORT YOU USE]' and make sure you have free devices on the queue status."
          ]
        }
      ],
      "source": [
        "# logging config (for printing tuning log to the screen)\n",
        "logging.getLogger('autotvm').setLevel(logging.DEBUG)\n",
        "logging.getLogger('autotvm').addHandler(logging.StreamHandler(sys.stdout))\n",
        "\n",
        "# There are two steps for measuring a config: build and run.\n",
        "# By default, we use all CPU cores to compile program. Then measure them sequentially.\n",
        "# We measure 5 times and take average to reduce variance.\n",
        "measure_option = autotvm.measure_option(\n",
        "    builder='local',\n",
        "    runner=autotvm.LocalRunner(number=5))\n",
        "\n",
        "# Begin tuning with RandomTuner, log records to file `matmul.log`\n",
        "# You can use alternatives like XGBTuner.\n",
        "tuner = autotvm.tuner.RandomTuner(task)\n",
        "tuner.tune(n_trial=10,\n",
        "           measure_option=measure_option,\n",
        "           callbacks=[autotvm.callback.log_to_file('matmul.log')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we apply history best from the cache file and check its correctness.\nWe can call the function :code:`matmul` directly under the\n:any:`autotvm.apply_history_best` context. When we call this function,\nit will query the dispatch context with its argument and get the best config\nwith the same argument.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# apply history best from log file\nwith autotvm.apply_history_best('matmul.log'):\n    with tvm.target.create(\"llvm\"):\n        s, arg_bufs = matmul(N, L, M, 'float32')\n        func = tvm.build(s, arg_bufs)\n\n# check correctness\na_np = np.random.uniform(size=(N, L)).astype(np.float32)\nb_np = np.random.uniform(size=(L, M)).astype(np.float32)\nc_np = a_np.dot(b_np)\n\nc_tvm = tvm.nd.empty(c_np.shape)\nfunc(tvm.nd.array(a_np), tvm.nd.array(b_np), c_tvm)\n\ntvm.testing.assert_allclose(c_np, c_tvm.asnumpy(), rtol=1e-2)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}