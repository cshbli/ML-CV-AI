{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDeploy a Quantized Model on Cuda\n================================\n**Author**: `Wuwei Lin <https://github.com/vinx13>`_\n\nThis article is an introductory tutorial of automatic quantization with TVM.\nAutomatic quantization is one of the quantization modes in TVM. More details on\nthe quantization story in TVM can be found\n`here <https://discuss.tvm.ai/t/quantization-story/3920>`_.\nIn this tutorial, we will import a GluonCV pre-trained model on ImageNet to\nRelay, quantize the Relay model and then perform the inference.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install mxnet on the machine\n",
        "\n",
        "pip3 install mxnet-cu100 for CUDA 10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "from tvm import te\n",
        "from tvm import relay\n",
        "import mxnet as mx\n",
        "from tvm.contrib.download import download_testdata\n",
        "from mxnet import gluon\n",
        "import logging\n",
        "import os\n",
        "\n",
        "batch_size = 1\n",
        "model_name = \"resnet18_v1\"\n",
        "target = 'cuda'\n",
        "ctx = tvm.context(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the Dataset\n-------------------\nWe will demonstrate how to prepare the calibration dataset for quantization.\nWe first download the validation set of ImageNet and pre-process the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading from url http://data.mxnet.io.s3-website-us-west-1.amazonaws.com/data/val_256_q90.rec to /home/hongbing/.tvm_test_data/val_256_q90.rec\n...1%, 23.52 MB, 8715 KB/s, 2 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...3%, 56.43 MB, 11538 KB/s, 5 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...6%, 89.78 MB, 12536 KB/s, 7 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...8%, 122.91 MB, 12918 KB/s, 9 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...10%, 156.93 MB, 13263 KB/s, 12 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...13%, 190.64 MB, 13488 KB/s, 14 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...15%, 225.12 MB, 13642 KB/s, 16 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...18%, 259.59 MB, 13869 KB/s, 19 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...20%, 293.62 MB, 13966 KB/s, 21 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...22%, 326.39 MB, 14058 KB/s, 23 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...25%, 360.63 MB, 14157 KB/s, 26 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...28%, 414.85 MB, 14230 KB/s, 29 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...31%, 450.77 MB, 14208 KB/s, 32 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...35%, 509.28 MB, 14277 KB/s, 36 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...37%, 542.29 MB, 14281 KB/s, 38 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...42%, 613.59 MB, 14345 KB/s, 43 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...44%, 646.72 MB, 14347 KB/s, 46 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...49%, 711.57 MB, 14365 KB/s, 50 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...51%, 744.91 MB, 14370 KB/s, 53 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...56%, 810.27 MB, 14441 KB/s, 57 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...58%, 843.01 MB, 14442 KB/s, 59 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...62%, 905.42 MB, 14439 KB/s, 64 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...67%, 964.99 MB, 14466 KB/s, 68 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...69%, 1000.41 MB, 14447 KB/s, 70 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...74%, 1064.62 MB, 14432 KB/s, 75 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...76%, 1097.42 MB, 14426 KB/s, 77 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...80%, 1157.29 MB, 14422 KB/s, 82 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...82%, 1191.21 MB, 14402 KB/s, 84 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...87%, 1253.27 MB, 14401 KB/s, 89 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...91%, 1313.44 MB, 14404 KB/s, 93 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...94%, 1362.84 MB, 14404 KB/s, 96 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n...99%, 1426.58 MB, 14420 KB/s, 101 seconds passedIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n"
        }
      ],
      "source": [
        "calibration_rec = download_testdata(\n",
        "    'http://data.mxnet.io.s3-website-us-west-1.amazonaws.com/data/val_256_q90.rec',\n",
        "    'val_256_q90.rec')\n",
        "\n",
        "def get_val_data(num_workers=4):\n",
        "    mean_rgb = [123.68, 116.779, 103.939]\n",
        "    std_rgb = [58.393, 57.12, 57.375]\n",
        "\n",
        "    def batch_fn(batch):\n",
        "        return batch.data[0].asnumpy(), batch.label[0].asnumpy()\n",
        "\n",
        "    img_size = 299 if model_name == 'inceptionv3' else 224\n",
        "    val_data = mx.io.ImageRecordIter(\n",
        "        path_imgrec=calibration_rec,\n",
        "        preprocess_threads=num_workers,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "        resize=256,\n",
        "        data_shape=(3, img_size, img_size),\n",
        "        mean_r=mean_rgb[0],\n",
        "        mean_g=mean_rgb[1],\n",
        "        mean_b=mean_rgb[2],\n",
        "        std_r=std_rgb[0],\n",
        "        std_g=std_rgb[1],\n",
        "        std_b=std_rgb[2],\n",
        "    )\n",
        "    return val_data, batch_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The calibration dataset should be an iterable object. We define the\ncalibration dataset as a generator object in Python. In this tutorial, we\nonly use a few samples for calibration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibration_samples = 10\n",
        "\n",
        "def calibrate_dataset():\n",
        "    val_data, batch_fn = get_val_data()\n",
        "    val_data.reset()\n",
        "    for i, batch in enumerate(val_data):\n",
        "        if i * batch_size >= calibration_samples:\n",
        "            break\n",
        "        data, _ = batch_fn(batch)\n",
        "        yield {'data': data}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the model\n----------------\nWe use the Relay MxNet frontend to import a model from the Gluon model zoo.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    gluon_model = gluon.model_zoo.vision.get_model(model_name, pretrained=True)\n",
        "    img_size = 299 if model_name == 'inceptionv3' else 224\n",
        "    data_shape = (batch_size, 3, img_size, img_size)\n",
        "    mod, params = relay.frontend.from_mxnet(gluon_model, {\"data\": data_shape})\n",
        "    return mod, params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantize the Model\n------------------\nIn quantization, we need to find the scale for each weight and intermediate\nfeature map tensor of each layer.\n\nFor weights, the scales are directly calculated based on the value of the\nweights. Two modes are supported: `power2` and `max`. Both modes find the\nmaximum value within the weight tensor first. In `power2` mode, the maximum\nis rounded down to power of two. If the scales of both weights and\nintermediate feature maps are power of two, we can leverage bit shifting for\nmultiplications. This make it computationally more efficient. In `max` mode,\nthe maximum is used as the scale. Without rounding, `max` mode might have\nbetter accuracy in some cases. When the scales are not powers of two, fixed\npoint multiplications will be used.\n\nFor intermediate feature maps, we can find the scales with data-aware\nquantization. Data-aware quantization takes a calibration dataset as the\ninput argument. Scales are calculated by minimizing the KL divergence between\ndistribution of activation before and after quantization.\nAlternatively, we can also use pre-defined global scales. This saves the time\nfor calibration. But the accuracy might be impacted.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def quantize(mod, params, data_aware):\n",
        "    if data_aware:\n",
        "        with relay.quantize.qconfig(calibrate_mode='kl_divergence', weight_scale='max'):\n",
        "            mod = relay.quantize.quantize(mod, params, dataset=calibrate_dataset())\n",
        "    else:\n",
        "        with relay.quantize.qconfig(calibrate_mode='global_scale', global_scale=8.0):\n",
        "            mod = relay.quantize.quantize(mod, params)\n",
        "    return mod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run Inference\n-------------\nWe create a Relay VM to build and execute the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading /home/hongbing/.mxnet/models/resnet18_v1-a0666292.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/resnet18_v1-a0666292.zip...\n...100%, 0.02 MB, 99 KB/s, 0 seconds passed\nANTLR runtime and generated code versions disagree: 4.8!=4.7.2\nANTLR runtime and generated code versions disagree: 4.8!=4.7.2\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 64, 56, 56), 'int8'), ('TENSOR', (64, 64, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_nchw_winograd.cuda', ('TENSOR', (1, 64, 56, 56), 'int8'), ('TENSOR', (64, 64, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 64, 56, 56), 'int8'), ('TENSOR', (128, 64, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 64, 56, 56), 'int8'), ('TENSOR', (128, 64, 3, 3), 'int8'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 128, 28, 28), 'int8'), ('TENSOR', (128, 128, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_nchw_winograd.cuda', ('TENSOR', (1, 128, 28, 28), 'int8'), ('TENSOR', (128, 128, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 128, 28, 28), 'int8'), ('TENSOR', (256, 128, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 128, 28, 28), 'int8'), ('TENSOR', (256, 128, 3, 3), 'int8'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 256, 14, 14), 'int8'), ('TENSOR', (256, 256, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_nchw_winograd.cuda', ('TENSOR', (1, 256, 14, 14), 'int8'), ('TENSOR', (256, 256, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 256, 14, 14), 'int8'), ('TENSOR', (512, 256, 1, 1), 'int8'), (2, 2), (0, 0, 0, 0), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 256, 14, 14), 'int8'), ('TENSOR', (512, 256, 3, 3), 'int8'), (2, 2), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_NCHWc_int8.cuda', ('TENSOR', (1, 512, 7, 7), 'int8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'NCHW', 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('conv2d_nchw_winograd.cuda', ('TENSOR', (1, 512, 7, 7), 'int8'), ('TENSOR', (512, 512, 3, 3), 'int8'), (1, 1), (1, 1, 1, 1), (1, 1), 'int32'). A fallback configuration is used, which may bring great performance regression.\nWARNING:autotvm:Cannot find config for target=cuda, workload=('dense_small_batch.cuda', ('TENSOR', (1, 512), 'float32'), ('TENSOR', (1000, 512), 'float32'), None, 'float32'). A fallback configuration is used, which may bring great performance regression.\n"
        },
        {
          "output_type": "error",
          "ename": "TVMError",
          "evalue": "Traceback (most recent call last):\n  [bt] (8) /home/hongbing/Projects/tvm/build/libtvm.so(TVMFuncCall+0x65) [0x7f98cf83a3b5]\n  [bt] (7) /home/hongbing/Projects/tvm/build/libtvm.so(+0xb3a7b6) [0x7f98cf7397b6]\n  [bt] (6) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::relay::vm::VMCompiler::Codegen()+0xb73) [0x7f98cf7390a3]\n  [bt] (5) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::build(tvm::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::Array<tvm::tir::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x419) [0x7f98cf331859]\n  [bt] (4) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::build(tvm::Map<tvm::Target, tvm::Array<tvm::tir::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x374) [0x7f98cf330e14]\n  [bt] (3) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target const&)+0x1df) [0x7f98cf35d70f]\n  [bt] (2) /home/hongbing/Projects/tvm/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule)>(tvm::runtime::Module (*)(tvm::IRModule))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x51) [0x7f98cf3ab0c1]\n  [bt] (1) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::codegen::BuildCUDA(tvm::IRModule)+0x780) [0x7f98cf7ca7f0]\n  [bt] (0) /home/hongbing/Projects/tvm/build/libtvm.so(+0xc3681b) [0x7f98cf83581b]\n  File \"/home/hongbing/Projects/tvm/python/tvm/_ffi/_ctypes/packed_func.py\", line 78, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/home/hongbing/Projects/tvm/python/tvm/autotvm/measure/measure_methods.py\", line 597, in tvm_callback_cuda_compile\n    ptx = nvcc.compile_cuda(code, target=target, arch=AutotvmGlobalScope.current.cuda_target_arch)\n  File \"/home/hongbing/Projects/tvm/python/tvm/contrib/nvcc.py\", line 103, in compile_cuda\n    raise RuntimeError(msg)\nRuntimeError: Compilation error:\n/tmp/tmp40vxtja_/my_kernel.cu(16): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(65): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(84): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(154): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(185): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(204): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(223): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(260): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(297): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(320): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(524): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(551): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(598): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(692): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(717): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(740): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(765): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(786): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(809): error: identifier \"__dp4a\" is undefined\n\n19 errors detected in the compilation of \"/tmp/tmpxft_00002c9d_00000000-6_my_kernel.cpp1.ii\".\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-75b38427cf3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-75b38427cf3e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_aware\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-75b38427cf3e>\u001b[0m in \u001b[0;36mrun_inference\u001b[0;34m(mod)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_executor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_val_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/relay/build_module.py\u001b[0m in \u001b[0;36mcreate_executor\u001b[0;34m(kind, mod, ctx, target)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mGraphExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vm\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mVMExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown execution strategy: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/relay/backend/vm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mod, ctx, target)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvm_rt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVirtualMachine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/relay/backend/vm.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(mod, target, target_host, params)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/relay/backend/vm.py\u001b[0m in \u001b[0;36mcodegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcodegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;34m\"\"\"Generate the kernel library.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_codegen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Projects/tvm/python/tvm/_ffi/_ctypes/packed_func.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 ctypes.byref(ret_val), ctypes.byref(ret_tcode)) != 0:\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  [bt] (8) /home/hongbing/Projects/tvm/build/libtvm.so(TVMFuncCall+0x65) [0x7f98cf83a3b5]\n  [bt] (7) /home/hongbing/Projects/tvm/build/libtvm.so(+0xb3a7b6) [0x7f98cf7397b6]\n  [bt] (6) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::relay::vm::VMCompiler::Codegen()+0xb73) [0x7f98cf7390a3]\n  [bt] (5) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::build(tvm::Map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::Array<tvm::tir::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x419) [0x7f98cf331859]\n  [bt] (4) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::build(tvm::Map<tvm::Target, tvm::Array<tvm::tir::LoweredFunc, void>, void, void> const&, tvm::Target const&, tvm::BuildConfig const&)+0x374) [0x7f98cf330e14]\n  [bt] (3) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::codegen::Build(tvm::IRModule, tvm::Target const&)+0x1df) [0x7f98cf35d70f]\n  [bt] (2) /home/hongbing/Projects/tvm/build/libtvm.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::IRModule)>::AssignTypedLambda<tvm::runtime::Module (*)(tvm::IRModule)>(tvm::runtime::Module (*)(tvm::IRModule))::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0x51) [0x7f98cf3ab0c1]\n  [bt] (1) /home/hongbing/Projects/tvm/build/libtvm.so(tvm::codegen::BuildCUDA(tvm::IRModule)+0x780) [0x7f98cf7ca7f0]\n  [bt] (0) /home/hongbing/Projects/tvm/build/libtvm.so(+0xc3681b) [0x7f98cf83581b]\n  File \"/home/hongbing/Projects/tvm/python/tvm/_ffi/_ctypes/packed_func.py\", line 78, in cfun\n    rv = local_pyfunc(*pyargs)\n  File \"/home/hongbing/Projects/tvm/python/tvm/autotvm/measure/measure_methods.py\", line 597, in tvm_callback_cuda_compile\n    ptx = nvcc.compile_cuda(code, target=target, arch=AutotvmGlobalScope.current.cuda_target_arch)\n  File \"/home/hongbing/Projects/tvm/python/tvm/contrib/nvcc.py\", line 103, in compile_cuda\n    raise RuntimeError(msg)\nRuntimeError: Compilation error:\n/tmp/tmp40vxtja_/my_kernel.cu(16): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(65): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(84): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(154): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(185): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(204): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(223): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(260): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(297): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(320): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(524): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(551): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(598): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(692): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(717): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(740): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(765): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(786): error: identifier \"__dp4a\" is undefined\n\n/tmp/tmp40vxtja_/my_kernel.cu(809): error: identifier \"__dp4a\" is undefined\n\n19 errors detected in the compilation of \"/tmp/tmpxft_00002c9d_00000000-6_my_kernel.cpp1.ii\".\n"
          ]
        }
      ],
      "source": [
        "def run_inference(mod):\n",
        "    executor = relay.create_executor('vm', mod, ctx, target)\n",
        "    val_data, batch_fn = get_val_data()\n",
        "    for i, batch in enumerate(val_data):\n",
        "        data, label = batch_fn(batch)\n",
        "        prediction = executor.evaluate()(data)\n",
        "        if i > 10:  # only run inference on a few samples in this tutorial\n",
        "            break\n",
        "\n",
        "def main():\n",
        "    mod, params = get_model()\n",
        "    mod = quantize(mod, params, data_aware=True)\n",
        "    run_inference(mod)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}