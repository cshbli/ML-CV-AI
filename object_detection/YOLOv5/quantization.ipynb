{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export the PyTorch model to ONNX using Ultralytics `export.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hongbing/Projects/yolov5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/hongbing/Projects/yolov5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ~/Projects/yolov5\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['yolov5m.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=13, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5m.pt with output shape (1, 25200, 85) (40.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.0s, saved as yolov5m.onnx (81.2 MB)\n",
      "\n",
      "Export complete (1.7s)\n",
      "Results saved to \u001b[1m/home/hongbing/Projects/yolov5\u001b[0m\n",
      "Detect:          python detect.py --weights yolov5m.onnx \n",
      "Validate:        python val.py --weights yolov5m.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights yolov5m.pt --include onnx --opset 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the benchmark accuray of this float ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "Loading yolov5m.onnx for ONNX Runtime inference...\n",
      "2022-12-13 10:37:53.725313759 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335      0.715      0.582      0.635      0.447\n",
      "Speed: 0.3ms pre-process, 138.6ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp6/yolov5m_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=44.44s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=9.90s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.451\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.641\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.281\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.578\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.355\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.694\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.784\n",
      "Results saved to \u001b[1mruns/val/exp6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Qunatize the FP32 model using quantize_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from onnxruntime.quantization import quantize_static, QuantType, CalibrationMethod, CalibrationDataReader\n",
    "import torch\n",
    "from utils.dataloaders import LoadImages\n",
    "from utils.general import check_dataset\n",
    "import numpy as np\n",
    "\n",
    "def representative_dataset_gen(dataset, ncalib=100):\n",
    "    # Representative dataset generator for use with converter.representative_dataset, returns a generator of np arrays\n",
    "    def data_gen():\n",
    "        for n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\n",
    "            input = np.transpose(img, [0, 1, 2])\n",
    "            input = np.expand_dims(input, axis=0).astype(np.float32)\n",
    "            input /= 255\n",
    "            yield [input]\n",
    "    return data_gen\n",
    "\n",
    "class CalibrationDataGenYOLO(CalibrationDataReader):\n",
    "    def __init__(self,\n",
    "        calib_data_gen,\n",
    "        input_name\n",
    "    ):\n",
    "        x_train = calib_data_gen\n",
    "        self.calib_data = iter([{input_name: np.array(data[0])} for data in x_train()])\n",
    "\n",
    "    def get_next(self):\n",
    "        return next(self.calib_data, None)\n",
    "\n",
    "\n",
    "dataset = LoadImages(check_dataset('./data/coco128.yaml')['train'], img_size=[640, 640], auto=False)\n",
    "data_generator = representative_dataset_gen(dataset)\n",
    "\n",
    "data_reader = CalibrationDataGenYOLO(\n",
    "    calib_data_gen=data_generator,\n",
    "    input_name='images'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "model_path = 'yolov5m'\n",
    "# Quantize the exported model\n",
    "quantize_static(\n",
    "    f'{model_path}.onnx',\n",
    "    f'{model_path}_ort_quant.u8s8.onnx',\n",
    "    calibration_data_reader=data_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    per_channel=True,\n",
    "    reduce_range=True,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check `x_scale` of the quantized model\n",
    "\n",
    "We can see the last 2 layers have large scale values. 8 bits quantization will lost accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate the mAP using Ultralytics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python val.py --weights yolov5m_ort_quant.u8s8.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qunatize by excluding those big scale nodes\n",
    "\n",
    "### 3.1 Quantize\n",
    "- Excluding to quantize nodes which taking in those large tensors and cause mAP to 0.\n",
    "\n",
    "```\n",
    "nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "model_path = 'yolov5m'\n",
    "# Quantize the exported model\n",
    "quantize_static(\n",
    "    f'{model_path}.onnx',\n",
    "    f'{model_path}_ort_quant.u8s8.exclude.bigscale.onnx',\n",
    "    calibration_data_reader=data_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "    per_channel=True,\n",
    "    reduce_range=True,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate the mAP using Ultralytics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m_ort_quant.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "Loading yolov5m_ort_quant.onnx for ONNX Runtime inference...\n",
      "2022-12-13 11:12:12.819420504 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335      0.682       0.56      0.605      0.393\n",
      "Speed: 0.4ms pre-process, 88.1ms inference, 1.5ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp7/yolov5m_ort_quant_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.31s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.72s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=48.41s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=10.81s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.611\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.530\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.752\n",
      "Results saved to \u001b[1mruns/val/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m_ort_quant.u8s8.exclude.bigscale.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mAP of this quantized model is 0.397. The mAP of the FP32 model is 0.451."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantize the weights with UINT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Qunatize the FP32 model using quantize_static\n",
    "\n",
    "- Excluding to quantize nodes which taking in those large tensors and cause mAP to 0.\n",
    "\n",
    "```\n",
    "nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "model_path = 'yolov5m'\n",
    "# Quantize the exported model\n",
    "quantize_static(\n",
    "    f'{model_path}.onnx',\n",
    "    f'{model_path}_ort_quant.u8u8.exclude.bigscale.onnx',\n",
    "    calibration_data_reader=data_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QUInt8,\n",
    "    nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "    per_channel=True,\n",
    "    reduce_range=True,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate the mAP using Ultralytics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m_ort_quant.u8u8.exclude.bigscale.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "Loading yolov5m_ort_quant.u8u8.exclude.bigscale.onnx for ONNX Runtime inference...\n",
      "2022-12-13 11:57:10.679788417 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335        0.7      0.568      0.617       0.41\n",
      "Speed: 0.3ms pre-process, 83.1ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp9/yolov5m_ort_quant.u8u8.exclude.bigscale_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=45.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=9.91s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.623\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.448\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.233\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.467\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.544\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.655\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756\n",
      "Results saved to \u001b[1mruns/val/exp9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m_ort_quant.u8u8.exclude.bigscale.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mAP of this U8U8 quantized model is 0.413. The mAP of U8S8 quantized model is 0.397. The mAP of the FP32 model is 0.451."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert to TFLite INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['yolov5m.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=True, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['tflite']\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5m.pt with output shape (1, 25200, 85) (40.8 MB)\n",
      "2022-12-13 19:45:36.049568: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.11.0...\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "2022-12-13 19:45:37.073162: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  1     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  1    444672  models.common.C3                        [192, 192, 4]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  1   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  1   4134912  models.common.C3                        [768, 768, 2]                 \n",
      "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1    343485  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768], [640, 640]]\n",
      "WARNING:tensorflow:From /home/hongbing/venv/torch1.13.0/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(1, 640, 640, 3)]   0           []                               \n",
      "                                                                                                  \n",
      " tf_conv (TFConv)               (1, 320, 320, 48)    5232        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf_conv_1 (TFConv)             (1, 160, 160, 96)    41568       ['tf_conv[0][0]']                \n",
      "                                                                                                  \n",
      " tfc3 (TFC3)                    (1, 160, 160, 96)    64896       ['tf_conv_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_9 (TFConv)             (1, 80, 80, 192)     166080      ['tfc3[0][0]']                   \n",
      "                                                                                                  \n",
      " tfc3_1 (TFC3)                  (1, 80, 80, 192)     443520      ['tf_conv_9[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_21 (TFConv)            (1, 40, 40, 384)     663936      ['tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_2 (TFC3)                  (1, 40, 40, 384)     2509824     ['tf_conv_21[0][0]']             \n",
      "                                                                                                  \n",
      " tf_conv_37 (TFConv)            (1, 20, 20, 768)     2654976     ['tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_3 (TFC3)                  (1, 20, 20, 768)     4131840     ['tf_conv_37[0][0]']             \n",
      "                                                                                                  \n",
      " tfsppf (TFSPPF)                (1, 20, 20, 768)     1475712     ['tfc3_3[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_conv_47 (TFConv)            (1, 20, 20, 384)     295296      ['tfsppf[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample (TFUpsample)       (1, 40, 40, 384)     0           ['tf_conv_47[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat (TFConcat)           (1, 40, 40, 768)     0           ['tf_upsample[0][0]',            \n",
      "                                                                  'tfc3_2[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_4 (TFC3)                  (1, 40, 40, 384)     1181184     ['tf_concat[0][0]']              \n",
      "                                                                                                  \n",
      " tf_conv_55 (TFConv)            (1, 40, 40, 192)     73920       ['tfc3_4[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_upsample_1 (TFUpsample)     (1, 80, 80, 192)     0           ['tf_conv_55[0][0]']             \n",
      "                                                                                                  \n",
      " tf_concat_1 (TFConcat)         (1, 80, 80, 384)     0           ['tf_upsample_1[0][0]',          \n",
      "                                                                  'tfc3_1[0][0]']                 \n",
      "                                                                                                  \n",
      " tfc3_5 (TFC3)                  (1, 80, 80, 192)     295680      ['tf_concat_1[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_63 (TFConv)            (1, 40, 40, 192)     331968      ['tfc3_5[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_2 (TFConcat)         (1, 40, 40, 384)     0           ['tf_conv_63[0][0]',             \n",
      "                                                                  'tf_conv_55[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_6 (TFC3)                  (1, 40, 40, 384)     1033728     ['tf_concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " tf_conv_71 (TFConv)            (1, 20, 20, 384)     1327488     ['tfc3_6[0][0]']                 \n",
      "                                                                                                  \n",
      " tf_concat_3 (TFConcat)         (1, 20, 20, 768)     0           ['tf_conv_71[0][0]',             \n",
      "                                                                  'tf_conv_47[0][0]']             \n",
      "                                                                                                  \n",
      " tfc3_7 (TFC3)                  (1, 20, 20, 768)     4131840     ['tf_concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " tf_detect (TFDetect)           ((1, 25200, 85),     343485      ['tfc3_5[0][0]',                 \n",
      "                                )                                 'tfc3_6[0][0]',                 \n",
      "                                                                  'tfc3_7[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,172,173\n",
      "Trainable params: 0\n",
      "Non-trainable params: 21,172,173\n",
      "__________________________________________________________________________________________________\n",
      "2022-12-13 19:45:39.835396: I tensorflow/core/grappler/devices.cc:75] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA or ROCm support)\n",
      "2022-12-13 19:45:39.835494: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 6.9s, saved as yolov5m_saved_model (81.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.11.0...\n",
      "WARNING:absl:Found untraced functions such as tf_conv_2_layer_call_fn, tf_conv_2_layer_call_and_return_conditional_losses, tf_conv_3_layer_call_fn, tf_conv_3_layer_call_and_return_conditional_losses, tf_conv_4_layer_call_fn while saving (showing 5 of 378). These functions will not be directly callable after loading.\n",
      "2022-12-13 19:46:34.031357: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-12-13 19:46:34.031379: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-12-13 19:46:34.031993: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpx9zo2arz\n",
      "2022-12-13 19:46:34.074385: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-12-13 19:46:34.074411: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpx9zo2arz\n",
      "2022-12-13 19:46:34.195522: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2022-12-13 19:46:34.235751: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-12-13 19:46:34.564762: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpx9zo2arz\n",
      "2022-12-13 19:46:34.734470: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 702481 microseconds.\n",
      "2022-12-13 19:46:35.322379: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-13 19:46:35.876443: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 52.021 G  ops, equivalently 26.011 G  MACs\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n",
      "2022-12-13 19:49:42.139987: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2111] Estimated count of arithmetic ops: 52.021 G  ops, equivalently 26.011 G  MACs\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 239.9s, saved as yolov5m-int8.tflite (20.8 MB)\n",
      "\n",
      "Export complete (247.5s)\n",
      "Results saved to \u001b[1m/home/hongbing/Projects/yolov5\u001b[0m\n",
      "Detect:          python detect.py --weights yolov5m-int8.tflite \n",
      "Validate:        python val.py --weights yolov5m-int8.tflite \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m-int8.tflite')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights yolov5m.pt --include tflite --int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluatate with COCO128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/coco128.yaml, weights=['yolov5m-int8.tflite'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "2022-12-13 19:54:53.314992: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading yolov5m-int8.tflite for TensorFlow Lite inference...\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco128/labels/train2017.cache...\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all        128        929      0.682      0.655      0.735      0.493\n",
      "Speed: 0.4ms pre-process, 555.5ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m-int8.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Evaluate with COCO Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m-int8.tflite'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "2022-12-13 19:57:35.808720: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading yolov5m-int8.tflite for TensorFlow Lite inference...\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335      0.699      0.561      0.614      0.401\n",
      "Speed: 0.4ms pre-process, 560.3ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp12/yolov5m-int8_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.29s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=44.47s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=9.67s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.621\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.437\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.217\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.461\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.542\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.325\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.537\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.589\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.758\n",
      "Results saved to \u001b[1mruns/val/exp12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m-int8.tflite --data coco.yaml"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch1.13.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
