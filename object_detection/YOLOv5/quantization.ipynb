{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Export the PyTorch model to ONNX using Ultralytics `export.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hongbing/Projects/yolov5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/hongbing/Projects/yolov5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ~/Projects/yolov5\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['yolov5m.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=13, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from yolov5m.pt with output shape (1, 25200, 85) (40.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.0...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 1.0s, saved as yolov5m.onnx (81.2 MB)\n",
      "\n",
      "Export complete (1.7s)\n",
      "Results saved to \u001b[1m/home/hongbing/Projects/yolov5\u001b[0m\n",
      "Detect:          python detect.py --weights yolov5m.onnx \n",
      "Validate:        python val.py --weights yolov5m.onnx \n",
      "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m.onnx')  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights yolov5m.pt --include onnx --opset 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Get the benchmark accuray of this float ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "Loading yolov5m.onnx for ONNX Runtime inference...\n",
      "2022-12-13 10:37:53.725313759 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335      0.715      0.582      0.635      0.447\n",
      "Speed: 0.3ms pre-process, 138.6ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp6/yolov5m_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.53s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=44.44s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=9.90s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.451\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.641\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.490\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.281\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.506\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.578\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.355\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.694\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.784\n",
      "Results saved to \u001b[1mruns/val/exp6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Qunatize the FP32 model using quantize_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from onnxruntime.quantization import quantize_static, QuantType, CalibrationMethod, CalibrationDataReader\n",
    "import torch\n",
    "from utils.dataloaders import LoadImages\n",
    "from utils.general import check_dataset\n",
    "import numpy as np\n",
    "\n",
    "def representative_dataset_gen(dataset, ncalib=100):\n",
    "    # Representative dataset generator for use with converter.representative_dataset, returns a generator of np arrays\n",
    "    def data_gen():\n",
    "        for n, (path, img, im0s, vid_cap, string) in enumerate(dataset):\n",
    "            input = np.transpose(img, [0, 1, 2])\n",
    "            input = np.expand_dims(input, axis=0).astype(np.float32)\n",
    "            input /= 255\n",
    "            yield [input]\n",
    "    return data_gen\n",
    "\n",
    "class CalibrationDataGenYOLO(CalibrationDataReader):\n",
    "    def __init__(self,\n",
    "        calib_data_gen,\n",
    "        input_name\n",
    "    ):\n",
    "        x_train = calib_data_gen\n",
    "        self.calib_data = iter([{input_name: np.array(data[0])} for data in x_train()])\n",
    "\n",
    "    def get_next(self):\n",
    "        return next(self.calib_data, None)\n",
    "\n",
    "\n",
    "dataset = LoadImages(check_dataset('./data/coco128.yaml')['train'], img_size=[640, 640], auto=False)\n",
    "data_generator = representative_dataset_gen(dataset)\n",
    "\n",
    "data_reader = CalibrationDataGenYOLO(\n",
    "    calib_data_gen=data_generator,\n",
    "    input_name='images'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "model_path = 'yolov5m'\n",
    "# Quantize the exported model\n",
    "quantize_static(\n",
    "    f'{model_path}.onnx',\n",
    "    f'{model_path}_ort_quant.u8s8.onnx',\n",
    "    calibration_data_reader=data_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    per_channel=True,\n",
    "    reduce_range=True,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Check `x_scale` of the quantized model\n",
    "\n",
    "We can see the last 2 layers have large scale values. 8 bits quantization will lost accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate the mAP using Ultralytics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python val.py --weights yolov5m_ort_quant.u8s8.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qunatize by excluding those big scale nodes\n",
    "\n",
    "### 3.1 Quantize\n",
    "- Excluding to quantize nodes which taking in those large tensors and cause mAP to 0.\n",
    "\n",
    "```\n",
    "nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "model_path = 'yolov5m'\n",
    "# Quantize the exported model\n",
    "quantize_static(\n",
    "    f'{model_path}.onnx',\n",
    "    f'{model_path}_ort_quant.u8s8.exclude.bigscale.onnx',\n",
    "    calibration_data_reader=data_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "    per_channel=True,\n",
    "    reduce_range=True,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate the mAP using Ultralytics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m_ort_quant.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "Loading yolov5m_ort_quant.onnx for ONNX Runtime inference...\n",
      "2022-12-13 11:12:12.819420504 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335      0.682       0.56      0.605      0.393\n",
      "Speed: 0.4ms pre-process, 88.1ms inference, 1.5ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp7/yolov5m_ort_quant_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.31s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.72s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=48.41s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=10.81s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.611\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.427\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.202\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.452\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.539\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.530\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.752\n",
      "Results saved to \u001b[1mruns/val/exp7\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m_ort_quant.u8s8.exclude.bigscale.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mAP of this quantized model is 0.397. The mAP of the FP32 model is 0.451."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantize the weights with UINT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Qunatize the FP32 model using quantize_static\n",
    "\n",
    "- Excluding to quantize nodes which taking in those large tensors and cause mAP to 0.\n",
    "\n",
    "```\n",
    "nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
      "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "model_path = 'yolov5m'\n",
    "# Quantize the exported model\n",
    "quantize_static(\n",
    "    f'{model_path}.onnx',\n",
    "    f'{model_path}_ort_quant.u8u8.exclude.bigscale.onnx',\n",
    "    calibration_data_reader=data_reader,\n",
    "    activation_type=QuantType.QUInt8,\n",
    "    weight_type=QuantType.QUInt8,\n",
    "    nodes_to_exclude=[\"/model.24/Mul_1\", \"/model.24/Mul_3\", \"/model.24/Mul_5\", \"/model.24/Mul_7\", \"/model.24/Mul_9\", \"/model.24/Mul_11\", \"/model.24/Concat\", \"/model.24/Concat_1\", \"/model.24/Concat_2\", \"/model.24/Concat_3\"],\n",
    "    per_channel=True,\n",
    "    reduce_range=True,\n",
    "    calibrate_method=CalibrationMethod.MinMax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate the mAP using Ultralytics implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/hongbing/Projects/yolov5/data/coco.yaml, weights=['yolov5m_ort_quant.u8u8.exclude.bigscale.onnx'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 ðŸš€ v7.0-24-gf8539a68 Python-3.8.10 torch-1.13.0+cu117 CUDA:0 (Quadro RTX 5000, 16125MiB)\n",
      "\n",
      "Loading yolov5m_ort_quant.u8u8.exclude.bigscale.onnx for ONNX Runtime inference...\n",
      "2022-12-13 11:57:10.679788417 [W:onnxruntime:Default, onnxruntime_pybind_state.cc:578 CreateExecutionProviderInstance] Failed to create CUDAExecutionProvider. Please reference https://onnxruntime.ai/docs/reference/execution-providers/CUDA-ExecutionProvider.html#requirements to ensure all dependencies are met.\n",
      "Forcing --batch-size 1 square inference (1,3,640,640) for non-PyTorch models\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/hongbing/Projects/datasets/coco/val2017.cache... 4952 images\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       5000      36335        0.7      0.568      0.617       0.41\n",
      "Speed: 0.3ms pre-process, 83.1ms inference, 1.3ms NMS per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving runs/val/exp9/yolov5m_ort_quant.u8u8.exclude.bigscale_predictions.json...\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=3.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=45.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=9.91s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.623\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.448\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.233\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.467\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.330\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.544\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.655\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.756\n",
      "Results saved to \u001b[1mruns/val/exp9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights yolov5m_ort_quant.u8u8.exclude.bigscale.onnx --data coco.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mAP of this U8U8 quantized model is 0.413. The mAP of U8S8 quantized model is 0.397. The mAP of the FP32 model is 0.451."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42be7b6d852b9b2b1a0308f8b9cc6db97febf6d6b1b2a588c6bfd7d771418521"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('torch1.13.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
